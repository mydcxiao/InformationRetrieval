[
  {
    "title": "Probability distributions - torch.distributions — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributions.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Probability distributions - torch.distributions\nShortcuts\nPROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n\nThe distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package.\n\nIt is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples \n𝑓\n(\n𝑥\n)\nf(x), the pathwise derivative requires the derivative \n𝑓\n′\n(\n𝑥\n)\nf\n′\n(x). The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs .\n\nScore function\n\nWhen the probability density function is differentiable with respect to its parameters, we only need sample() and log_prob() to implement REINFORCE:\n\nΔ\n𝜃\n=\n𝛼\n𝑟\n∂\nlog\n⁡\n𝑝\n(\n𝑎\n∣\n𝜋\n𝜃\n(\n𝑠\n)\n)\n∂\n𝜃\nΔθ=αr\n∂θ\n∂logp(a∣π\nθ\n(s))\n\t​\n\n\nwhere \n𝜃\nθ are the parameters, \n𝛼\nα is the learning rate, \n𝑟\nr is the reward and \n𝑝\n(\n𝑎\n∣\n𝜋\n𝜃\n(\n𝑠\n)\n)\np(a∣π\nθ\n(s)) is the probability of taking action \n𝑎\na in state \n𝑠\ns given policy \n𝜋\n𝜃\nπ\nθ\n.\n\nIn practice we would sample an action from the output of a network, apply this action in an environment, and then use log_prob to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:\n\nprobs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n\nPathwise derivative\n\nThe other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the rsample() method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:\n\nparams = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n\nDistribution\nCLASS\ntorch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n[SOURCE]\n\nBases: object\n\nDistribution is the abstract base class for probability distributions.\n\nPROPERTY arg_constraints: DICT[STR, CONSTRAINT]\n\nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.\n\nPROPERTY batch_shape: SIZE\n\nReturns the shape over which parameters are batched.\n\ncdf(value)\n[SOURCE]\n\nReturns the cumulative density/mass function evaluated at value.\n\nParameters\n\nvalue (Tensor) –\n\nReturn type\n\nTensor\n\nentropy()\n[SOURCE]\n\nReturns entropy of distribution, batched over batch_shape.\n\nReturns\n\nTensor of shape batch_shape.\n\nReturn type\n\nTensor\n\nenumerate_support(expand=True)\n[SOURCE]\n\nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], …]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...\n\nTo iterate over the full Cartesian product use itertools.product(m.enumerate_support()).\n\nParameters\n\nexpand (bool) – whether to expand the support over the batch dims to match the distribution’s batch_shape.\n\nReturns\n\nTensor iterating over dimension 0.\n\nReturn type\n\nTensor\n\nPROPERTY event_shape: SIZE\n\nReturns the shape of a single sample (without batching).\n\nexpand(batch_shape, _instance=None)\n[SOURCE]\n\nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution’s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.\n\nParameters\n\nbatch_shape (torch.Size) – the desired expanded size.\n\n_instance – new instance provided by subclasses that need to override .expand.\n\nReturns\n\nNew distribution instance with batch dimensions expanded to batch_size.\n\nicdf(value)\n[SOURCE]\n\nReturns the inverse cumulative density/mass function evaluated at value.\n\nParameters\n\nvalue (Tensor) –\n\nReturn type\n\nTensor\n\nlog_prob(value)\n[SOURCE]\n\nReturns the log of the probability density/mass function evaluated at value.\n\nParameters\n\nvalue (Tensor) –\n\nReturn type\n\nTensor\n\nPROPERTY mean: TENSOR\n\nReturns the mean of the distribution.\n\nPROPERTY mode: TENSOR\n\nReturns the mode of the distribution.\n\nperplexity()\n[SOURCE]\n\nReturns perplexity of distribution, batched over batch_shape.\n\nReturns\n\nTensor of shape batch_shape.\n\nReturn type\n\nTensor\n\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.\n\nReturn type\n\nTensor\n\nsample(sample_shape=torch.Size([]))\n[SOURCE]\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.\n\nReturn type\n\nTensor\n\nsample_n(n)\n[SOURCE]\n\nGenerates n samples or n batches of samples if the distribution parameters are batched.\n\nReturn type\n\nTensor\n\nSTATIC set_default_validate_args(value)\n[SOURCE]\n\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python’s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.\n\nParameters\n\nvalue (bool) – Whether to enable validation.\n\nPROPERTY stddev: TENSOR\n\nReturns the standard deviation of the distribution.\n\nPROPERTY support: OPTIONAL[ANY]\n\nReturns a Constraint object representing this distribution’s support.\n\nPROPERTY variance: TENSOR\n\nReturns the variance of the distribution.\n\nExponentialFamily\nCLASS\ntorch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below\n\n𝑝\n𝐹\n(\n𝑥\n;\n𝜃\n)\n=\nexp\n⁡\n(\n⟨\n𝑡\n(\n𝑥\n)\n,\n𝜃\n⟩\n−\n𝐹\n(\n𝜃\n)\n+\n𝑘\n(\n𝑥\n)\n)\np\nF\n\t​\n\n(x;θ)=exp(⟨t(x),θ⟩−F(θ)+k(x))\n\nwhere \n𝜃\nθ denotes the natural parameters, \n𝑡\n(\n𝑥\n)\nt(x) denotes the sufficient statistic, \n𝐹\n(\n𝜃\n)\nF(θ) is the log normalizer function for a given family and \n𝑘\n(\n𝑥\n)\nk(x) is the carrier measure.\n\nNOTE\n\nThis class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).\n\nentropy()\n[SOURCE]\n\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\nBernoulli\nCLASS\ntorch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Bernoulli distribution parameterized by probs or logits (but not both).\n\nSamples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.\n\nExample:\n\n>>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n\nParameters\n\nprobs (Number, Tensor) – the probability of sampling 1\n\nlogits (Number, Tensor) – the log-odds of sampling 1\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)}\nentropy()\n[SOURCE]\nenumerate_support(expand=True)\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_enumerate_support = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = BOOLEAN()\nPROPERTY variance\nBeta\nCLASS\ntorch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nBeta distribution parameterized by concentration1 and concentration0.\n\nExample:\n\n>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n\nParameters\n\nconcentration1 (float or Tensor) – 1st concentration parameter of the distribution (often referred to as alpha)\n\nconcentration0 (float or Tensor) – 2nd concentration parameter of the distribution (often referred to as beta)\n\narg_constraints = {'CONCENTRATION0': GREATERTHAN(LOWER_BOUND=0.0), 'CONCENTRATION1': GREATERTHAN(LOWER_BOUND=0.0)}\nPROPERTY concentration0\nPROPERTY concentration1\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=())\n[SOURCE]\nsupport = INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)\nPROPERTY variance\nBinomial\nCLASS\ntorch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits.\n\nExample:\n\n>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n\nParameters\n\ntotal_count (int or Tensor) – number of Bernoulli trials\n\nprobs (Tensor) – Event probabilities\n\nlogits (Tensor) – Event log-odds\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0), 'TOTAL_COUNT': INTEGERGREATERTHAN(LOWER_BOUND=0)}\nentropy()\n[SOURCE]\nenumerate_support(expand=True)\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_enumerate_support = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY support\nPROPERTY variance\nCategorical\nCLASS\ntorch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a categorical distribution parameterized by either probs or logits (but not both).\n\nNOTE\n\nIt is equivalent to the distribution that torch.multinomial() samples from.\n\nSamples are integers from \n{\n0\n,\n…\n,\n𝐾\n−\n1\n}\n{0,…,K−1} where K is probs.size(-1).\n\nIf probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.\n\nIf probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.\n\nNOTE\n\nThe probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.\n\nSee also: torch.multinomial()\n\nExample:\n\n>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n\nParameters\n\nprobs (Tensor) – event probabilities\n\nlogits (Tensor) – event log probabilities (unnormalized)\n\narg_constraints = {'LOGITS': INDEPENDENTCONSTRAINT(REAL(), 1), 'PROBS': SIMPLEX()}\nentropy()\n[SOURCE]\nenumerate_support(expand=True)\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_enumerate_support = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY support\nPROPERTY variance\nCauchy\nCLASS\ntorch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.\n\nExample:\n\n>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n\nParameters\n\nloc (float or Tensor) – mode or median of the distribution.\n\nscale (float or Tensor) – half width at half maximum.\n\narg_constraints = {'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = REAL()\nPROPERTY variance\nChi2\nCLASS\ntorch.distributions.chi2.Chi2(df, validate_args=None)\n[SOURCE]\n\nBases: Gamma\n\nCreates a Chi-squared distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)\n\nExample:\n\n>>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n\nParameters\n\ndf (float or Tensor) – shape parameter of the distribution\n\narg_constraints = {'DF': GREATERTHAN(LOWER_BOUND=0.0)}\nPROPERTY df\nexpand(batch_shape, _instance=None)\n[SOURCE]\nContinuousBernoulli\nCLASS\ntorch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a continuous Bernoulli distribution parameterized by probs or logits (but not both).\n\nThe distribution is supported in [0, 1] and parameterized by ‘probs’ (in (0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’ does not correspond to a probability and ‘logits’ does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.\n\nExample:\n\n>>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n\nParameters\n\nprobs (Number, Tensor) – (0,1) valued parameters\n\nlogits (Number, Tensor) – real valued parameters whose sigmoid matches ‘probs’\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY param_shape\nPROPERTY probs\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY stddev\nsupport = INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)\nPROPERTY variance\nDirichlet\nCLASS\ntorch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Dirichlet distribution parameterized by concentration concentration.\n\nExample:\n\n>>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentration [0.5, 0.5]\ntensor([ 0.1046,  0.8954])\n\nParameters\n\nconcentration (Tensor) – concentration parameter of the distribution (often referred to as alpha)\n\narg_constraints = {'CONCENTRATION': INDEPENDENTCONSTRAINT(GREATERTHAN(LOWER_BOUND=0.0), 1)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=())\n[SOURCE]\nsupport = SIMPLEX()\nPROPERTY variance\nExponential\nCLASS\ntorch.distributions.exponential.Exponential(rate, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Exponential distribution parameterized by rate.\n\nExample:\n\n>>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n\nParameters\n\nrate (float or Tensor) – rate = 1 / scale of the distribution\n\narg_constraints = {'RATE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY stddev\nsupport = GREATERTHANEQ(LOWER_BOUND=0.0)\nPROPERTY variance\nFisherSnedecor\nCLASS\ntorch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Fisher-Snedecor distribution parameterized by df1 and df2.\n\nExample:\n\n>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n\nParameters\n\ndf1 (float or Tensor) – degrees of freedom parameter 1\n\ndf2 (float or Tensor) – degrees of freedom parameter 2\n\narg_constraints = {'DF1': GREATERTHAN(LOWER_BOUND=0.0), 'DF2': GREATERTHAN(LOWER_BOUND=0.0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = GREATERTHAN(LOWER_BOUND=0.0)\nPROPERTY variance\nGamma\nCLASS\ntorch.distributions.gamma.Gamma(concentration, rate, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Gamma distribution parameterized by shape concentration and rate.\n\nExample:\n\n>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n\nParameters\n\nconcentration (float or Tensor) – shape parameter of the distribution (often referred to as alpha)\n\nrate (float or Tensor) – rate = 1 / scale of the distribution (often referred to as beta)\n\narg_constraints = {'CONCENTRATION': GREATERTHAN(LOWER_BOUND=0.0), 'RATE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = GREATERTHANEQ(LOWER_BOUND=0.0)\nPROPERTY variance\nGeometric\nCLASS\ntorch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in \n𝑘\n+\n1\nk+1 Bernoulli trials, the first \n𝑘\nk trials failed, before seeing a success.\n\nSamples are non-negative integers [0, \ninf\n⁡\ninf).\n\nExample:\n\n>>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n\nParameters\n\nprobs (Number, Tensor) – the probability of sampling 1. Must be in range (0, 1]\n\nlogits (Number, Tensor) – the log-odds of sampling 1.\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = INTEGERGREATERTHAN(LOWER_BOUND=0)\nPROPERTY variance\nGumbel\nCLASS\ntorch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nSamples from a Gumbel Distribution.\n\nExamples:\n\n>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n\nParameters\n\nloc (float or Tensor) – Location parameter of the distribution\n\nscale (float or Tensor) – Scale parameter of the distribution\n\narg_constraints: DICT[STR, CONSTRAINT] = {'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY stddev\nsupport = REAL()\nPROPERTY variance\nHalfCauchy\nCLASS\ntorch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nCreates a half-Cauchy distribution parameterized by scale where:\n\nX ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n\n\nExample:\n\n>>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n\nParameters\n\nscale (float or Tensor) – scale of the full Cauchy distribution\n\narg_constraints: DICT[STR, CONSTRAINT] = {'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(prob)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY scale\nsupport = GREATERTHANEQ(LOWER_BOUND=0.0)\nPROPERTY variance\nHalfNormal\nCLASS\ntorch.distributions.half_normal.HalfNormal(scale, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nCreates a half-normal distribution parameterized by scale where:\n\nX ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n\n\nExample:\n\n>>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n\nParameters\n\nscale (float or Tensor) – scale of the full Normal distribution\n\narg_constraints: DICT[STR, CONSTRAINT] = {'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(prob)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY scale\nsupport = GREATERTHANEQ(LOWER_BOUND=0.0)\nPROPERTY variance\nIndependent\nCLASS\ntorch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nReinterprets some of the batch dims of a distribution as event dims.\n\nThis is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:\n\n>>> from torch.distributions.multivariate_normal import MultivariateNormal\n>>> from torch.distributions.normal import Normal\n>>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size([]), torch.Size([3])]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size([3]), torch.Size([])]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size([]), torch.Size([3])]\n\nParameters\n\nbase_distribution (torch.distributions.distribution.Distribution) – a base distribution\n\nreinterpreted_batch_ndims (int) – the number of batch dims to reinterpret as event dims\n\narg_constraints: DICT[STR, CONSTRAINT] = {}\nentropy()\n[SOURCE]\nenumerate_support(expand=True)\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nPROPERTY has_enumerate_support\nPROPERTY has_rsample\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY support\nPROPERTY variance\nKumaraswamy\nCLASS\ntorch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nSamples from a Kumaraswamy distribution.\n\nExample:\n\n>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n\nParameters\n\nconcentration1 (float or Tensor) – 1st concentration parameter of the distribution (often referred to as alpha)\n\nconcentration0 (float or Tensor) – 2nd concentration parameter of the distribution (often referred to as beta)\n\narg_constraints: DICT[STR, CONSTRAINT] = {'CONCENTRATION0': GREATERTHAN(LOWER_BOUND=0.0), 'CONCENTRATION1': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nPROPERTY mean\nPROPERTY mode\nsupport = INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)\nPROPERTY variance\nLKJCholesky\nCLASS\ntorch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nLKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \n𝜂\nη to make the probability of the correlation matrix \n𝑀\nM generated from a Cholesky factor proportional to \ndet\n⁡\n(\n𝑀\n)\n𝜂\n−\n1\ndet(M)\nη−1\n. Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices:\n\nL ~ LKJCholesky(dim, concentration)\nX = L @ L' ~ LKJCorr(dim, concentration)\n\n\nNote that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.\n\nExample:\n\n>>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n\nParameters\n\ndimension (dim) – dimension of the matrices\n\nconcentration (float or Tensor) – concentration/shape parameter of the distribution (often referred to as eta)\n\nReferences\n\n[1] Generating random correlation matrices based on vines and extended onion method (2009), Daniel Lewandowski, Dorota Kurowicka, Harry Joe. Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008\n\narg_constraints = {'CONCENTRATION': GREATERTHAN(LOWER_BOUND=0.0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = CORRCHOLESKY()\nLaplace\nCLASS\ntorch.distributions.laplace.Laplace(loc, scale, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Laplace distribution parameterized by loc and scale.\n\nExample:\n\n>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n\nParameters\n\nloc (float or Tensor) – mean of the distribution\n\nscale (float or Tensor) – scale of the distribution\n\narg_constraints = {'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY stddev\nsupport = REAL()\nPROPERTY variance\nLogNormal\nCLASS\ntorch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nCreates a log-normal distribution parameterized by loc and scale where:\n\nX ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n\n\nExample:\n\n>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n\nParameters\n\nloc (float or Tensor) – mean of log of distribution\n\nscale (float or Tensor) – standard deviation of log of the distribution\n\narg_constraints: DICT[STR, CONSTRAINT] = {'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nPROPERTY loc\nPROPERTY mean\nPROPERTY mode\nPROPERTY scale\nsupport = GREATERTHAN(LOWER_BOUND=0.0)\nPROPERTY variance\nLowRankMultivariateNormal\nCLASS\ntorch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag:\n\ncovariance_matrix = cov_factor @ cov_factor.T + cov_diag\n\n\nExample\n\n>>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n\nParameters\n\nloc (Tensor) – mean of the distribution with shape batch_shape + event_shape\n\ncov_factor (Tensor) – factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)\n\ncov_diag (Tensor) – diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape\n\nNOTE\n\nThe computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size “capacitance” matrix:\n\ncapacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n\narg_constraints = {'COV_DIAG': INDEPENDENTCONSTRAINT(GREATERTHAN(LOWER_BOUND=0.0), 1), 'COV_FACTOR': INDEPENDENTCONSTRAINT(REAL(), 2), 'LOC': INDEPENDENTCONSTRAINT(REAL(), 1)}\nPROPERTY covariance_matrix\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY precision_matrix\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY scale_tril\nsupport = INDEPENDENTCONSTRAINT(REAL(), 1)\nPROPERTY variance\nMixtureSameFamily\nCLASS\ntorch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nThe MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical “selecting distribution” (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.\n\nExamples:\n\n>>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n>>> # weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally\n>>> # weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n...          torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct a batch of 3 Gaussian Mixture Models in 2D each\n>>> # consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n...         torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\nParameters\n\nmixture_distribution – torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]\n\ncomponent_distribution – torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.\n\narg_constraints: DICT[STR, CONSTRAINT] = {}\ncdf(x)\n[SOURCE]\nPROPERTY component_distribution\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = FALSE\nlog_prob(x)\n[SOURCE]\nPROPERTY mean\nPROPERTY mixture_distribution\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY support\nPROPERTY variance\nMultinomial\nCLASS\ntorch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches.\n\nNote that total_count need not be specified if only log_prob() is called (see example below)\n\nNOTE\n\nThe probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.\n\nsample() requires a single shared total_count for all parameters and samples.\n\nlog_prob() allows different total_count for each parameter and sample.\n\nExample:\n\n>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n\nParameters\n\ntotal_count (int) – number of trials\n\nprobs (Tensor) – event probabilities\n\nlogits (Tensor) – event log probabilities (unnormalized)\n\narg_constraints = {'LOGITS': INDEPENDENTCONSTRAINT(REAL(), 1), 'PROBS': SIMPLEX()}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY support\ntotal_count: INT\nPROPERTY variance\nMultivariateNormal\nCLASS\ntorch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.\n\nThe multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \n𝛴\nΣ or a positive definite precision matrix \n𝛴\n−\n1\nΣ\n−1\n or a lower-triangular matrix \n𝐿\nL with positive-valued diagonal entries, such that \n𝛴\n=\n𝐿\n𝐿\n⊤\nΣ=LL\n⊤\n. This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.\n\nExample\n\n>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n\nParameters\n\nloc (Tensor) – mean of the distribution\n\ncovariance_matrix (Tensor) – positive-definite covariance matrix\n\nprecision_matrix (Tensor) – positive-definite precision matrix\n\nscale_tril (Tensor) – lower-triangular factor of covariance, with positive-valued diagonal\n\nNOTE\n\nOnly one of covariance_matrix or precision_matrix or scale_tril can be specified.\n\nUsing scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.\n\narg_constraints = {'COVARIANCE_MATRIX': POSITIVEDEFINITE(), 'LOC': INDEPENDENTCONSTRAINT(REAL(), 1), 'PRECISION_MATRIX': POSITIVEDEFINITE(), 'SCALE_TRIL': LOWERCHOLESKY()}\nPROPERTY covariance_matrix\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY precision_matrix\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY scale_tril\nsupport = INDEPENDENTCONSTRAINT(REAL(), 1)\nPROPERTY variance\nNegativeBinomial\nCLASS\ntorch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of success of each Bernoulli trial is probs.\n\nParameters\n\ntotal_count (float or Tensor) – non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count\n\nprobs (Tensor) – Event probabilities of success in the half open interval [0, 1)\n\nlogits (Tensor) – Event log-odds for probabilities of success\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': HALFOPENINTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0), 'TOTAL_COUNT': GREATERTHANEQ(LOWER_BOUND=0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = INTEGERGREATERTHAN(LOWER_BOUND=0)\nPROPERTY variance\nNormal\nCLASS\ntorch.distributions.normal.Normal(loc, scale, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a normal (also called Gaussian) distribution parameterized by loc and scale.\n\nExample:\n\n>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n\nParameters\n\nloc (float or Tensor) – mean of the distribution (often referred to as mu)\n\nscale (float or Tensor) – standard deviation of the distribution (often referred to as sigma)\n\narg_constraints = {'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY stddev\nsupport = REAL()\nPROPERTY variance\nOneHotCategorical\nCLASS\ntorch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a one-hot categorical distribution parameterized by probs or logits.\n\nSamples are one-hot coded vectors of size probs.size(-1).\n\nNOTE\n\nThe probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.\n\nSee also: torch.distributions.Categorical() for specifications of probs and logits.\n\nExample:\n\n>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n\nParameters\n\nprobs (Tensor) – event probabilities\n\nlogits (Tensor) – event log probabilities (unnormalized)\n\narg_constraints = {'LOGITS': INDEPENDENTCONSTRAINT(REAL(), 1), 'PROBS': SIMPLEX()}\nentropy()\n[SOURCE]\nenumerate_support(expand=True)\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_enumerate_support = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY mean\nPROPERTY mode\nPROPERTY param_shape\nPROPERTY probs\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = ONEHOT()\nPROPERTY variance\nPareto\nCLASS\ntorch.distributions.pareto.Pareto(scale, alpha, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nSamples from a Pareto Type 1 distribution.\n\nExample:\n\n>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n\nParameters\n\nscale (float or Tensor) – Scale parameter of the distribution\n\nalpha (float or Tensor) – Shape parameter of the distribution\n\narg_constraints: DICT[STR, CONSTRAINT] = {'ALPHA': GREATERTHAN(LOWER_BOUND=0.0), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY support\nPROPERTY variance\nPoisson\nCLASS\ntorch.distributions.poisson.Poisson(rate, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Poisson distribution parameterized by rate, the rate parameter.\n\nSamples are nonnegative integers, with a pmf given by\n\nr\na\nt\ne\n𝑘\n𝑒\n−\nr\na\nt\ne\n𝑘\n!\nrate\nk\nk!\ne\n−rate\n\t​\n\n\nExample:\n\n>>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n\nParameters\n\nrate (Number, Tensor) – the rate parameter\n\narg_constraints = {'RATE': GREATERTHANEQ(LOWER_BOUND=0.0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = INTEGERGREATERTHAN(LOWER_BOUND=0)\nPROPERTY variance\nRelaxedBernoulli\nCLASS\ntorch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nCreates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.\n\nExample:\n\n>>> m = RelaxedBernoulli(torch.tensor([2.2]),\n...                      torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n\nParameters\n\ntemperature (Tensor) – relaxation temperature\n\nprobs (Number, Tensor) – the probability of sampling 1\n\nlogits (Number, Tensor) – the log-odds of sampling 1\n\narg_constraints: DICT[STR, CONSTRAINT] = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nPROPERTY logits\nPROPERTY probs\nsupport = INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)\nPROPERTY temperature\nLogitRelaxedBernoulli\nCLASS\ntorch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution.\n\nSamples are logits of values in (0, 1). See [1] for more details.\n\nParameters\n\ntemperature (Tensor) – relaxation temperature\n\nprobs (Number, Tensor) – the probability of sampling 1\n\nlogits (Number, Tensor) – the log-odds of sampling 1\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)\n\n[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n\narg_constraints = {'LOGITS': REAL(), 'PROBS': INTERVAL(LOWER_BOUND=0.0, UPPER_BOUND=1.0)}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY logits\nPROPERTY param_shape\nPROPERTY probs\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = REAL()\nRelaxedOneHotCategorical\nCLASS\ntorch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nCreates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable.\n\nExample:\n\n>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n...                              torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n\nParameters\n\ntemperature (Tensor) – relaxation temperature\n\nprobs (Tensor) – event probabilities\n\nlogits (Tensor) – unnormalized log probability for each event\n\narg_constraints: DICT[STR, CONSTRAINT] = {'LOGITS': INDEPENDENTCONSTRAINT(REAL(), 1), 'PROBS': SIMPLEX()}\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nPROPERTY logits\nPROPERTY probs\nsupport = SIMPLEX()\nPROPERTY temperature\nStudentT\nCLASS\ntorch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nCreates a Student’s t-distribution parameterized by degree of freedom df, mean loc and scale scale.\n\nExample:\n\n>>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n\nParameters\n\ndf (float or Tensor) – degrees of freedom\n\nloc (float or Tensor) – mean of the distribution\n\nscale (float or Tensor) – scale of the distribution\n\narg_constraints = {'DF': GREATERTHAN(LOWER_BOUND=0.0), 'LOC': REAL(), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nsupport = REAL()\nPROPERTY variance\nTransformedDistribution\nCLASS\ntorch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nExtension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:\n\nX ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n\n\nNote that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.\n\nAn example for the usage of TransformedDistribution would be:\n\n# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n\n\nFor more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical\n\narg_constraints: DICT[STR, CONSTRAINT] = {}\ncdf(value)\n[SOURCE]\n\nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.\n\nexpand(batch_shape, _instance=None)\n[SOURCE]\nPROPERTY has_rsample\nicdf(value)\n[SOURCE]\n\nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.\n\nlog_prob(value)\n[SOURCE]\n\nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.\n\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.\n\nsample(sample_shape=torch.Size([]))\n[SOURCE]\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.\n\nPROPERTY support\nUniform\nCLASS\ntorch.distributions.uniform.Uniform(low, high, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nGenerates uniformly distributed random samples from the half-open interval [low, high).\n\nExample:\n\n>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n\nParameters\n\nlow (float or Tensor) – lower range (inclusive).\n\nhigh (float or Tensor) – upper range (exclusive).\n\narg_constraints = {'HIGH': DEPENDENT(), 'LOW': DEPENDENT()}\ncdf(value)\n[SOURCE]\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nicdf(value)\n[SOURCE]\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nrsample(sample_shape=torch.Size([]))\n[SOURCE]\nPROPERTY stddev\nPROPERTY support\nPROPERTY variance\nVonMises\nCLASS\ntorch.distributions.von_mises.VonMises(loc, concentration, validate_args=None)\n[SOURCE]\n\nBases: Distribution\n\nA circular von Mises distribution.\n\nThis implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.\n\nExample::\n>>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n\nParameters\n\nloc (torch.Tensor) – an angle in radians.\n\nconcentration (torch.Tensor) – concentration parameter\n\narg_constraints = {'CONCENTRATION': GREATERTHAN(LOWER_BOUND=0.0), 'LOC': REAL()}\nexpand(batch_shape)\n[SOURCE]\nhas_rsample = FALSE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\n\nThe provided mean is the circular one.\n\nPROPERTY mode\nsample(sample_shape=torch.Size([]))\n[SOURCE]\n\nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. “Efficient simulation of the von Mises distribution.” Applied Statistics (1979): 152-157.\n\nsupport = REAL()\nPROPERTY variance\n\nThe provided variance is the circular one.\n\nWeibull\nCLASS\ntorch.distributions.weibull.Weibull(scale, concentration, validate_args=None)\n[SOURCE]\n\nBases: TransformedDistribution\n\nSamples from a two-parameter Weibull distribution.\n\nExample\n\n>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n\nParameters\n\nscale (float or Tensor) – Scale parameter of distribution (lambda).\n\nconcentration (float or Tensor) – Concentration parameter of distribution (k/shape).\n\narg_constraints: DICT[STR, CONSTRAINT] = {'CONCENTRATION': GREATERTHAN(LOWER_BOUND=0.0), 'SCALE': GREATERTHAN(LOWER_BOUND=0.0)}\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nsupport = GREATERTHAN(LOWER_BOUND=0.0)\nPROPERTY variance\nWishart\nCLASS\ntorch.distributions.wishart.Wishart(df, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n[SOURCE]\n\nBases: ExponentialFamily\n\nCreates a Wishart distribution parameterized by a symmetric positive definite matrix \nΣ\nΣ, or its Cholesky decomposition \n𝛴\n=\n𝐿\n𝐿\n⊤\nΣ=LL\n⊤\n\nExample\n\n>>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))\n>>> m.sample()  # Wishart distributed with mean=`df * I` and\n>>>             # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n\nParameters\n\ndf (float or Tensor) – real-valued parameter larger than the (dimension of Square matrix) - 1\n\ncovariance_matrix (Tensor) – positive-definite covariance matrix\n\nprecision_matrix (Tensor) – positive-definite precision matrix\n\nscale_tril (Tensor) – lower-triangular factor of covariance, with positive-valued diagonal\n\nNOTE\n\nOnly one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. ‘torch.distributions.LKJCholesky’ is a restricted Wishart distribution.[1]\n\nReferences\n\n[1] Wang, Z., Wu, Y. and Chu, H., 2018. On equivalence of the LKJ distribution and the restricted Wishart distribution. [2] Sawyer, S., 2007. Wishart Distributions and Inverse-Wishart Sampling. [3] Anderson, T. W., 2003. An Introduction to Multivariate Statistical Analysis (3rd ed.). [4] Odell, P. L. & Feiveson, A. H., 1966. A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.\n\narg_constraints = {'COVARIANCE_MATRIX': POSITIVEDEFINITE(), 'DF': GREATERTHAN(LOWER_BOUND=0), 'PRECISION_MATRIX': POSITIVEDEFINITE(), 'SCALE_TRIL': LOWERCHOLESKY()}\nPROPERTY covariance_matrix\nentropy()\n[SOURCE]\nexpand(batch_shape, _instance=None)\n[SOURCE]\nhas_rsample = TRUE\nlog_prob(value)\n[SOURCE]\nPROPERTY mean\nPROPERTY mode\nPROPERTY precision_matrix\nrsample(sample_shape=torch.Size([]), max_try_correction=None)\n[SOURCE]\n\nWARNING\n\nIn some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly.\n\nPROPERTY scale_tril\nsupport = POSITIVEDEFINITE()\nPROPERTY variance\nKL Divergence\ntorch.distributions.kl.kl_divergence(p, q)\n[SOURCE]\n\nCompute Kullback-Leibler divergence \n𝐾\n𝐿\n(\n𝑝\n∥\n𝑞\n)\nKL(p∥q) between two distributions.\n\n𝐾\n𝐿\n(\n𝑝\n∥\n𝑞\n)\n=\n∫\n𝑝\n(\n𝑥\n)\nlog\n⁡\n𝑝\n(\n𝑥\n)\n𝑞\n(\n𝑥\n)\n \n𝑑\n𝑥\nKL(p∥q)=∫p(x)log\nq(x)\np(x)\n\t​\n\ndx\nParameters\n\np (Distribution) – A Distribution object.\n\nq (Distribution) – A Distribution object.\n\nReturns\n\nA batch of KL divergences of shape batch_shape.\n\nReturn type\n\nTensor\n\nRaises\n\nNotImplementedError – If the distribution types have not been registered via register_kl().\n\nKL divergence is currently implemented for the following distribution pairs:\n\nBernoulli and Bernoulli\n\nBernoulli and Poisson\n\nBeta and Beta\n\nBeta and ContinuousBernoulli\n\nBeta and Exponential\n\nBeta and Gamma\n\nBeta and Normal\n\nBeta and Pareto\n\nBeta and Uniform\n\nBinomial and Binomial\n\nCategorical and Categorical\n\nCauchy and Cauchy\n\nContinuousBernoulli and ContinuousBernoulli\n\nContinuousBernoulli and Exponential\n\nContinuousBernoulli and Normal\n\nContinuousBernoulli and Pareto\n\nContinuousBernoulli and Uniform\n\nDirichlet and Dirichlet\n\nExponential and Beta\n\nExponential and ContinuousBernoulli\n\nExponential and Exponential\n\nExponential and Gamma\n\nExponential and Gumbel\n\nExponential and Normal\n\nExponential and Pareto\n\nExponential and Uniform\n\nExponentialFamily and ExponentialFamily\n\nGamma and Beta\n\nGamma and ContinuousBernoulli\n\nGamma and Exponential\n\nGamma and Gamma\n\nGamma and Gumbel\n\nGamma and Normal\n\nGamma and Pareto\n\nGamma and Uniform\n\nGeometric and Geometric\n\nGumbel and Beta\n\nGumbel and ContinuousBernoulli\n\nGumbel and Exponential\n\nGumbel and Gamma\n\nGumbel and Gumbel\n\nGumbel and Normal\n\nGumbel and Pareto\n\nGumbel and Uniform\n\nHalfNormal and HalfNormal\n\nIndependent and Independent\n\nLaplace and Beta\n\nLaplace and ContinuousBernoulli\n\nLaplace and Exponential\n\nLaplace and Gamma\n\nLaplace and Laplace\n\nLaplace and Normal\n\nLaplace and Pareto\n\nLaplace and Uniform\n\nLowRankMultivariateNormal and LowRankMultivariateNormal\n\nLowRankMultivariateNormal and MultivariateNormal\n\nMultivariateNormal and LowRankMultivariateNormal\n\nMultivariateNormal and MultivariateNormal\n\nNormal and Beta\n\nNormal and ContinuousBernoulli\n\nNormal and Exponential\n\nNormal and Gamma\n\nNormal and Gumbel\n\nNormal and Laplace\n\nNormal and Normal\n\nNormal and Pareto\n\nNormal and Uniform\n\nOneHotCategorical and OneHotCategorical\n\nPareto and Beta\n\nPareto and ContinuousBernoulli\n\nPareto and Exponential\n\nPareto and Gamma\n\nPareto and Normal\n\nPareto and Pareto\n\nPareto and Uniform\n\nPoisson and Bernoulli\n\nPoisson and Binomial\n\nPoisson and Poisson\n\nTransformedDistribution and TransformedDistribution\n\nUniform and Beta\n\nUniform and ContinuousBernoulli\n\nUniform and Exponential\n\nUniform and Gamma\n\nUniform and Gumbel\n\nUniform and Normal\n\nUniform and Pareto\n\nUniform and Uniform\n\ntorch.distributions.kl.register_kl(type_p, type_q)\n[SOURCE]\n\nDecorator to register a pairwise function with kl_divergence(). Usage:\n\n@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n\n\nLookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:\n\n@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n\n\nyou should register a third most-specific implementation, e.g.:\n\nregister_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\nParameters\n\ntype_p (type) – A subclass of Distribution.\n\ntype_q (type) – A subclass of Distribution.\n\nTransforms\nCLASS\ntorch.distributions.transforms.AbsTransform(cache_size=0)\n[SOURCE]\n\nTransform via the mapping \n𝑦\n=\n∣\n𝑥\n∣\ny=∣x∣.\n\nCLASS\ntorch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)\n[SOURCE]\n\nTransform via the pointwise affine mapping \n𝑦\n=\nloc\n+\nscale\n×\n𝑥\ny=loc+scale×x.\n\nParameters\n\nloc (Tensor or float) – Location parameter.\n\nscale (Tensor or float) – Scale parameter.\n\nevent_dim (int) – Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.\n\nCLASS\ntorch.distributions.transforms.CatTransform(tseq, dim=0, lengths=None, cache_size=0)\n[SOURCE]\n\nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim, of length lengths[dim], in a way compatible with torch.cat().\n\nExample:\n\nx0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\nx = torch.cat([x0, x0], dim=0)\nt0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\nt = CatTransform([t0, t0], dim=0, lengths=[20, 20])\ny = t(x)\n\nCLASS\ntorch.distributions.transforms.ComposeTransform(parts, cache_size=0)\n[SOURCE]\n\nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.\n\nParameters\n\nparts (list of Transform) – A list of transforms to compose.\n\ncache_size (int) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.\n\nCLASS\ntorch.distributions.transforms.CorrCholeskyTransform(cache_size=0)\n[SOURCE]\n\nTransforms an uncontrained real vector \n𝑥\nx with length \n𝐷\n∗\n(\n𝐷\n−\n1\n)\n/\n2\nD∗(D−1)/2 into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:\n\nFirst we convert x into a lower triangular matrix in row order.\n\nFor each row \n𝑋\n𝑖\nX\ni\n\t​\n\n of the lower triangular part, we apply a signed version of class StickBreakingTransform to transform \n𝑋\n𝑖\nX\ni\n\t​\n\n into a unit Euclidean length vector using the following steps: - Scales into the interval \n(\n−\n1\n,\n1\n)\n(−1,1) domain: \n𝑟\n𝑖\n=\ntanh\n⁡\n(\n𝑋\n𝑖\n)\nr\ni\n\t​\n\n=tanh(X\ni\n\t​\n\n). - Transforms into an unsigned domain: \n𝑧\n𝑖\n=\n𝑟\n𝑖\n2\nz\ni\n\t​\n\n=r\ni\n2\n\t​\n\n. - Applies \n𝑠\n𝑖\n=\n𝑆\n𝑡\n𝑖\n𝑐\n𝑘\n𝐵\n𝑟\n𝑒\n𝑎\n𝑘\n𝑖\n𝑛\n𝑔\n𝑇\n𝑟\n𝑎\n𝑛\n𝑠\n𝑓\n𝑜\n𝑟\n𝑚\n(\n𝑧\n𝑖\n)\ns\ni\n\t​\n\n=StickBreakingTransform(z\ni\n\t​\n\n). - Transforms back into signed domain: \n𝑦\n𝑖\n=\n𝑠\n𝑖\n𝑔\n𝑛\n(\n𝑟\n𝑖\n)\n∗\n𝑠\n𝑖\ny\ni\n\t​\n\n=sign(r\ni\n\t​\n\n)∗\ns\ni\n\t​\n\n\t​\n\n.\n\nCLASS\ntorch.distributions.transforms.CumulativeDistributionTransform(distribution, cache_size=0)\n[SOURCE]\n\nTransform via the cumulative distribution function of a probability distribution.\n\nParameters\n\ndistribution (Distribution) – Distribution whose cumulative distribution function to use for the transformation.\n\nExample:\n\n# Construct a Gaussian copula from a multivariate normal.\nbase_dist = MultivariateNormal(\n    loc=torch.zeros(2),\n    scale_tril=LKJCholesky(2).sample(),\n)\ntransform = CumulativeDistributionTransform(Normal(0, 1))\ncopula = TransformedDistribution(base_dist, [transform])\n\nCLASS\ntorch.distributions.transforms.ExpTransform(cache_size=0)\n[SOURCE]\n\nTransform via the mapping \n𝑦\n=\nexp\n⁡\n(\n𝑥\n)\ny=exp(x).\n\nCLASS\ntorch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)\n[SOURCE]\n\nWrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().\n\nParameters\n\nbase_transform (Transform) – A base transform.\n\nreinterpreted_batch_ndims (int) – The number of extra rightmost dimensions to treat as dependent.\n\nCLASS\ntorch.distributions.transforms.LowerCholeskyTransform(cache_size=0)\n[SOURCE]\n\nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.\n\nThis is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.\n\nCLASS\ntorch.distributions.transforms.PositiveDefiniteTransform(cache_size=0)\n[SOURCE]\n\nTransform from unconstrained matrices to positive-definite matrices.\n\nCLASS\ntorch.distributions.transforms.PowerTransform(exponent, cache_size=0)\n[SOURCE]\n\nTransform via the mapping \n𝑦\n=\n𝑥\nexponent\ny=x\nexponent\n.\n\nCLASS\ntorch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0)\n[SOURCE]\n\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\nNote that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().\n\nParameters\n\nin_shape (torch.Size) – The input event shape.\n\nout_shape (torch.Size) – The output event shape.\n\nCLASS\ntorch.distributions.transforms.SigmoidTransform(cache_size=0)\n[SOURCE]\n\nTransform via the mapping \n𝑦\n=\n1\n1\n+\nexp\n⁡\n(\n−\n𝑥\n)\ny=\n1+exp(−x)\n1\n\t​\n\n and \n𝑥\n=\nlogit\n(\n𝑦\n)\nx=logit(y).\n\nCLASS\ntorch.distributions.transforms.SoftplusTransform(cache_size=0)\n[SOURCE]\n\nTransform via the mapping \nSoftplus\n(\n𝑥\n)\n=\nlog\n⁡\n(\n1\n+\nexp\n⁡\n(\n𝑥\n)\n)\nSoftplus(x)=log(1+exp(x)). The implementation reverts to the linear function when \n𝑥\n>\n20\nx>20.\n\nCLASS\ntorch.distributions.transforms.TanhTransform(cache_size=0)\n[SOURCE]\n\nTransform via the mapping \n𝑦\n=\ntanh\n⁡\n(\n𝑥\n)\ny=tanh(x).\n\nIt is equivalent to ` ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)]) ` However this might not be numerically stable, thus it is recommended to use TanhTransform instead.\n\nNote that one should use cache_size=1 when it comes to NaN/Inf values.\n\nCLASS\ntorch.distributions.transforms.SoftmaxTransform(cache_size=0)\n[SOURCE]\n\nTransform from unconstrained space to the simplex via \n𝑦\n=\nexp\n⁡\n(\n𝑥\n)\ny=exp(x) then normalizing.\n\nThis is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.\n\nCLASS\ntorch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0)\n[SOURCE]\n\nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().\n\nExample:\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\nt = StackTransform([ExpTransform(), identity_transform], dim=1)\ny = t(x)\n\nCLASS\ntorch.distributions.transforms.StickBreakingTransform(cache_size=0)\n[SOURCE]\n\nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.\n\nThis transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.\n\nThis is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.\n\nCLASS\ntorch.distributions.transforms.Transform(cache_size=0)\n[SOURCE]\n\nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution.\n\nCaching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:\n\ny = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n\n\nHowever the following will error when caching due to dependency reversal:\n\ny = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n\n\nDerived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().\n\nParameters\n\ncache_size (int) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.\n\nVariables\n\ndomain (Constraint) – The constraint representing valid inputs to this transform.\n\ncodomain (Constraint) – The constraint representing valid outputs to this transform which are inputs to the inverse transform.\n\nbijective (bool) – Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y).\n\nsign (int or Tensor) – For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.\n\nPROPERTY inv\n\nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t.\n\nPROPERTY sign\n\nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.\n\nlog_abs_det_jacobian(x, y)\n[SOURCE]\n\nComputes the log det jacobian log |dy/dx| given input and output.\n\nforward_shape(shape)\n[SOURCE]\n\nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape.\n\ninverse_shape(shape)\n[SOURCE]\n\nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.\n\nConstraints\n\nThe following constraints are implemented:\n\nconstraints.boolean\n\nconstraints.cat\n\nconstraints.corr_cholesky\n\nconstraints.dependent\n\nconstraints.greater_than(lower_bound)\n\nconstraints.greater_than_eq(lower_bound)\n\nconstraints.independent(constraint, reinterpreted_batch_ndims)\n\nconstraints.integer_interval(lower_bound, upper_bound)\n\nconstraints.interval(lower_bound, upper_bound)\n\nconstraints.less_than(upper_bound)\n\nconstraints.lower_cholesky\n\nconstraints.lower_triangular\n\nconstraints.multinomial\n\nconstraints.nonnegative\n\nconstraints.nonnegative_integer\n\nconstraints.one_hot\n\nconstraints.positive_integer\n\nconstraints.positive\n\nconstraints.positive_semidefinite\n\nconstraints.positive_definite\n\nconstraints.real_vector\n\nconstraints.real\n\nconstraints.simplex\n\nconstraints.symmetric\n\nconstraints.stack\n\nconstraints.square\n\nconstraints.symmetric\n\nconstraints.unit_interval\n\nCLASS\ntorch.distributions.constraints.Constraint\n[SOURCE]\n\nAbstract base class for constraints.\n\nA constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.\n\nVariables\n\nis_discrete (bool) – Whether constrained space is discrete. Defaults to False.\n\nevent_dim (int) – Number of rightmost dimensions that together define an event. The check() method will remove this many dimensions when computing validity.\n\ncheck(value)\n[SOURCE]\n\nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint.\n\ntorch.distributions.constraints.cat\n\nalias of _Cat\n\ntorch.distributions.constraints.dependent_property\n\nalias of _DependentProperty\n\ntorch.distributions.constraints.greater_than\n\nalias of _GreaterThan\n\ntorch.distributions.constraints.greater_than_eq\n\nalias of _GreaterThanEq\n\ntorch.distributions.constraints.independent\n\nalias of _IndependentConstraint\n\ntorch.distributions.constraints.integer_interval\n\nalias of _IntegerInterval\n\ntorch.distributions.constraints.interval\n\nalias of _Interval\n\ntorch.distributions.constraints.half_open_interval\n\nalias of _HalfOpenInterval\n\ntorch.distributions.constraints.less_than\n\nalias of _LessThan\n\ntorch.distributions.constraints.multinomial\n\nalias of _Multinomial\n\ntorch.distributions.constraints.stack\n\nalias of _Stack\n\nConstraint Registry\n\nPyTorch provides two global ConstraintRegistry objects that link Constraint objects to Transform objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.\n\nbiject_to(constraint) looks up a bijective Transform from constraints.real to the given constraint. The returned transform is guaranteed to have .bijective = True and should implement .log_abs_det_jacobian().\n\ntransform_to(constraint) looks up a not-necessarily bijective Transform from constraints.real to the given constraint. The returned transform is not guaranteed to implement .log_abs_det_jacobian().\n\nThe transform_to() registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s .arg_constraints dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:\n\nloc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n\n\nThe biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant.:\n\ndist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n\n\nNOTE\n\nAn example where transform_to and biject_to differ is constraints.simplex: transform_to(constraints.simplex) returns a SoftmaxTransform that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, biject_to(constraints.simplex) returns a StickBreakingTransform that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.\n\nThe biject_to and transform_to objects can be extended by user-defined constraints and transforms using their .register() method either as a function on singleton constraints:\n\ntransform_to.register(my_constraint, my_transform)\n\n\nor as a decorator on parameterized constraints:\n\n@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n\n\nYou can create your own registry by creating a new ConstraintRegistry object.\n\nCLASS\ntorch.distributions.constraint_registry.ConstraintRegistry\n[SOURCE]\n\nRegistry to link constraints to transforms.\n\nregister(constraint, factory=None)\n[SOURCE]\n\nRegisters a Constraint subclass in this registry. Usage:\n\n@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n\nParameters\n\nconstraint (subclass of Constraint) – A subclass of Constraint, or a singleton object of the desired class.\n\nfactory (Callable) – A callable that inputs a constraint object and returns a Transform object.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nProbability distributions - torch.distributions\nScore function\nPathwise derivative\nDistribution\nExponentialFamily\nBernoulli\nBeta\nBinomial\nCategorical\nCauchy\nChi2\nContinuousBernoulli\nDirichlet\nExponential\nFisherSnedecor\nGamma\nGeometric\nGumbel\nHalfCauchy\nHalfNormal\nIndependent\nKumaraswamy\nLKJCholesky\nLaplace\nLogNormal\nLowRankMultivariateNormal\nMixtureSameFamily\nMultinomial\nMultivariateNormal\nNegativeBinomial\nNormal\nOneHotCategorical\nPareto\nPoisson\nRelaxedBernoulli\nLogitRelaxedBernoulli\nRelaxedOneHotCategorical\nStudentT\nTransformedDistribution\nUniform\nVonMises\nWeibull\nWishart\nKL Divergence\nTransforms\nConstraints\nConstraint Registry\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.fx — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/fx.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.fx\nShortcuts\nTORCH.FX\nOverview\n\nFX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action:\n\nimport torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %param : [num_users=1] = get_attr[target=param]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n    return clamp\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add = x + param;  x = param = None\n    linear = self.linear(add);  add = None\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n    return clamp\n\"\"\"\n\n\nThe symbolic tracer performs “symbolic execution” of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation.\n\nThe intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied.\n\nPython code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph’s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph.\n\nTaken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!\n\nSeveral example transformations can be found at the examples repository.\n\nWriting Transformations\n\nWhat is an FX transform? Essentially, it’s a function that looks like this.\n\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n\n\nYour transform will take in a torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module – you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability.\n\nNOTE\n\nIt is also possible to modify an existing GraphModule instead of creating a new one, like so:\n\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n\n\nNote that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph.\n\nGiven that you’ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph.\n\nA Quick Primer on Graphs\n\nFull treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is:\n\nWhat are the inputs to the method?\n\nWhat are the operations that run inside the method?\n\nWhat is the output (i.e. return) value from the method?\n\nAll three of these concepts are represented with Node instances. Let’s see what we mean by that with a short example:\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n\n\nHere we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph:\n\nopcode\n\n\t\n\nname\n\n\t\n\ntarget\n\n\t\n\nargs\n\n\t\n\nkwargs\n\n\n\n\nplaceholder\n\n\t\n\nx\n\n\t\n\nx\n\n\t\n\n()\n\n\t\n\n{}\n\n\n\n\nget_attr\n\n\t\n\nlinear_weight\n\n\t\n\nlinear.weight\n\n\t\n\n()\n\n\t\n\n{}\n\n\n\n\ncall_function\n\n\t\n\nadd_1\n\n\t\n\n<built-in function add>\n\n\t\n\n(x, linear_weight)\n\n\t\n\n{}\n\n\n\n\ncall_module\n\n\t\n\nlinear_1\n\n\t\n\nlinear\n\n\t\n\n(add_1,)\n\n\t\n\n{}\n\n\n\n\ncall_method\n\n\t\n\nrelu_1\n\n\t\n\nrelu\n\n\t\n\n(linear_1,)\n\n\t\n\n{}\n\n\n\n\ncall_function\n\n\t\n\nsum_1\n\n\t\n\n<built-in method sum …>\n\n\t\n\n(relu_1,)\n\n\t\n\n{‘dim’: -1}\n\n\n\n\ncall_function\n\n\t\n\ntopk_1\n\n\t\n\n<built-in method topk …>\n\n\t\n\n(sum_1, 3)\n\n\t\n\n{}\n\n\n\n\noutput\n\n\t\n\noutput\n\n\t\n\noutput\n\n\t\n\n(topk_1,)\n\n\t\n\n{}\n\nWe can use this information to answer the questions we posed above.\n\nWhat are the inputs to the method? In FX, method inputs are specified via special placeholder nodes. In this case, we have a single placeholder node with a target of x, meaning we have a single (non-self) argument named x.\n\nWhat are the operations within the method? The get_attr, call_function, call_module, and call_method nodes represent the operations in the method. A full treatment of the semantics of all of these can be found in the Node documentation.\n\nWhat is the return value of the method? The return value in a Graph is specified by a special output node.\n\nGiven that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph.\n\nGraph Manipulation\nDirect Graph Manipulation\n\nOne approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let’s say we desire to replace torch.add() calls with torch.mul() calls.\n\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n\n\nWe can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below.\n\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n\n\nFor simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter.\n\nSubgraph Rewriting With replace_pattern()\n\nFX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a “find/replace” tool for editing Graphs. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.\n\nGraph Manipulation Examples\n\nReplace one op\n\nConv/Batch Norm fusion\n\nreplace_pattern: Basic usage\n\nQuantization\n\nInvert Transformation\n\nProxy/Retracing\n\nAnother way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let’s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph.\n\nTo use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arguments. These Proxy objects will capture the operations that are performed on them and append them to the Graph.\n\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n\n\nIn addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling Proxy we also passed a tracer pointing to the underlying variable graph. This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to Proxy does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using Proxy especially when the underlying operators can not be safely assumed to be unary.\n\nA worked example of using Proxys for Graph manipulation can be found here.\n\nThe Interpreter Pattern\n\nA useful code organizational pattern in FX is to loop over all the Nodes in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like:\n\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n\n\nAs you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides.\n\nIn addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods.\n\nExamples of the Interpreter Pattern\n\nShape Propagation\n\nPerformance Profiler\n\nDebugging\nIntroduction\n\nOften in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.\n\nIf you’re not familiar with debuggers, please see the auxiliary section Available Debuggers.\n\nCommon Pitfalls in Transform Authoring\n\nNondeterministic set iteration order. In Python, the set datatype is unordered. Using set to contain collections of objects like Nodes, for example, can cause unexpected nondeterminism. An example is iterating over a set of Nodes to insert them into a Graph. Because the set data type is unordered, the ordering of the operations in the output program will be nondeterministic and can change across program invocations. The recommended alternative is to use a dict data type, which is insertion ordered as of Python 3.7 (and as of cPython 3.6). A dict can be used equivalently to a set by storing values to be deduplicated in the keys of the dict.\n\nChecking Correctness of Modules\n\nBecause the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let’s use an example:\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n\n\nHere, we’ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:\n\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n\n\nThis is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.\n\nDebugging the Generated Code\n\nBecause FX generates the forward() function on GraphModules, using traditional debugging techniques like print statements or pdb is not as straightforward. Luckily, we have several techniques we can use for debugging the generated code.\n\nUse pdb\n\nInvoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked.\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n\nPrint the Generated Code\n\nIf you’d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there.\n\n# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n\nUse the to_folder Function From GraphModule\n\nGraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder.\n\nm = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n\n\nAfter running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code.\n\nDebugging the Transformation\n\nNow that we’ve identified that a transformation is creating incorrect code, it’s time to debug the transformation itself. First, we’ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module:\n\n# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add = x + y;  x = y = None\n    return add\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %y : [num_users=1] = placeholder[target=y]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n    return add\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\"\"\"\n\n\nUsing the utility functions above, we can compare our traced Module before and after we’ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it’s still not clear what’s going wrong, a debugger like pdb can be a good next step.\n\nGoing off of the example above, consider the following code:\n\n# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n\n\nUsing the above example, let’s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what’s happening during the transform by breaking on transform_graph(traced), then pressing s to “step into” the call to transform_graph(traced).\n\nWe may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node’s input_nodes and users.)\n\nAvailable Debuggers\n\nThe most common Python debugger is pdb. You can start your program in “debug mode” with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It’s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you’re running your file in debug mode, you can step through the code and examine your program’s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython’s “Python Debugging With Pdb”.\n\nIDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View → Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).\n\nLimitations of Symbolic Tracing\n\nFX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance).\n\nAlthough symbolic tracing works for most neural net code, it has some limitations.\n\nDynamic Control Flow\n\nThe main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program.\n\nFor example, let’s examine the following program:\n\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n\n\nThe condition to the if statement relies on the value of x.sum(), which relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens.\n\nStatic Control Flow\n\nOn the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example:\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n\n\nThe if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.\n\nMany instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by binding concrete values to arguments during symbolic tracing:\n\ndef f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n\n\nIn the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them.\n\nNon-torch Functions\n\nFX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example:\n\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n\n\nThe error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API:\n\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n\nCustomizing Tracing with the Tracer class\n\nThe Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so:\n\nclass MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n\nLeaf Modules\n\nLeaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example:\n\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n\n\nThe set of leaf modules can be customized by overriding Tracer.is_leaf_module().\n\nMiscellanea\n\nTensor constructors (e.g. torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_coo_tensor) are currently not traceable.\n\nThe deterministic constructors (zeros, ones) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, ones_like or zeros_like may be a viable substitute.\n\nNondeterministic constructors (rand, randn) will have a single random value embedded in the trace. This is likely not the intended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead.\n\n@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)\n\n\nThis behavior may be fixed in a future release.\n\nType annotations\n\nPython 3-style type annotations (e.g. func(x : torch.Tensor, y : int) -> torch.Tensor) are supported and will be preserved by symbolic tracing.\n\nPython 2-style comment type annotations # type: (torch.Tensor, int) -> torch.Tensor are not currently supported.\n\nAnnotations on local names within a function are not currently supported.\n\nGotcha around training flag and submodules\n\nWhen using functionals like torch.nn.functional.dropout, it will be common for the training argument to be passed in as self.training. During FX tracing, this will likely be baked in as a constant value.\n\nimport torch\nimport torch.fx\n\nclass DropoutRepro(torch.nn.Module):\n  def forward(self, x):\n    return torch.nn.functional.dropout(x, training=self.training)\n\n\ntraced = torch.fx.symbolic_trace(DropoutRepro())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n  dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n  return dropout\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\"\"\"\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 15 / 15 (100.0%)\nGreatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n\"\"\"\n\n\nHowever, when the standard nn.Dropout() submodule is used, the training flag is encapsulated and–because of the preservation of the nn.Module object model–can be changed.\n\nclass DropoutRepro2(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.drop = torch.nn.Dropout()\n\n  def forward(self, x):\n    return self.drop(x)\n\ntraced = torch.fx.symbolic_trace(DropoutRepro2())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n  drop = self.drop(x);  x = None\n  return drop\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\n\nBecause of this difference, consider marking modules that interact with the training flag dynamically as leaf modules.\n\nAPI Reference\ntorch.fx.symbolic_trace(root, concrete_args=None)\n[SOURCE]\n\nSymbolic tracing API\n\nGiven an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.\n\nconcrete_args allows you to partially specialize your function, whether it’s to remove control flow or data structures.\n\nFor example:\n\ndef f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2\n\n\nFX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this:\n\nf = fx.symbolic_trace(f, concrete_args={'b': False})\nassert f(3, False)  == 6\n\n\nNote that although you can still pass in different values of b, they will be ignored.\n\nWe can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn’t be specialized. For example:\n\ndef f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7\n\nParameters\n\nroot (Union[torch.nn.Module, Callable]) – Module or function to be traced and converted into a Graph representation.\n\nconcrete_args (Optional[Dict[str, any]]) – Inputs to be partially specialized\n\nReturns\n\na Module created from the recorded operations from root.\n\nReturn type\n\nGraphModule\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ntorch.fx.wrap(fn_or_name)\n[SOURCE]\n\nThis function can be called at module-level scope to register fn_or_name as a “leaf function”. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through:\n\n# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n\n\nThis function can also equivalently be used as a decorator:\n\n# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n\n\nA wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through.\n\nParameters\n\nfn_or_name (Union[str, Callable]) – The function or name of the global function to insert into the graph when it’s called\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nCLASS\ntorch.fx.GraphModule(*args, **kwargs)\n[SOURCE]\n\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.\n\nWARNING\n\nWhen graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\n__init__(root, graph, class_name='GraphModule')\n[SOURCE]\n\nConstruct a GraphModule.\n\nParameters\n\nroot (Union[torch.nn.Module, Dict[str, Any]) – root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph’s Nodes’ target field will be copied over from the respective place within root’s Module hierarchy into the GraphModule’s module hierarchy. In the case that root is a dict, the qualified name found in a Node’s target will be looked up directly in the dict’s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule’s module hierarchy.\n\ngraph (Graph) – graph contains the nodes this GraphModule should use for code generation\n\nclass_name (str) – name denotes the name of this GraphModule for debugging purposes. If it’s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root’s original name or a name that makes sense within the context of your transform.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nadd_submodule(target, m)\n[SOURCE]\n\nAdds the given submodule to self.\n\nThis installs empty Modules where none exist yet if they are subpaths of target.\n\nParameters\n\ntarget (str) – The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)\n\nm (Module) – The submodule itself; the actual object we want to install in the current Module\n\nReturns\n\nWhether or not the submodule could be inserted. For\n\nthis method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute)\n\nReturn type\n\nbool\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY code: STR\n\nReturn the Python code generated from the Graph underlying this GraphModule.\n\ndelete_all_unused_submodules()\n[SOURCE]\n\nDeletes all unused submodules from self.\n\nA Module is considered “used” if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node\n\nThis method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ndelete_submodule(target)\n[SOURCE]\n\nDeletes the given submodule from self.\n\nThe module will not be deleted if target is not a valid target.\n\nParameters\n\ntarget (str) – The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)\n\nReturns\n\nWhether or not the target string referenced a\n\nsubmodule we want to delete. A return value of False means that the target was not a valid reference to a submodule.\n\nReturn type\n\nbool\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY graph: GRAPH\n\nReturn the Graph underlying this GraphModule\n\nprint_readable(print_output=True)\n[SOURCE]\n\nReturn the Python code generated for current GraphModule and its children GraphModules\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nrecompile()\n[SOURCE]\n\nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nPythonCode\n\nto_folder(folder, module_name='FxModule')\n[SOURCE]\nDumps out module to folder with module_name so that it can be\n\nimported with from <folder> import <module_name>\n\nArgs:\n\nfolder (Union[str, os.PathLike]): The folder to write the code out to\n\nmodule_name (str): Top-level name to use for the Module while\n\nwriting out the code\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nCLASS\ntorch.fx.Graph(owning_module=None, tracer_cls=None, tracer_extras=None)\n[SOURCE]\n\nGraph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function.\n\nFor example, the following code\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\n\nWill produce the following Graph:\n\nprint(gm.graph)\n\ngraph(x):\n    %linear_weight : [num_users=1] = self.linear.weight\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n\n\nFor the semantics of operations represented in the Graph, please see Node.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\n__init__(owning_module=None, tracer_cls=None, tracer_extras=None)\n[SOURCE]\n\nConstruct an empty Graph.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_function(the_function, args=None, kwargs=None, type_expr=None)\n[SOURCE]\n\nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function.\n\nParameters\n\nthe_function (Callable[..., Any]) – The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces.\n\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed to the called function.\n\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed to the called function\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nReturns\n\nThe newly created and inserted call_function node.\n\nReturn type\n\nNode\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node().\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_method(method_name, args=None, kwargs=None, type_expr=None)\n[SOURCE]\n\nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.\n\nParameters\n\nmethod_name (str) – The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name.\n\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed to the called method. Note that this should include a self argument.\n\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed to the called method\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nReturns\n\nThe newly created and inserted call_method node.\n\nReturn type\n\nNode\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node().\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_module(module_name, args=None, kwargs=None, type_expr=None)\n[SOURCE]\n\nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.\n\nParameters\n\nmodule_name (str) – The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module.\n\nargs (Optional[Tuple[Argument, ...]]) – The positional arguments to be passed to the called method. Note that this should not include a self argument.\n\nkwargs (Optional[Dict[str, Argument]]) – The keyword arguments to be passed to the called method\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nReturns\n\nThe newly-created and inserted call_module node.\n\nReturn type\n\nNode\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node().\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None)\n[SOURCE]\n\nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().\n\nParameters\n\nop (str) – the opcode for this Node. One of ‘call_function’, ‘call_method’, ‘get_attr’, ‘call_module’, ‘placeholder’, or ‘output’. The semantics of these opcodes are described in the Graph docstring.\n\nargs (Optional[Tuple[Argument, ...]]) – is a tuple of arguments to this node.\n\nkwargs (Optional[Dict[str, Argument]]) – the kwargs of this Node\n\nname (Optional[str]) – an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code.\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nReturns\n\nThe newly-created and inserted node.\n\nReturn type\n\nNode\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\neliminate_dead_code()\n[SOURCE]\n\nRemove all dead code from the graph, based on each node’s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling.\n\nReturns\n\nWhether the graph was changed as a result of the pass.\n\nReturn type\n\nbool\n\nExample:\n\nBefore dead code is eliminated, a from a = x + 1 below has no users and thus can be eliminated from the graph without having an effect.\n\ndef forward(self, x):\n    a = x + 1\n    return x + self.attr_1\n\n\nAfter dead code is eliminated, a = x + 1 has been removed, and the rest of forward remains.\n\ndef forward(self, x):\n    return x + self.attr_1\n\n\nWARNING\n\nDead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nerase_node(to_erase)\n[SOURCE]\n\nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.\n\nParameters\n\nto_erase (Node) – The Node to erase from the Graph.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nget_attr(qualified_name, type_expr=None)\n[SOURCE]\n\nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.\n\nParameters\n\nqualified_name (str) – the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name.\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nReturns\n\nThe newly-created and inserted get_attr node.\n\nReturn type\n\nNode\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ngraph_copy(g, val_map, return_output_node=False)\n[SOURCE]\n\nCopy all nodes from a given graph into self.\n\nParameters\n\ng (Graph) – The source graph from which to copy Nodes.\n\nval_map (Dict[Node, Node]) – a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.\n\nReturns\n\nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.\n\nReturn type\n\nOptional[Union[Tuple[Any, …], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ninserting_after(n=None)\n[SOURCE]\nSet the point at which create_node and companion methods will insert into the graph.\n\nWhen used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:\n\nwith g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n\n\nArgs:\n\nn (Optional[Node]): The node before which to insert. If None this will insert after\n\nthe beginning of the entire graph.\n\nReturns:\n\nA resource manager that will restore the insert point on __exit__.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ninserting_before(n=None)\n[SOURCE]\nSet the point at which create_node and companion methods will insert into the graph.\n\nWhen used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:\n\nwith g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n\n\nArgs:\n\nn (Optional[Node]): The node before which to insert. If None this will insert before\n\nthe beginning of the entire graph.\n\nReturns:\n\nA resource manager that will restore the insert point on __exit__.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nlint()\n[SOURCE]\n\nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nnode_copy(node, arg_transform=<function Graph.<lambda>>)\n[SOURCE]\n\nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:\n\n# Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n\nParameters\n\nnode (Node) – The node to copy into self.\n\narg_transform (Callable[[Node], Argument]) – A function that transforms Node arguments in node’s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.\n\nReturn type\n\nNode\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY nodes: _NODE_LIST\n\nGet the list of Nodes that constitute this Graph.\n\nNote that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.\n\nReturns\n\nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.\n\non_generate_code(make_transformer)\n[SOURCE]\n\nRegister a transformer function when python code is generated\n\nArgs:\nmake_transformer (Callable[[Optional[TransformCodeFunc]], TransformCodeFunc]):\n\na function that returns a code transformer to be registered. This function is called by on_generate_code to obtain the code transformer.\n\nThis function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.\n\nReturns:\n\na context manager that when used in a with statement, to automatically restore the previously registered code transformer.\n\nExample:\n\ngm: fx.GraphModule = ...\n\n# This is a code transformer we want to register. This code\n# transformer prepends a pdb import and trace statement at the very\n# beginning of the generated torch.fx code to allow for manual\n# debugging with the PDB library.\ndef insert_pdb(body):\n    return [\"import pdb; pdb.set_trace()\\n\", *body]\n\n# Registers `insert_pdb`, and overwrites the current registered\n# code transformer (given by `_` to the lambda):\ngm.graph.on_generate_code(\n    lambda _: insert_pdb\n)\n\n# Or alternatively, registers a code transformer which first\n# runs `body` through existing registered transformer, then\n# through `insert_pdb`:\ngm.graph.on_generate_code(\n    lambda current_trans: (\n        lambda body: insert_pdb(\n            current_trans(body) if current_trans\n            else body\n        )\n    )\n)\n\ngm.recompile()\ngm(*inputs)  # drops into pdb\n\n\nThis function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:\n\n# ... continue from previous example\n\nwith gm.graph.on_generate_code(lambda _: insert_pdb):\n    # do more stuff with `gm`...\n    gm.recompile()\n    gm(*inputs)  # drops into pdb\n\n# now previous code transformer is restored (but `gm`'s code with pdb\n# remains - that means you can run `gm` with pdb here too, until you\n# run next `recompile()`).\n\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\noutput(result, type_expr=None)\n[SOURCE]\n\nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.\n\nParameters\n\nresult (Argument) – The value to be returned.\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have.\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nplaceholder(name, type_expr=None, default_value)\n[SOURCE]\n\nInsert a placeholder node into the Graph. A placeholder represents a function input.\n\nParameters\n\nname (str) – A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents.\n\ntype_expr (Optional[Any]) – an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).\n\ndefault_value (Any) – The default value this function argument should take on. NOTE: to allow for None as a default value, inspect.Signature.empty should be passed as this argument to specify that the parameter does _not_ have a default value.\n\nReturn type\n\nNode\n\nNOTE\n\nThe same insertion point and type expression rules apply for this method as Graph.create_node.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nprint_tabular()\n[SOURCE]\n\nPrints the intermediate representation of the graph in tabular format. Note that this API requires the tabulate module to be installed.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nprocess_inputs(*args)\n[SOURCE]\n\nProcesses args so that they can be passed to the FX graph.\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nprocess_outputs(out)\n[SOURCE]\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\npython_code(root_module, *, verbose=False)\n[SOURCE]\n\nTurn this Graph into valid Python code.\n\nParameters\n\nroot_module (str) – The name of the root module on which to look-up qualified name targets. This is usually ‘self’.\n\nReturns\n\nsrc: the Python source code representing the object globals: a dictionary of global names in src -> the objects that they reference.\n\nReturn type\n\nA PythonCode object, consisting of two fields\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nset_codegen(codegen)\n[SOURCE]\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nCLASS\ntorch.fx.Node(graph, name, op, target, args, kwargs, return_type=None)\n[SOURCE]\n\nNode is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:\n\nplaceholder represents a function input. The name attribute specifies the name this value will take on. target is similarly the name of the argument. args holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. kwargs is don’t-care. Placeholders correspond to the function parameters (e.g. x) in the graph printout.\n\nget_attr retrieves a parameter from the module hierarchy. name is similarly the name the result of the fetch is assigned to. target is the fully-qualified name of the parameter’s position in the module hierarchy. args and kwargs are don’t-care\n\ncall_function applies a free function to some values. name is similarly the name of the value to assign to. target is the function to be applied. args and kwargs represent the arguments to the function, following the Python calling convention\n\ncall_module applies a module in the module hierarchy’s forward() method to given arguments. name is as previous. target is the fully-qualified name of the module in the module hierarchy to call. args and kwargs represent the arguments to invoke the module on, excluding the self argument.\n\ncall_method calls a method on a value. name is as similar. target is the string name of the method to apply to the self argument. args and kwargs represent the arguments to invoke the module on, including the self argument\n\noutput contains the output of the traced function in its args[0] attribute. This corresponds to the “return” statement in the Graph printout.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY all_input_nodes: LIST[NODE]\n\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.\n\nReturns\n\nList of Nodes that appear in the args and kwargs of this Node, in that order.\n\nappend(x)\n[SOURCE]\n\nInsert x after this node in the list of nodes in the graph. Equivalent to self.next.prepend(x)\n\nParameters\n\nx (Node) – The node to put after this node. Must be a member of the same graph.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY args: TUPLE[OPTIONAL[UNION[TUPLE[ANY, ...], LIST[ANY], DICT[STR, ANY], SLICE, RANGE, NODE, STR, INT, FLOAT, BOOL, COMPLEX, DTYPE, TENSOR, DEVICE, MEMORY_FORMAT, LAYOUT, OPOVERLOAD]], ...]\n\nThe tuple of arguments to this Node. The interpretation of arguments depends on the node’s opcode. See the Node docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.\n\nformat_node(placeholder_names=None, maybe_return_typename=None)\n[SOURCE]\n\nReturn a descriptive string representation of self.\n\nThis method can be used with no arguments as a debugging utility.\n\nThis function is also used internally in the __str__ method of Graph. Together, the strings in placeholder_names and maybe_return_typename make up the signature of the autogenerated forward function in this Graph’s surrounding GraphModule. placeholder_names and maybe_return_typename should not be used otherwise.\n\nParameters\n\nplaceholder_names (Optional[List[str]]) – A list that will store formatted strings representing the placeholders in the generated forward function. Internal use only.\n\nmaybe_return_typename (Optional[List[str]]) – A single-element list that will store a formatted string representing the output of the generated forward function. Internal use only.\n\nReturns\n\nIf 1) we’re using format_node as an internal helper\n\nin the __str__ method of Graph, and 2) self is a placeholder Node, return None. Otherwise, return a descriptive string representation of the current Node.\n\nReturn type\n\nstr\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nis_impure()\n[SOURCE]\n\nReturns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure.\n\nReturns\n\nIf the op is impure or not.\n\nReturn type\n\nbool\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nPROPERTY kwargs: DICT[STR, OPTIONAL[UNION[TUPLE[ANY, ...], LIST[ANY], DICT[STR, ANY], SLICE, RANGE, NODE, STR, INT, FLOAT, BOOL, COMPLEX, DTYPE, TENSOR, DEVICE, MEMORY_FORMAT, LAYOUT, OPOVERLOAD]]]\n\nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node’s opcode. See the Node docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.\n\nPROPERTY next: NODE\n\nReturns the next Node in the linked list of Nodes.\n\nReturns\n\nThe next Node in the linked list of Nodes.\n\nnormalized_arguments(root, arg_types=None, kwarg_types=None, normalize_to_only_use_kwargs=False)\n[SOURCE]\n\nReturns normalized arguments to Python targets. This means that args/kwargs will be matched up to the module/functional’s signature and return exclusively kwargs in positional order if normalize_to_only_use_kwargs is true. Also populates default values. Does not support positional-only parameters or varargs parameters.\n\nSupports module calls.\n\nMay require arg_types and kwarg_types in order to disambiguate overloads.\n\nParameters\n\nroot (torch.nn.Module) – Module upon which to resolve module targets.\n\narg_types (Optional[Tuple[Any]]) – Tuple of arg types for the args\n\nkwarg_types (Optional[Dict[str, Any]]) – Dict of arg types for the kwargs\n\nnormalize_to_only_use_kwargs (bool) – Whether to normalize to only use kwargs.\n\nReturns\n\nReturns NamedTuple ArgsKwargsPair, or None if not successful.\n\nReturn type\n\nOptional[ArgsKwargsPair]\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nprepend(x)\n[SOURCE]\n\nInsert x before this node in the list of nodes in the graph. Example:\n\nBefore: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n\nParameters\n\nx (Node) – The node to put before this node. Must be a member of the same graph.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY prev: NODE\n\nReturns the previous Node in the linked list of Nodes.\n\nReturns\n\nThe previous Node in the linked list of Nodes.\n\nreplace_all_uses_with(replace_with, delete_user_cb=<function Node.<lambda>>, *, propagate_meta=False)\n[SOURCE]\n\nReplace all uses of self in the Graph with the Node replace_with.\n\nParameters\n\nreplace_with (Node) – The node to replace all uses of self with.\n\ndelete_user_cb (Callable) – Callback that is called to determine whether a given user of the self node should be removed.\n\npropagate_meta (bool) – Whether or not to copy all properties on the .meta field of the original node onto the replacement node. For safety, this is only valid to do if the replacement node doesn’t already have an existing .meta field.\n\nReturns\n\nThe list of Nodes on which this change was made.\n\nReturn type\n\nList[Node]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nreplace_input_with(old_input, new_input)\n[SOURCE]\n\nLoop through input nodes of self, and replace all instances of old_input with new_input.\n\nParameters\n\nold_input (Node) – The old input node to be replaced.\n\nnew_input (Node) – The new input node to replace old_input.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nPROPERTY stack_trace: OPTIONAL[STR]\n\nReturn the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by Tracer.create_proxy. To record stack traces during tracing for debug purposes, set record_stack_traces = True on the Tracer instance. When traced with dynamo, this property will be populated by default by OutputGraph.create_proxy.\n\nstack_trace would have the innermost frame at the end of the string.\n\nupdate_arg(idx, arg)\n[SOURCE]\n\nUpdate an existing positional argument to contain the new value arg. After calling, self.args[idx] == arg.\n\nParameters\n\nidx (int) – The index into self.args of the element to update\n\narg (Argument) – The new argument value to write into args\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nupdate_kwarg(key, arg)\n[SOURCE]\n\nUpdate an existing keyword argument to contain the new value arg. After calling, self.kwargs[key] == arg.\n\nParameters\n\nkey (str) – The key in self.kwargs of the element to update\n\narg (Argument) – The new argument value to write into kwargs\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nCLASS\ntorch.fx.Tracer(autowrap_modules=(math,), autowrap_functions=())\n[SOURCE]\n\nTracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m).\n\nTracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_module(m, forward, args, kwargs)\n[SOURCE]\n\nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.\n\nBy default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.\n\nThis method can be overridden to–for example–create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries.\n\nParameters\n\nm (Module) – The module for which a call is being emitted\n\nforward (Callable) – The forward() method of the Module to be invoked\n\nargs (Tuple) – args of the module callsite\n\nkwargs (Dict) – kwargs of the module callsite\n\nReturns\n\nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncreate_arg(a)\n[SOURCE]\n\nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.\n\nBy default, the behavior includes:\n\nIterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements.\n\nGiven a Proxy object, return a reference to the underlying IR Node\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nFor a Parameter, emit a get_attr node referring to that Parameter\n\nFor a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.\n\nThis method can be overridden to support more types.\n\nParameters\n\na (Any) – The value to be emitted as an Argument in the Graph.\n\nReturns\n\nThe value a converted into the appropriate Argument\n\nReturn type\n\nOptional[Union[Tuple[Any, …], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncreate_args_for_root(root_fn, is_module, concrete_args=None)\n[SOURCE]\n\nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root’s signature and emits those nodes accordingly, also supporting *args and **kwargs.\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\ncreate_node(kind, target, args, kwargs, name=None, type_expr=None)\n\nInserts a graph node given target, args, kwargs, and name.\n\nThis method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nNode\n\ncreate_proxy(kind, target, args, kwargs, name=None, type_expr=None, proxy_factory_fn=None)\n\nCreate a Node from the given arguments, then return the Node wrapped in a Proxy object.\n\nIf kind = ‘placeholder’, then we’re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the args tuple. args is otherwise empty for placeholder Nodes.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ngetattr(attr, attr_val, parameter_proxy_cache)\n[SOURCE]\n\nMethod that specifies the behavior of this Tracer when we call getattr on a call to an nn.Module instance.\n\nBy default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the parameter_proxy_cache, so that future calls will reuse the proxy rather than creating a new one.\n\nThis method can be overridden to –for example– not return proxies when querying parameters.\n\nParameters\n\nattr (str) – The name of the attribute being queried\n\nattr_val (Any) – The value of the attribute\n\nparameter_proxy_cache (Dict[str, Any]) – A cache of attr names to proxies\n\nReturns\n\nThe return value from the getattr call.\n\nWARNING\n\nThis API is experimental and is NOT backward-compatible.\n\nis_leaf_module(m, module_qualified_name)\n[SOURCE]\n\nA method to specify whether a given nn.Module is a “leaf” module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.\n\nParameters\n\nm (Module) – The module being queried about\n\nmodule_qualified_name (str) – The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.\n\nReturn type\n\nbool\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\niter(obj)\nCalled when a proxy object is being iterated over, such as\n\nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nIterator\n\nkeys(obj)\nCalled when a proxy object is has the keys() method called.\n\nThis is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nAny\n\npath_of_module(mod)\n[SOURCE]\n\nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string “foo.bar”.\n\nParameters\n\nmod (str) – The Module to retrieve the qualified name for.\n\nReturn type\n\nstr\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nproxy(node)\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nProxy\n\nto_bool(obj)\nCalled when a proxy object is being converted to a boolean, such as\n\nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nbool\n\ntrace(root, concrete_args=None)\n[SOURCE]\n\nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.\n\nNote that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.\n\nParameters\n\nroot (Union[Module, Callable]) – Either a Module or a function to be traced through. Backwards-compatibility for this parameter is guaranteed.\n\nconcrete_args (Optional[Dict[str, any]]) – Concrete arguments that should not be treated as Proxies. This parameter is experimental and its backwards-compatibility is NOT guaranteed.\n\nReturns\n\nA Graph representing the semantics of the passed-in root.\n\nReturn type\n\nGraph\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nCLASS\ntorch.fx.Proxy(node, tracer=None)\n[SOURCE]\n\nProxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph.\n\nIf you’re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph.\n\nProxy objects cannot be iterated. In other words, the symbolic tracer will throw an error if a Proxy is used in a loop or as an *args/**kwargs function argument.\n\nThere are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use fx.wrap on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:\n\nfor i in range(self.some_hyperparameter):\n    indexed_item = proxied_value[i]\n\n\nFor a more detailed description into the Proxy internals, check out the “Proxy” section in torch/fx/OVERVIEW.md\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nCLASS\ntorch.fx.Interpreter(module, garbage_collect_values=True)\n[SOURCE]\n\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.\n\nMethods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:\n\nrun()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n\n\nExample\n\nSuppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so:\n\nclass NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_close(result, torch.neg(input).sigmoid())\n\nParameters\n\nmodule (GraphModule) – The module to be executed\n\ngarbage_collect_values (bool) – Whether to delete values after their last use within the Module’s execution. This ensures optimal memory usage during execution. This can be disabled to, for example, examine all of the intermediate values in the execution by looking at the Interpreter.env attribute.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nboxed_run(args_list)\n[SOURCE]\n\nRun module via interpretation and return the result. This uses the “boxed” calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_function(target, args, kwargs)\n[SOURCE]\n\nExecute a call_function node and return the result.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturn type\n\nAny\n\nReturn\n\nAny: The value returned by the function invocation\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_method(target, args, kwargs)\n[SOURCE]\n\nExecute a call_method node and return the result.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturn type\n\nAny\n\nReturn\n\nAny: The value returned by the method invocation\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_module(target, args, kwargs)\n[SOURCE]\n\nExecute a call_module node and return the result.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturn type\n\nAny\n\nReturn\n\nAny: The value returned by the module invocation\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nfetch_args_kwargs_from_env(n)\n[SOURCE]\n\nFetch the concrete values of args and kwargs of node n from the current execution environment.\n\nParameters\n\nn (Node) – The node for which args and kwargs should be fetched.\n\nReturns\n\nargs and kwargs with concrete values for n.\n\nReturn type\n\nTuple[Tuple, Dict]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nfetch_attr(target)\n[SOURCE]\n\nFetch an attribute from the Module hierarchy of self.module.\n\nParameters\n\ntarget (str) – The fully-qualified name of the attribute to fetch\n\nReturns\n\nThe value of the attribute.\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nget_attr(target, args, kwargs)\n[SOURCE]\n\nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturns\n\nThe value of the attribute that was retrieved\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nmap_nodes_to_values(args, n)\n[SOURCE]\n\nRecursively descend through args and look up the concrete value for each Node in the current execution environment.\n\nParameters\n\nargs (Argument) – Data structure within which to look up concrete values\n\nn (Node) – Node to which args belongs. This is only used for error reporting.\n\nReturn type\n\nOptional[Union[Tuple[Any, …], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\noutput(target, args, kwargs)\n[SOURCE]\n\nExecute an output node. This really just retrieves the value referenced by the output node and returns it.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturns\n\nThe return value referenced by the output node\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nplaceholder(target, args, kwargs)\n[SOURCE]\n\nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturns\n\nThe argument value that was retrieved.\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nrun(*args, initial_env=None, enable_io_processing=True)\n[SOURCE]\n\nRun module via interpretation and return the result.\n\nParameters\n\n*args – The arguments to the Module to run, in positional order\n\ninitial_env (Optional[Dict[Node, Any]]) – An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.\n\nenable_io_processing (bool) – If true, we process the inputs and outputs with graph’s process_inputs and process_outputs function first before using them.\n\nReturns\n\nThe value returned from executing the Module\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nrun_node(n)\n[SOURCE]\n\nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op\n\nParameters\n\nn (Node) – The Node to execute\n\nReturns\n\nThe result of executing n\n\nReturn type\n\nAny\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nCLASS\ntorch.fx.Transformer(module)\n[SOURCE]\n\nTransformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically.\n\nExample\n\nSuppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so:\n\nclass NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())\n\nParameters\n\nmodule (GraphModule) – The Module to be transformed.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ncall_function(target, args, kwargs)\n[SOURCE]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nAny\n\ncall_module(target, args, kwargs)\n[SOURCE]\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nAny\n\nget_attr(target, args, kwargs)\n[SOURCE]\n\nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturn type\n\nProxy\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nplaceholder(target, args, kwargs)\n[SOURCE]\n\nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.\n\nParameters\n\ntarget (Target) – The call target for this node. See Node for details on semantics\n\nargs (Tuple) – Tuple of positional args for this invocation\n\nkwargs (Dict) – Dict of keyword arguments for this invocation\n\nReturn type\n\nProxy\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\ntransform()\n[SOURCE]\n\nTransform self.module and return the transformed GraphModule.\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nReturn type\n\nGraphModule\n\ntorch.fx.replace_pattern(gm, pattern, replacement)\n[SOURCE]\n\nMatches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).\n\nParameters\n\ngm (GraphModule) – The GraphModule that wraps the Graph to operate on\n\npattern (Union[Callable, GraphModule]) – The subgraph to match in gm for replacement\n\nreplacement (Union[Callable, GraphModule]) – The subgraph to replace pattern with\n\nReturns\n\nA list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as:\n\nclass Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n\n\nReturn type\n\nList[Match]\n\nExamples:\n\nimport torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\n\nThe above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m).\n\nThe return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn’t have to extend to the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (“First” here being defined as the first in a topological ordering of the Nodes’ use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.)\n\nOne important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn’t use x, so it shouldn’t specify x as a parameter. As an example of the second rule, consider replacing\n\ndef pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n\n\nwith\n\ndef replacement(x, y):\n    return torch.relu(x)\n\n\nIn this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn’t used in replacement.\n\nAfter calling subgraph_rewriter.replace_pattern, the generated Python code looks like this:\n\ndef forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n\n\nNOTE\n\nBackwards-compatibility for this API is guaranteed.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.fx\nOverview\nWriting Transformations\nDebugging\nLimitations of Symbolic Tracing\nAPI Reference\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.monitor — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/monitor.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.monitor\nShortcuts\nTORCH.MONITOR\n\nWARNING\n\nThis module is a prototype release, and its interfaces and functionality may change without warning in future PyTorch releases.\n\ntorch.monitor provides an interface for logging events and counters from PyTorch.\n\nThe stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.\n\nFor more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used.\n\nEvent handlers can be registered to handle the events and pass them to an external event sink.\n\nAPI Reference\nCLASS\ntorch.monitor.Aggregation\n\nThese are types of aggregations that can be used to accumulate stats.\n\nMembers:\n\nVALUE :\n\nVALUE returns the last value to be added.\n\nMEAN :\n\nMEAN computes the arithmetic mean of all the added values.\n\nCOUNT :\n\nCOUNT returns the total number of added values.\n\nSUM :\n\nSUM returns the sum of the added values.\n\nMAX :\n\nMAX returns the max of the added values.\n\nMIN :\n\nMIN returns the min of the added values.\n\nPROPERTY name\nCLASS\ntorch.monitor.Stat\n\nStat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every window_size duration. When the window closes the stats are logged via the event handlers as a torch.monitor.Stat event.\n\nwindow_size should be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.\n\nIf max_samples is set, the stat will cap the number of samples per window by discarding add calls once max_samples adds have occurred. If it’s not set, all add calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.\n\nWhen the Stat is destructed it will log any remaining data even if the window hasn’t elapsed.\n\n__init__(self: torch._C._monitor.Stat, name: str, aggregations: List[torch._C._monitor.Aggregation], window_size: datetime.timedelta, max_samples: int = 9223372036854775807) → None\n\nConstructs the Stat.\n\nadd(self: torch._C._monitor.Stat, v: float) → None\n\nAdds a value to the stat to be aggregated according to the configured stat type and aggregations.\n\nPROPERTY count\n\nNumber of data points that have currently been collected. Resets once the event has been logged.\n\nget(self: torch._C._monitor.Stat) → Dict[torch._C._monitor.Aggregation, float]\n\nReturns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero.\n\nPROPERTY name\n\nThe name of the stat that was set during creation.\n\nCLASS\ntorch.monitor.data_value_t\n\ndata_value_t is one of str, float, int, bool.\n\nCLASS\ntorch.monitor.Event\n\nEvent represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.\n\nAll Events of the same type should have the same name so downstream handlers can correctly process them.\n\n__init__(self: torch._C._monitor.Event, name: str, timestamp: datetime.datetime, data: Dict[str, data_value_t]) → None\n\nConstructs the Event.\n\nPROPERTY data\n\nThe structured data contained within the Event.\n\nPROPERTY name\n\nThe name of the Event.\n\nPROPERTY timestamp\n\nThe timestamp when the Event happened.\n\nCLASS\ntorch.monitor.EventHandlerHandle\n\nEventHandlerHandle is a wrapper type returned by register_event_handler used to unregister the handler via unregister_event_handler. This cannot be directly initialized.\n\ntorch.monitor.log_event(event: torch._C._monitor.Event) → None\n\nlog_event logs the specified event to all of the registered event handlers. It’s up to the event handlers to log the event out to the corresponding event sink.\n\nIf there are no event handlers registered this method is a no-op.\n\ntorch.monitor.register_event_handler(callback: Callable[[torch._C._monitor.Event], None]) → torch._C._monitor.EventHandlerHandle\n\nregister_event_handler registers a callback to be called whenever an event is logged via log_event. These handlers should avoid blocking the main thread since that may interfere with training as they run during the log_event call.\n\ntorch.monitor.unregister_event_handler(handler: torch._C._monitor.EventHandlerHandle) → None\n\nunregister_event_handler unregisters the EventHandlerHandle returned after calling register_event_handler. After this returns the event handler will no longer receive events.\n\nCLASS\ntorch.monitor.TensorboardEventHandler(writer)\n[SOURCE]\n\nTensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.\n\nThis currently only supports torch.monitor.Stat events which are logged as scalars.\n\nExample\n\n>>> from torch.utils.tensorboard import SummaryWriter\n>>> from torch.monitor import TensorboardEventHandler, register_event_handler\n>>> writer = SummaryWriter(\"log_dir\")\n>>> register_event_handler(TensorboardEventHandler(writer))\n\n__init__(writer)\n[SOURCE]\n\nConstructs the TensorboardEventHandler.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.monitor\nAPI Reference\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.signal — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/signal.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.signal\nShortcuts\nTORCH.SIGNAL\n\nThe torch.signal module, modeled after SciPy’s signal module.\n\ntorch.signal.windows\n\nbartlett\n\n\t\n\nComputes the Bartlett window.\n\n\n\n\nblackman\n\n\t\n\nComputes the Blackman window.\n\n\n\n\ncosine\n\n\t\n\nComputes a window with a simple cosine waveform.\n\n\n\n\nexponential\n\n\t\n\nComputes a window with an exponential waveform.\n\n\n\n\ngaussian\n\n\t\n\nComputes a window with a gaussian waveform.\n\n\n\n\ngeneral_cosine\n\n\t\n\nComputes the general cosine window.\n\n\n\n\ngeneral_hamming\n\n\t\n\nComputes the general Hamming window.\n\n\n\n\nhamming\n\n\t\n\nComputes the Hamming window.\n\n\n\n\nhann\n\n\t\n\nComputes the Hann window.\n\n\n\n\nkaiser\n\n\t\n\nComputes the Kaiser window.\n\n\n\n\nnuttall\n\n\t\n\nComputes the minimum 4-term Blackman-Harris window according to Nuttall.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.signal\ntorch.signal.windows\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.linalg — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/linalg.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.linalg\nShortcuts\nTORCH.LINALG\n\nCommon linear algebra operations.\n\nSee Linear algebra (torch.linalg) for some common numerical edge-cases.\n\nMatrix Properties\n\nnorm\n\n\t\n\nComputes a vector or matrix norm.\n\n\n\n\nvector_norm\n\n\t\n\nComputes a vector norm.\n\n\n\n\nmatrix_norm\n\n\t\n\nComputes a matrix norm.\n\n\n\n\ndiagonal\n\n\t\n\nAlias for torch.diagonal() with defaults dim1= -2, dim2= -1.\n\n\n\n\ndet\n\n\t\n\nComputes the determinant of a square matrix.\n\n\n\n\nslogdet\n\n\t\n\nComputes the sign and natural logarithm of the absolute value of the determinant of a square matrix.\n\n\n\n\ncond\n\n\t\n\nComputes the condition number of a matrix with respect to a matrix norm.\n\n\n\n\nmatrix_rank\n\n\t\n\nComputes the numerical rank of a matrix.\n\nDecompositions\n\ncholesky\n\n\t\n\nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.\n\n\n\n\nqr\n\n\t\n\nComputes the QR decomposition of a matrix.\n\n\n\n\nlu\n\n\t\n\nComputes the LU decomposition with partial pivoting of a matrix.\n\n\n\n\nlu_factor\n\n\t\n\nComputes a compact representation of the LU factorization with partial pivoting of a matrix.\n\n\n\n\neig\n\n\t\n\nComputes the eigenvalue decomposition of a square matrix if it exists.\n\n\n\n\neigvals\n\n\t\n\nComputes the eigenvalues of a square matrix.\n\n\n\n\neigh\n\n\t\n\nComputes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.\n\n\n\n\neigvalsh\n\n\t\n\nComputes the eigenvalues of a complex Hermitian or real symmetric matrix.\n\n\n\n\nsvd\n\n\t\n\nComputes the singular value decomposition (SVD) of a matrix.\n\n\n\n\nsvdvals\n\n\t\n\nComputes the singular values of a matrix.\n\nSolvers\n\nsolve\n\n\t\n\nComputes the solution of a square system of linear equations with a unique solution.\n\n\n\n\nsolve_triangular\n\n\t\n\nComputes the solution of a triangular system of linear equations with a unique solution.\n\n\n\n\nlu_solve\n\n\t\n\nComputes the solution of a square system of linear equations with a unique solution given an LU decomposition.\n\n\n\n\nlstsq\n\n\t\n\nComputes a solution to the least squares problem of a system of linear equations.\n\nInverses\n\ninv\n\n\t\n\nComputes the inverse of a square matrix if it exists.\n\n\n\n\npinv\n\n\t\n\nComputes the pseudoinverse (Moore-Penrose inverse) of a matrix.\n\nMatrix Functions\n\nmatrix_exp\n\n\t\n\nComputes the matrix exponential of a square matrix.\n\n\n\n\nmatrix_power\n\n\t\n\nComputes the n-th power of a square matrix for an integer n.\n\nMatrix Products\n\ncross\n\n\t\n\nComputes the cross product of two 3-dimensional vectors.\n\n\n\n\nmatmul\n\n\t\n\nAlias for torch.matmul()\n\n\n\n\nvecdot\n\n\t\n\nComputes the dot product of two batches of vectors along a dimension.\n\n\n\n\nmulti_dot\n\n\t\n\nEfficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.\n\n\n\n\nhouseholder_product\n\n\t\n\nComputes the first n columns of a product of Householder matrices.\n\nTensor Operations\n\ntensorinv\n\n\t\n\nComputes the multiplicative inverse of torch.tensordot().\n\n\n\n\ntensorsolve\n\n\t\n\nComputes the solution X to the system torch.tensordot(A, X) = B.\n\nMisc\n\nvander\n\n\t\n\nGenerates a Vandermonde matrix.\n\nExperimental Functions\n\ncholesky_ex\n\n\t\n\nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.\n\n\n\n\ninv_ex\n\n\t\n\nComputes the inverse of a square matrix if it is invertible.\n\n\n\n\nsolve_ex\n\n\t\n\nA version of solve() that does not perform error checks unless check_errors= True.\n\n\n\n\nlu_factor_ex\n\n\t\n\nThis is a version of lu_factor() that does not perform error checks unless check_errors= True.\n\n\n\n\nldl_factor\n\n\t\n\nComputes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.\n\n\n\n\nldl_factor_ex\n\n\t\n\nThis is a version of ldl_factor() that does not perform error checks unless check_errors= True.\n\n\n\n\nldl_solve\n\n\t\n\nComputes the solution of a system of linear equations using the LDL factorization.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.linalg\nMatrix Properties\nDecompositions\nSolvers\nInverses\nMatrix Functions\nMatrix Products\nTensor Operations\nMisc\nExperimental Functions\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "TorchScript — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/jit.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > TorchScript\nShortcuts\nTORCHSCRIPT\nTorchScript Language Reference\n\nCreating TorchScript Code\n\nMixing Tracing and Scripting\n\nTorchScript Language\n\nBuilt-in Functions and Modules\n\nPyTorch Functions and Modules\n\nPython Functions and Modules\n\nPython Language Reference Comparison\n\nDebugging\n\nDisable JIT for Debugging\n\nInspecting Code\n\nInterpreting Graphs\n\nTracer\n\nFrequently Asked Questions\n\nKnown Issues\n\nAppendix\n\nMigrating to PyTorch 1.2 Recursive Scripting API\n\nFusion Backends\n\nReferences\n\nTorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.\n\nWe provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.\n\nFor a gentle introduction to TorchScript, see the Introduction to TorchScript tutorial.\n\nFor an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the Loading a PyTorch Model in C++ tutorial.\n\nCreating TorchScript Code\n\nscript\n\n\t\n\nScripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction.\n\n\n\n\ntrace\n\n\t\n\nTrace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation.\n\n\n\n\nscript_if_tracing\n\n\t\n\nCompiles fn when it is first called during tracing.\n\n\n\n\ntrace_module\n\n\t\n\nTrace a module and return an executable ScriptModule that will be optimized using just-in-time compilation.\n\n\n\n\nfork\n\n\t\n\nCreates an asynchronous task executing func and a reference to the value of the result of this execution.\n\n\n\n\nwait\n\n\t\n\nForces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task.\n\n\n\n\nScriptModule\n\n\t\n\nA wrapper around C++ torch::jit::Module.\n\n\n\n\nScriptFunction\n\n\t\n\nFunctionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters.\n\n\n\n\nfreeze\n\n\t\n\nFreezing a ScriptModule will clone it and attempt to inline the cloned module's submodules, parameters, and attributes as constants in the TorchScript IR Graph.\n\n\n\n\noptimize_for_inference\n\n\t\n\nPerforms a set of optimization passes to optimize a model for the purposes of inference.\n\n\n\n\nenable_onednn_fusion\n\n\t\n\nEnables or disables onednn JIT fusion based on the parameter enabled.\n\n\n\n\nonednn_fusion_enabled\n\n\t\n\nReturns whether onednn JIT fusion is enabled\n\n\n\n\nset_fusion_strategy\n\n\t\n\nSets the type and number of specializations that can occur during fusion.\n\n\n\n\nstrict_fusion\n\n\t\n\nThis class errors if not all nodes have been fused in inference, or symbolically differentiated in training.\n\n\n\n\nsave\n\n\t\n\nSave an offline version of this module for use in a separate process.\n\n\n\n\nload\n\n\t\n\nLoad a ScriptModule or ScriptFunction previously saved with torch.jit.save\n\n\n\n\nignore\n\n\t\n\nThis decorator indicates to the compiler that a function or method should be ignored and left as a Python function.\n\n\n\n\nunused\n\n\t\n\nThis decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.\n\n\n\n\nisinstance\n\n\t\n\nThis function provides for container type refinement in TorchScript.\n\n\n\n\nAttribute\n\n\t\n\nThis method is a pass-through function that returns value, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type of type.\n\n\n\n\nannotate\n\n\t\n\nThis method is a pass-through function that returns the_value, used to hint TorchScript compiler the type of the_value.\n\nMixing Tracing and Scripting\n\nIn many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.\n\nScripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.\n\nExample (calling a traced function in script):\n\nimport torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n\n\nTraced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.\n\nExample (calling a script function in a traced function):\n\nimport torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() > y.max():\n        r = x\n    else:\n        r = y\n    return r\n\n\ndef bar(x, y, z):\n    return foo(x, y) + z\n\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\n\n\nThis composition also works for nn.Modules as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.\n\nExample (using a traced module):\n\nimport torch\nimport torchvision\n\nclass MyScriptModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    def forward(self, input):\n        return self.resnet(input - self.means)\n\nmy_script_module = torch.jit.script(MyScriptModule())\n\nTorchScript Language\n\nTorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full TorchScript Language Reference for details.\n\nBuilt-in Functions and Modules\n\nTorchScript supports the use of most PyTorch functions and many Python built-ins. See TorchScript Builtins for a full reference of supported functions.\n\nPyTorch Functions and Modules\n\nTorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the torch namespace, all functions in torch.nn.functional and most modules from torch.nn are supported in TorchScript.\n\nSee TorchScript Unsupported PyTorch Constructs for a list of unsupported PyTorch functions and modules.\n\nPython Functions and Modules\n\nMany of Python’s built-in functions are supported in TorchScript. The math module is also supported (see math Module for details), but no other Python modules (built-in or third party) are supported.\n\nPython Language Reference Comparison\n\nFor a full listing of supported Python features, see Python Language Reference Coverage.\n\nDebugging\nDisable JIT for Debugging\nPYTORCH_JIT\n\nSetting the environment variable PYTORCH_JIT=0 will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like pdb to debug the model code. For example:\n\n@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))\n\n\nDebugging this script with pdb works except for when we invoke the @torch.jit.script function. We can globally disable JIT, so that we can call the @torch.jit.script function as a normal Python function and not compile it. If the above script is called disable_jit_example.py, we can invoke it like so:\n\n$ PYTORCH_JIT=0 python disable_jit_example.py\n\n\nand we will be able to step into the @torch.jit.script function as a normal Python function. To disable the TorchScript compiler for a specific function, see @torch.jit.ignore.\n\nInspecting Code\n\nTorchScript provides a code pretty-printer for all ScriptModule instances. This pretty-printer gives an interpretation of the script method’s code as valid Python syntax. For example:\n\n@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)\n\n\nA ScriptModule with a single forward method will have an attribute code, which you can use to inspect the ScriptModule’s code. If the ScriptModule has more than one method, you will need to access .code on the method itself and not the module. We can inspect the code of a method named foo on a ScriptModule by accessing .foo.code. The example above produces this output:\n\ndef foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0\n\n\nThis is TorchScript’s compilation of the code for the forward method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.\n\nInterpreting Graphs\n\nTorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.\n\nTorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:\n\n@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)\n\n\ngraph follows the same rules described in the Inspecting Code section with regard to forward method lookup.\n\nThe example script above produces the graph:\n\ngraph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv)\n\n\nTake the instruction %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10 for example.\n\n%rv.1 : Tensor means we assign the output to a (unique) value named rv.1, that value is of Tensor type and that we do not know its concrete shape.\n\naten::zeros is the operator (equivalent to torch.zeros) and the input list (%4, %6, %6, %10, %12) specifies which values in scope should be passed as inputs. The schema for built-in functions like aten::zeros can be found at Builtin Functions.\n\n# test.py:9:10 is the location in the original source file that generated this instruction. In this case, it is a file named test.py, on line 9, and at character 10.\n\nNotice that operators can also have associated blocks, namely the prim::Loop and prim::If operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.\n\nGraphs can be inspected as shown to confirm that the computation described by a ScriptModule is correct, in both automated and manual fashion, as described below.\n\nTracer\nTracing Edge Cases\n\nThere are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:\n\nTracing of control flow that is dependent on inputs (e.g. tensor shapes)\n\nTracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)\n\nNote that these cases may in fact be traceable in the future.\n\nAutomatic Trace Checking\n\nOne way to automatically catch many errors in traces is by using check_inputs on the torch.jit.trace() API. check_inputs takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:\n\ndef loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n\n\nGives us the following diagnostic information:\n\nERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }\n\n\nThis message indicates to us that the computation differed between when we first traced it and when we traced it with the check_inputs. Indeed, the loop within the body of loop_in_traced_fn depends on the shape of the input x, and thus when we try another x with a different shape, the trace differs.\n\nIn this case, data-dependent control flow like this can be captured using torch.jit.script() instead:\n\ndef fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple))\n\n\nWhich produces:\n\ngraph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n}\n\nTracer Warnings\n\nThe tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:\n\ndef fill_row_zero(x):\n    x[0] = torch.rand(*x.shape[1:2])\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n\n\nProduces several warnings and a graph which simply returns the input:\n\nfill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}\n\n\nWe can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with torch.cat:\n\ndef fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n\nFrequently Asked Questions\n\nQ: I would like to train a model on GPU and do inference on CPU. What are the best practices?\n\nFirst convert your model from GPU to CPU and then save it, like so:\n\ncpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)\n\n\nThis is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model before saving it ensures that the tracer has the correct device information.\n\nQ: How do I store attributes on a ScriptModule?\n\nSay we have a model like:\n\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())\n\n\nIf Model is instantiated it will result in a compilation error since the compiler doesn’t know about x. There are 4 ways to inform the compiler of attributes on ScriptModule:\n\n1. nn.Parameter - Values wrapped in nn.Parameter will work as they do on nn.Modules\n\n2. register_buffer - Values wrapped in register_buffer will work as they do on nn.Modules. This is equivalent to an attribute (see 4) of type Tensor.\n\n3. Constants - Annotating a class member as Final (or adding it to a list called __constants__ at the class definition level) will mark the contained names as constants. Constants are saved directly in the code of the model. See builtin-constants for details.\n\n4. Attributes - Values that are a supported type can be added as mutable attributes. Most types can be inferred but some may need to be specified, see module attributes for details.\n\nQ: I would like to trace module’s method but I keep getting this error:\n\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\n\nThis error usually means that the method you are tracing uses a module’s parameters and you are passing the module’s method instead of the module instance (e.g. my_module_instance.forward vs my_module_instance).\n\nInvoking trace with a module’s method captures module parameters (which may require gradients) as constants.\n\nOn the other hand, invoking trace with module’s instance (e.g. my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.\n\nTo trace a specific method on a module, see torch.jit.trace_module\n\nKnown Issues\n\nIf you’re using Sequential with TorchScript, the inputs of some of the Sequential submodules may be falsely inferred to be Tensor, even if they’re annotated otherwise. The canonical solution is to subclass nn.Sequential and redeclare forward with the input typed correctly.\n\nAppendix\nMigrating to PyTorch 1.2 Recursive Scripting API\n\nThis section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.\n\n1. torch.jit.script will now attempt to recursively compile functions, methods, and classes that it encounters. Once you call torch.jit.script, compilation is “opt-out”, rather than “opt-in”.\n\n2. torch.jit.script(nn_module_instance) is now the preferred way to create ScriptModules, instead of inheriting from torch.jit.ScriptModule. These changes combine to provide a simpler, easier-to-use API for converting your nn.Modules into ScriptModules, ready to be optimized and executed in a non-Python environment.\n\nThe new usage looks like this:\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)\n\n\nThe module’s forward is compiled by default. Methods called from forward are lazily compiled in the order they are used in forward.\n\nTo compile a method other than forward that is not called from forward, add @torch.jit.export.\n\nTo stop the compiler from compiling a method, add @torch.jit.ignore or @torch.jit.unused. @ignore leaves the\n\nmethod as a call to python, and @unused replaces it with an exception. @ignored cannot be exported; @unused can.\n\nMost attribute types can be inferred, so torch.jit.Attribute is not necessary. For empty container types, annotate their types using PEP 526-style class annotations.\n\nConstants can be marked with a Final class annotation instead of adding the name of the member to __constants__.\n\nPython 3 type hints can be used in place of torch.jit.annotate\n\nAs a result of these changes, the following items are considered deprecated and should not appear in new code:\n\nThe @torch.jit.script_method decorator\n\nClasses that inherit from torch.jit.ScriptModule\n\nThe torch.jit.Attribute wrapper class\n\nThe __constants__ array\n\nThe torch.jit.annotate function\n\nModules\n\nWARNING\n\nThe @torch.jit.ignore annotation’s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use @torch.jit.unused(). @torch.jit.ignore is now equivalent to @torch.jit.ignore(drop=False). See @torch.jit.ignore and @torch.jit.unused for details.\n\nWhen passed to the torch.jit.script function, a torch.nn.Module's data is copied to a ScriptModule and the TorchScript compiler compiles the module. The module’s forward is compiled by default. Methods called from forward are lazily compiled in the order they are used in forward, as well as any @torch.jit.export methods.\n\ntorch.jit.export(fn)\n[SOURCE]\n\nThis decorator indicates that a method on an nn.Module is used as an entry point into a ScriptModule and should be compiled.\n\nforward implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from forward are compiled as they are seen by the compiler, so they do not need this decorator either.\n\nExample (using @torch.jit.export on a method):\n\nimport torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def implicitly_compiled_method(self, x):\n        return x + 99\n\n    # `forward` is implicitly decorated with `@torch.jit.export`,\n    # so adding it here would have no effect\n    def forward(self, x):\n        return x + 10\n\n    @torch.jit.export\n    def another_forward(self, x):\n        # When the compiler sees this call, it will compile\n        # `implicitly_compiled_method`\n        return self.implicitly_compiled_method(x)\n\n    def unused_method(self, x):\n        return x - 20\n\n# `m` will contain compiled methods:\n#     `forward`\n#     `another_forward`\n#     `implicitly_compiled_method`\n# `unused_method` will not be compiled since it was not called from\n# any compiled methods and wasn't decorated with `@torch.jit.export`\nm = torch.jit.script(MyModule())\n\nFunctions\n\nFunctions don’t change much, they can be decorated with @torch.jit.ignore or torch.jit.unused if needed.\n\n# Same behavior as pre-PyTorch 1.2\n@torch.jit.script\ndef some_fn():\n    return 2\n\n# Marks a function as ignored, if nothing\n# ever calls it then this has no effect\n@torch.jit.ignore\ndef some_fn2():\n    return 2\n\n# As with ignore, if nothing calls it then it has no effect.\n# If it is called in script it is replaced with an exception.\n@torch.jit.unused\ndef some_fn3():\n  import pdb; pdb.set_trace()\n  return 4\n\n# Doesn't do anything, this function is already\n# the main entry point\n@torch.jit.export\ndef some_fn4():\n    return 2\n\nTorchScript Classes\n\nWARNING\n\nTorchScript class support is experimental. Currently it is best suited for simple record-like types (think a NamedTuple with methods attached).\n\nEverything in a user defined TorchScript Class is exported by default, functions can be decorated with @torch.jit.ignore if needed.\n\nAttributes\n\nThe TorchScript compiler needs to know the types of module attributes. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with PEP 526-style class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting ScriptModule\n\nOld API:\n\nfrom typing import Dict\nimport torch\n\nclass MyModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super().__init__()\n        self.my_dict = torch.jit.Attribute({}, Dict[str, int])\n        self.my_int = torch.jit.Attribute(20, int)\n\nm = MyModule()\n\n\nNew API:\n\nfrom typing import Dict\n\nclass MyModule(torch.nn.Module):\n    my_dict: Dict[str, int]\n\n    def __init__(self):\n        super().__init__()\n        # This type cannot be inferred and must be specified\n        self.my_dict = {}\n\n        # The attribute type here is inferred to be `int`\n        self.my_int = 20\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n\nConstants\n\nThe Final type constructor can be used to mark members as constant. If members are not marked constant, they will be copied to the resulting ScriptModule as an attribute. Using Final opens opportunities for optimization if the value is known to be fixed and gives additional type safety.\n\nOld API:\n\nclass MyModule(torch.jit.ScriptModule):\n    __constants__ = ['my_constant']\n\n    def __init__(self):\n        super().__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\nm = MyModule()\n\n\nNew API:\n\nfrom typing import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super().__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n\nVariables\n\nContainers are assumed to have type Tensor and be non-optional (see Default Types for more information). Previously, torch.jit.annotate was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.\n\nimport torch\nfrom typing import Dict, Optional\n\n@torch.jit.script\ndef make_dict(flag: bool):\n    x: Dict[str, int] = {}\n    x['hi'] = 2\n    b: Optional[int] = None\n    if flag:\n        b = 2\n    return x, b\n\nFusion Backends\n\nThere are a couple of fusion backends available to optimize TorchScript execution. The default fuser on CPUs is NNC, which can perform fusions for both CPUs and GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators and has demonstrated generated kernels with improved throughput. See the NVFuser documentation for more details on usage and debugging.\n\nReferences\nPython Language Reference Coverage\nTorchScript Unsupported PyTorch Constructs\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nTorchScript\nCreating TorchScript Code\nMixing Tracing and Scripting\nTorchScript Language\nBuilt-in Functions and Modules\nDebugging\nFrequently Asked Questions\nKnown Issues\nAppendix\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.hub — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/hub.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.hub\nShortcuts\nTORCH.HUB\n\nPytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n\nPublishing models\n\nPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a GitHub repository by adding a simple hubconf.py file;\n\nhubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).\n\ndef entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...\n\nHow to implement an entrypoint?\n\nHere is a code snippet specifies an entrypoint for resnet18 model if we expand the implementation in pytorch/vision/hubconf.py. In most case importing the right function in hubconf.py is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo\n\ndependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n\n\ndependencies variable is a list of package names required to load the model. Note this might be slightly different from dependencies required for training a model.\n\nargs and kwargs are passed along to the real callable function.\n\nDocstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here.\n\nEntrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.\n\nCallables prefixed with underscore are considered as helper functions which won’t show up in torch.hub.list().\n\nPretrained weights can either be stored locally in the GitHub repo, or loadable by torch.hub.load_state_dict_from_url(). If less than 2GB, it’s recommended to attach it to a project release and use the url from the release. In the example above torchvision.models.resnet.resnet18 handles pretrained, alternatively you can put the following logic in the entrypoint definition.\n\nif pretrained:\n    # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n\nImportant Notice\n\nThe published models should be at least in a branch/tag. It can’t be a random commit.\n\nLoading models from Hub\n\nPytorch Hub provides convenient APIs to explore all available models in hub through torch.hub.list(), show docstring and examples through torch.hub.help() and load the pre-trained models using torch.hub.load().\n\ntorch.hub.list(github, force_reload=False, skip_validation=False, trust_repo=None)\n[SOURCE]\n\nList all callable entrypoints available in the repo specified by github.\n\nParameters\n\ngithub (str) – a string with format “repo_owner/repo_name[:ref]” with an optional ref (tag or branch). If ref is not specified, the default branch is assumed to be main if it exists, and otherwise master. Example: ‘pytorch/vision:0.10’\n\nforce_reload (bool, optional) – whether to discard the existing cache and force a fresh download. Default is False.\n\nskip_validation (bool, optional) – if False, torchhub will check that the branch or commit specified by the github argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the GITHUB_TOKEN environment variable. Default is False.\n\ntrust_repo (bool, str or None) –\n\n\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n\nIf False, a prompt will ask the user whether the repo should be trusted.\n\nIf True, the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n\nIf \"check\", the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the trust_repo=False option.\n\nIf None: this will raise a warning, inviting the user to set trust_repo to either False, True or \"check\". This is only present for backward compatibility and will be removed in v2.0.\n\nDefault is None and will eventually change to \"check\" in v2.0.\n\nReturns\n\nThe available callables entrypoint\n\nReturn type\n\nlist\n\nExample\n\n>>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n\ntorch.hub.help(github, model, force_reload=False, skip_validation=False, trust_repo=None)\n[SOURCE]\n\nShow the docstring of entrypoint model.\n\nParameters\n\ngithub (str) – a string with format <repo_owner/repo_name[:ref]> with an optional ref (a tag or a branch). If ref is not specified, the default branch is assumed to be main if it exists, and otherwise master. Example: ‘pytorch/vision:0.10’\n\nmodel (str) – a string of entrypoint name defined in repo’s hubconf.py\n\nforce_reload (bool, optional) – whether to discard the existing cache and force a fresh download. Default is False.\n\nskip_validation (bool, optional) – if False, torchhub will check that the ref specified by the github argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the GITHUB_TOKEN environment variable. Default is False.\n\ntrust_repo (bool, str or None) –\n\n\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n\nIf False, a prompt will ask the user whether the repo should be trusted.\n\nIf True, the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n\nIf \"check\", the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the trust_repo=False option.\n\nIf None: this will raise a warning, inviting the user to set trust_repo to either False, True or \"check\". This is only present for backward compatibility and will be removed in v2.0.\n\nDefault is None and will eventually change to \"check\" in v2.0.\n\nExample\n\n>>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n\ntorch.hub.load(repo_or_dir, model, *args, source='github', trust_repo=None, force_reload=False, verbose=True, skip_validation=False, **kwargs)\n[SOURCE]\n\nLoad a model from a github repo or a local directory.\n\nNote: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.\n\nIf source is ‘github’, repo_or_dir is expected to be of the form repo_owner/repo_name[:ref] with an optional ref (a tag or a branch).\n\nIf source is ‘local’, repo_or_dir is expected to be a path to a local directory.\n\nParameters\n\nrepo_or_dir (str) – If source is ‘github’, this should correspond to a github repo with format repo_owner/repo_name[:ref] with an optional ref (tag or branch), for example ‘pytorch/vision:0.10’. If ref is not specified, the default branch is assumed to be main if it exists, and otherwise master. If source is ‘local’ then it should be a path to a local directory.\n\nmodel (str) – the name of a callable (entrypoint) defined in the repo/dir’s hubconf.py.\n\n*args (optional) – the corresponding args for callable model.\n\nsource (str, optional) – ‘github’ or ‘local’. Specifies how repo_or_dir is to be interpreted. Default is ‘github’.\n\ntrust_repo (bool, str or None) –\n\n\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n\nIf False, a prompt will ask the user whether the repo should be trusted.\n\nIf True, the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n\nIf \"check\", the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the trust_repo=False option.\n\nIf None: this will raise a warning, inviting the user to set trust_repo to either False, True or \"check\". This is only present for backward compatibility and will be removed in v2.0.\n\nDefault is None and will eventually change to \"check\" in v2.0.\n\nforce_reload (bool, optional) – whether to force a fresh download of the github repo unconditionally. Does not have any effect if source = 'local'. Default is False.\n\nverbose (bool, optional) – If False, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if source = 'local'. Default is True.\n\nskip_validation (bool, optional) – if False, torchhub will check that the branch or commit specified by the github argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the GITHUB_TOKEN environment variable. Default is False.\n\n**kwargs (optional) – the corresponding kwargs for callable model.\n\nReturns\n\nThe output of the model callable when called with the given *args and **kwargs.\n\nExample\n\n>>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V1')\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', weights='ResNet50_Weights.DEFAULT')\n\ntorch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)\n[SOURCE]\n\nDownload object at the given URL to a local path.\n\nParameters\n\nurl (str) – URL of the object to download\n\ndst (str) – Full path where object will be saved, e.g. /tmp/temporary_file\n\nhash_prefix (str, optional) – If not None, the SHA256 downloaded file should start with hash_prefix. Default: None\n\nprogress (bool, optional) – whether or not to display a progress bar to stderr Default: True\n\nExample\n\n>>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n\ntorch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None, weights_only=False)\n[SOURCE]\n\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in model_dir, it’s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().\n\nParameters\n\nurl (str) – URL of the object to download\n\nmodel_dir (str, optional) – directory in which to save the object\n\nmap_location (optional) – a function or a dict specifying how to remap storage locations (see torch.load)\n\nprogress (bool, optional) – whether or not to display a progress bar to stderr. Default: True\n\ncheck_hash (bool, optional) – If True, the filename part of the URL should follow the naming convention filename-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False\n\nfile_name (str, optional) – name for the downloaded file. Filename from url will be used if not set.\n\nweights_only (bool, optional) – If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See load() for more details.\n\nReturn type\n\nDict[str, Any]\n\nExample\n\n>>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n\nRunning a loaded model:\n\nNote that *args and **kwargs in torch.hub.load() are used to instantiate a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is\n\ndir(model) to see all available methods of the model.\n\nhelp(model.foo) to check what arguments model.foo takes to run\n\nTo help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.\n\nWhere are my downloaded models saved?\n\nThe locations are used in the order of\n\nCalling hub.set_dir(<PATH_TO_HUB_DIR>)\n\n$TORCH_HOME/hub, if environment variable TORCH_HOME is set.\n\n$XDG_CACHE_HOME/torch/hub, if environment variable XDG_CACHE_HOME is set.\n\n~/.cache/torch/hub\n\ntorch.hub.get_dir()\n[SOURCE]\n\nGet the Torch Hub cache directory used for storing downloaded models & weights.\n\nIf set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set.\n\ntorch.hub.set_dir(d)\n[SOURCE]\n\nOptionally set the Torch Hub directory used to save downloaded models & weights.\n\nParameters\n\nd (str) – path to a local folder to save downloaded models & weights.\n\nCaching logic\n\nBy default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by get_dir().\n\nUsers can force a reload by calling hub.load(..., force_reload=True). This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.\n\nKnown limitations:\n\nTorch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches sys.modules and sys.path_importer_cache which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a model subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the sys.modules dict; more details can be found in this GitHub issue.\n\nA known limitation that is worth mentioning here: users CANNOT load two different branches of the same repo in the same python process. It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.hub\nPublishing models\nLoading models from Hub\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.fft — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/fft.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.fft\nShortcuts\nTORCH.FFT\n\nDiscrete Fourier transforms and related functions.\n\nFast Fourier Transforms\n\nfft\n\n\t\n\nComputes the one dimensional discrete Fourier transform of input.\n\n\n\n\nifft\n\n\t\n\nComputes the one dimensional inverse discrete Fourier transform of input.\n\n\n\n\nfft2\n\n\t\n\nComputes the 2 dimensional discrete Fourier transform of input.\n\n\n\n\nifft2\n\n\t\n\nComputes the 2 dimensional inverse discrete Fourier transform of input.\n\n\n\n\nfftn\n\n\t\n\nComputes the N dimensional discrete Fourier transform of input.\n\n\n\n\nifftn\n\n\t\n\nComputes the N dimensional inverse discrete Fourier transform of input.\n\n\n\n\nrfft\n\n\t\n\nComputes the one dimensional Fourier transform of real-valued input.\n\n\n\n\nirfft\n\n\t\n\nComputes the inverse of rfft().\n\n\n\n\nrfft2\n\n\t\n\nComputes the 2-dimensional discrete Fourier transform of real input.\n\n\n\n\nirfft2\n\n\t\n\nComputes the inverse of rfft2().\n\n\n\n\nrfftn\n\n\t\n\nComputes the N-dimensional discrete Fourier transform of real input.\n\n\n\n\nirfftn\n\n\t\n\nComputes the inverse of rfftn().\n\n\n\n\nhfft\n\n\t\n\nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.\n\n\n\n\nihfft\n\n\t\n\nComputes the inverse of hfft().\n\n\n\n\nhfft2\n\n\t\n\nComputes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal.\n\n\n\n\nihfft2\n\n\t\n\nComputes the 2-dimensional inverse discrete Fourier transform of real input.\n\n\n\n\nhfftn\n\n\t\n\nComputes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.\n\n\n\n\nihfftn\n\n\t\n\nComputes the N-dimensional inverse discrete Fourier transform of real input.\n\nHelper Functions\n\nfftfreq\n\n\t\n\nComputes the discrete Fourier Transform sample frequencies for a signal of size n.\n\n\n\n\nrfftfreq\n\n\t\n\nComputes the sample frequencies for rfft() with a signal of size n.\n\n\n\n\nfftshift\n\n\t\n\nReorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.\n\n\n\n\nifftshift\n\n\t\n\nInverse of fftshift().\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.fft\nFast Fourier Transforms\nHelper Functions\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.futures — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/futures.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.futures\nShortcuts\nTORCH.FUTURES\n\nThis package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.\n\nCLASS\ntorch.futures.Future(*, devices=None)\n\nWrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.\n\nWARNING\n\nGPU support is a beta feature, subject to changes.\n\nadd_done_callback(callback)\n[SOURCE]\n\nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.\n\nWe recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.\n\nWith respect to GPU tensors, this method behaves in the same way as then().\n\nParameters\n\ncallback (Future) – a Callable that takes in one argument, which is the reference to this Future.\n\nNOTE\n\nNote that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.\n\nExample::\n>>> def callback(fut):\n...     print(\"This will run after the future has finished.\")\n...     print(fut.wait())\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\nThis will run after the future has finished.\n5\n\ndone()\n[SOURCE]\n\nReturn True if this Future is done. A Future is done if it has a result or an exception.\n\nIf the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).\n\nReturn type\n\nbool\n\nset_exception(result)\n[SOURCE]\n\nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.\n\nParameters\n\nresult (BaseException) – the exception for this Future.\n\nExample::\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\nTraceback (most recent call last):\n...\nValueError: foo\n\nset_result(result)\n[SOURCE]\n\nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.\n\nIf the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it’s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn’t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.\n\nParameters\n\nresult (object) – the result object of this Future.\n\nExample::\n>>> import threading\n>>> import time\n>>> def slow_set_future(fut, value):\n...     time.sleep(0.5)\n...     fut.set_result(value)\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n...     target=slow_set_future,\n...     args=(fut, torch.ones(2) * 3)\n... )\n>>> t.start()\n>>> print(fut.wait())\ntensor([3., 3.])\n>>> t.join()\n\nthen(callback)\n[SOURCE]\n\nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.\n\nIf the Future’s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven’t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn’t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().\n\nSimilarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn’t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.\n\nParameters\n\ncallback (Callable) – a Callable that takes this Future as the only argument.\n\nReturns\n\nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.\n\nReturn type\n\nFuture[S]\n\nNOTE\n\nNote that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.\n\nExample::\n>>> def callback(fut):\n...     print(f\"RPC return value is {fut.wait()}.\")\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n...     lambda x : print(f\"Chained cb done. {x.wait()}\")\n... )\n>>> fut.set_result(5)\nRPC return value is 5.\nChained cb done. None\n\nvalue()\n[SOURCE]\n\nObtain the value of an already-completed future.\n\nThis method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.\n\nIf the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it’s already being taken care of by then()).\n\nReturns\n\nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.\n\nReturn type\n\nT\n\nwait()\n[SOURCE]\n\nBlock until the value of this Future is ready.\n\nIf the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn’t change streams.\n\nReturns\n\nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.\n\nReturn type\n\nT\n\ntorch.futures.collect_all(futures)\n[SOURCE]\n\nCollects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.\n\nParameters\n\nfutures (list) – a list of Future objects.\n\nReturns\n\nReturns a Future object to a list of the passed in Futures.\n\nReturn type\n\nFuture[List[Future]]\n\nExample::\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\nfut0 result = 0\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\nfut1 result = 1\n\ntorch.futures.wait_all(futures)\n[SOURCE]\n\nWaits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.\n\nParameters\n\nfutures (list) – a list of Future object.\n\nReturns\n\nA list of the completed Future results. This method will throw an error if wait on any Future throws.\n\nReturn type\n\nList\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.futures\nFuture\ncollect_all()\nwait_all()\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.func — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/func.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.func\nShortcuts\nTORCH.FUNC\n\ntorch.func, previously known as “functorch”, is JAX-like composable function transforms for PyTorch.\n\nNOTE\n\nThis library is currently in beta. What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don’t have full coverage over PyTorch operations.\n\nIf you have suggestions on the API or use-cases you’d like to be covered, please open an GitHub issue or reach out. We’d love to hear about how you’re using the library.\n\nWhat are composable function transforms?\n\nA “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity.\n\ntorch.func has auto-differentiation transforms (grad(f) returns a function that computes the gradient of f), a vectorization/batching transform (vmap(f) returns a function that computes f over batches of inputs), and others.\n\nThese function transforms can compose with each other arbitrarily. For example, composing vmap(grad(f)) computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.\n\nWhy composable function transforms?\n\nThere are a number of use cases that are tricky to do in PyTorch today:\n\ncomputing per-sample-gradients (or other per-sample quantities)\n\nrunning ensembles of models on a single machine\n\nefficiently batching together tasks in the inner-loop of MAML\n\nefficiently computing Jacobians and Hessians\n\nefficiently computing batched Jacobians and Hessians\n\nComposing vmap(), grad(), and vjp() transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the JAX framework.\n\nRead More\ntorch.func Whirlwind Tour\nWhat is torch.func?\nWhy composable function transforms?\nWhat are the transforms?\ntorch.func API Reference\nFunction Transforms\nUtilities for working with torch.nn.Modules\nUX Limitations\nGeneral limitations\ntorch.autograd APIs\nvmap limitations\nRandomness\nMigrating from functorch to torch.func\nfunction transforms\nNN module utilities\nfunctorch.compile\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.func\nWhat are composable function transforms?\nWhy composable function transforms?\nRead More\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Distributed Checkpoint - torch.distributed.checkpoint — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.checkpoint.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Distributed Checkpoint - torch.distributed.checkpoint\nShortcuts\nDISTRIBUTED CHECKPOINT - TORCH.DISTRIBUTED.CHECKPOINT\n\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topology and loading into another.\n\nDCP is different than torch.save and torch.load in a few significant ways:\n\nIt produces multiple files per checkpoint, with at least one per rank.\n\nIt operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\n\nThe entrypoints to load and save a checkpoint are the following:\n\ntorch.distributed.checkpoint.load_state_dict(state_dict, storage_reader, process_group=None, coordinator_rank=0, no_dist=False, planner=None)\n[SOURCE]\n\nLoads a distributed state_dict in SPMD style.\n\nEach rank will try to read the least amount of data necessary to fullfill the requested state_dict. When loading ShardedTensor instances, each rank only reads data for their local shards.\n\nWARNING\n\nAll tensors in state_dict must be allocated on their destination device prior to calling this function.\n\nAll non-tensor data is loaded using torch.load() and modified in place on state_dict.\n\nWARNING\n\nUsers must call load_state_dict on the root module to ensure load pos-processing and non-tensor data properly propagates.\n\nParameters\n\nstate_dict (Dict[str, Any]) – The state_dict to load. Note that this state dict will updated in place.\n\nstorage_reader (StorageReader) – StorageReader used to load data from.\n\nprocess_group (ProcessGroup) – ProcessGroup to be used for cross-rank synchronization.\n\ncoordinator_rank (int) – Rank to use to coordinate the checkpoint. rank0 is used by default.\n\nno_dist (bool) – If True, distributed checkpoint will not save in SPMD style. (Default: False)\n\nReturns\n\nNone.\n\nReturn type\n\nNone\n\nExamples\n>>> my_model = MyModule()\n>>> optimizer = Adagrad(my_model.parameters())\n>>> model_state_dict = my_model.state_dict()\n>>> fs_storage_reader = torch.distributed.checkpoint.FileSystemReader(\"/checkpoint/1\")\n\n>>> torch.distributed.checkpoint.load_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_reader=fs_storage_reader,\n>>> )\n\n>>> # module.load_state_dict() function might have customized steps\n>>> # to flush the state_dict, must call it to\n>>> # ensure correct behavior.\n>>> my_model.load_state_dict(model_state_dict)\n\n\nNOTE\n\nload_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().\n\ntorch.distributed.checkpoint.save_state_dict(state_dict, storage_writer, process_group=None, coordinator_rank=0, no_dist=False, planner=None)\n[SOURCE]\n\nSaves a distributed model in SPMD style.\n\nThis function is different from torch.save() as it handles ShardedTensor by having each rank only save their local shards.\n\nWARNING\n\nThere is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.\n\nWARNING\n\nIf using the process_group argument, make sure that only its ranks call save_state_dict and that all data in state_dict belong to it.\n\nNOTE\n\nWhen saving checkpoint for FSDP’s ShardingStrategy.HYBRID_SHARD, only one of the shard_group should be calling save_state_dict and the corresponding process group needs to be passed in.\n\nNOTE\n\nThis function can be used to save a state_dict without having a process group initialized by passing no_dist=True.\n\nParameters\n\nstate_dict (Dict[str, Any]) – The state_dict to save.\n\nstorage_writer (StorageWriter) – Instance of StorageWrite use to perform writes.\n\nprocess_group (ProcessGroup) – ProcessGroup to be used for cross-rank synchronization.\n\ncoordinator_rank (int) – Rank to use to coordinate the checkpoint. rank0 is used by default.\n\nno_dist (bool) – If True, distributed checkpoint will not save in SPMD style. (Default: False)\n\nReturns\n\nMetadata object for the saved checkpoint.\n\nReturn type\n\nMetadata\n\nExample\n\n>>> my_model = MyModule()\n\n>>> model_state_dict = my_model.state_dict()\n\n>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\"/checkpoint/1\")\n>>> torch.distributed.checkpoint.save_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n\n\nNOTE\n\nsave_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().\n\nThis example shows how to use Pytorch Distributed Checkpoint to save a FSDP model.\n\nThe following types define the IO interface used during checkpoint:\n\nCLASS\ntorch.distributed.checkpoint.StorageReader\n[SOURCE]\n\nInterface used by load_state_dict to read from storage.\n\nOne StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.\n\nA subclass should expected the following sequence of calls by load_state_dict:\n\n(all ranks) read_metadata()\n\n(all ranks) set_up_storage_reader()\n\n(all ranks) prepare_local_plan()\n\n(coordinator) prepare_global_plan()\n\n(all ranks) read_data()\n\nABSTRACT prepare_global_plan(plans)\n[SOURCE]\n\nPerform centralized planning of storage loading.\n\nThis method is only called on the coordinator instance.\n\nWhile this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.\n\nParameters\n\nplans (List[LoadPlan]) – A list of LoadPlan instances, one for each rank.\n\nReturns\n\nA list of transformed LoadPlan after storage global planning\n\nReturn type\n\nList[LoadPlan]\n\nABSTRACT prepare_local_plan(plan)\n[SOURCE]\n\nPerform storage-specific local planning.\n\nWhile this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.\n\nParameters\n\nplan (LoadPlan) – The local plan from the LoadPlan in use.\n\nReturns\n\nA transformed LoadPlan after storage local planning\n\nReturn type\n\nLoadPlan\n\nABSTRACT read_data(plan, planner)\n[SOURCE]\n\nReads all items from plan using planner to resolve the data.\n\nA subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place.\n\nA subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into.\n\nIt’s the StorageLayer responsibility to properly schedule any cross device copies required.\n\nParameters\n\nplan (LoadPlan) – The local plan to execute on\n\nplanner (LoadPlanner) – The planner object to use to resolve items.\n\nReturns\n\nA future that completes once all reads are finished.\n\nReturn type\n\nFuture[None]\n\nABSTRACT read_metadata()\n[SOURCE]\n\nReads the checkpoint metadata.\n\nReturns\n\nThe metadata object associated with the checkpoint being loaded.\n\nReturn type\n\nMetadata\n\nABSTRACT set_up_storage_reader(metadata, is_coordinator)\n[SOURCE]\n\nInitialize this instance.\n\nParameters\n\nmetadata (Metadata) – The metadata schema to use.\n\nis_coordinator (bool) – Whether this instance is responsible for coordinating the checkpoint.\n\nCLASS\ntorch.distributed.checkpoint.StorageWriter\n[SOURCE]\n\nInterface used by save_state_dict to write to storage.\n\nOne StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.\n\nA subclass should expect the following sequence of calls.\n\n(all ranks) set_up_storage_writer()\n\n(all ranks) prepare_local_plan()\n\n(coordinator) prepare_global_plan()\n\n(all ranks) write_data()\n\n(coordinator) finish()\n\nABSTRACT finish(metadata, results)\n[SOURCE]\n\nWrites the metadata and marks the current checkpoint as successful.\n\nThe actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it’s recoverable in to the same object graph.\n\nParameters\n\nmetadata (Metadata) – metadata for the new checkpoint\n\nresults (List[List[WriteResult]]) – A list of WriteResults from all ranks.\n\nReturns\n\nNone\n\nReturn type\n\nNone\n\nABSTRACT prepare_global_plan(plans)\n[SOURCE]\n\nPerform centralized planning of storage.\n\nThis method is only called on the coordinator instance.\n\nWhile this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.\n\nParameters\n\nplans (List[SavePlan]) – A list of SavePlan instances, one for each rank.\n\nReturns\n\nA list of transformed SavePlan after storage global planning\n\nReturn type\n\nList[SavePlan]\n\nABSTRACT prepare_local_plan(plan)\n[SOURCE]\n\nPerform storage-specific local planning.\n\nWhile this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.\n\nParameters\n\nplan (SavePlan) – The local plan from the SavePlanner in use.\n\nReturns\n\nA transformed SavePlan after storage local planning\n\nReturn type\n\nSavePlan\n\nABSTRACT set_up_storage_writer(is_coordinator)\n[SOURCE]\n\nInitialize this instance.\n\nParameters\n\nis_coordinator (bool) – Whether this instance is responsible for coordinating the checkpoint.\n\nABSTRACT write_data(plan, planner)\n[SOURCE]\n\nWrite all items from plan using planner to resolve the data.\n\nA subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write.\n\nSubclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:\n\nThey might be on any device, including not matching the one on WriteItem::tensor_data\n\nThey might be views or not contiguous. Only the projection needs to be saved.\n\nParameters\n\nplan (SavePlan) – The save plan to execute.\n\nplanner (SavePlanner) – Planner object to be used to resolve items to data.\n\nReturns\n\nA future that completes to a list of WriteResult\n\nReturn type\n\nFuture[List[WriteResult]]\n\nThe following types define the planner interface used during checkpoint:\n\nCLASS\ntorch.distributed.checkpoint.LoadPlanner\n[SOURCE]\n\nAbstract class defining the protocol used by load_state_dict to plan the load process.\n\nLoadPlanner are stateful objects that can be used to customize the whole load process.\n\nLoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.\n\nA planner subclass can expect the following sequence of calls during load_state_dict:\n\nset_up_planner - called on all ranks.\n\nSignals the start of loading a checkpoint.\n\ncreate_local_plan - called on all ranks.\n\nProcess the state_dict and produces a LoadPlan that will be sent for global planning.\n\ncreate_global_plan - called on the coordinator rank only.\n\nTakes the LoadPlan from all ranks and make any global decision.\n\nload_bytes - called multiple times on each rank\n\nThis is called once per non-tensor value in state_dict.\n\nresolve_tensor and commit_tensor - called multiple times on each rank\n\nThey are called in pair for each Tensor value in state_dict.\n\nUsers are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method.\n\nThere are two usual patterns of extension:\n\nRewriting state_dict. This is the simplest way to extend the load process as it doesn’t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place\n\n>>> class RenamePlanner(DefaultLoadPlanner):\n>>>     def set_up_planner(self, state_dict, metadata, is_coordinator):\n>>>         self.original_state_dict = state_dict\n>>>         super().set_up_planner(self, {\"foo_\" + k: v for k, v in state_dict.items()}, is_coordinator)\n>>>\n>>>     def load_bytes(self, read_item, value):\n>>>         # Remove the \"foo_\" prefix\n>>>         self.original_state_dict[read_item.dest_index.fqn[4:]] = torch.load(value)\n\n\nModifying resolve_tensor and commit_tensor to handle load time transformation.\n\n>>> class MetaModelMaterialize(DefaultSavePlanner):\n>>>     def resolve_tensor(self, read_item):\n>>>         tensor = super().resolve_tensor(read_item)\n>>>         return torch.empty_like(tensor, device=\"cpu\")\n>>>\n>>>     def commit_tensor(self, read_item, tensor):\n>>>         self.state_dict[read_item.dest_index.fqn] = tensor\n\nABSTRACT commit_tensor(read_item, tensor)\n[SOURCE]\n\nThis method is called once the StorageReader finished loading data into tensor.\n\nThe provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict.\n\nThe contents of tensor will follow its device synchronization model.\n\nABSTRACT create_global_plan(global_plan)\n[SOURCE]\n\nCompute the global load plan and return plans for each rank.\n\n. N.B. This is called on the coordinator rank only\n\nReturn type\n\nList[LoadPlan]\n\nABSTRACT create_local_plan()\n[SOURCE]\n\nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner.\n\n. N.B. This is called on every rank.\n\nReturn type\n\nLoadPlan\n\nABSTRACT finish_plan(central_plan)\n[SOURCE]\n\nAccept the plan from coordinator and return final LoadPlan.\n\nReturn type\n\nLoadPlan\n\nABSTRACT load_bytes(read_item, value)\n[SOURCE]\n\nLoad the item described by read_item``and ``value.\n\nThis method is expected to modify in-place the underlying state_dict.\n\nThe contents of value are defined by the SavePlanner used to produce the checkpoint being loaded.\n\nABSTRACT resolve_tensor(read_item)\n[SOURCE]\n\nReturn the tensor described by read_item to be used by the StorageReader to load read_item.\n\nThe tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that’s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.\n\nReturn type\n\nTensor\n\nABSTRACT set_up_planner(state_dict, metadata, is_coordinator)\n[SOURCE]\n\nInitialize this instance to load data into state_dict\n\n. N.B. This is called on every rank.\n\nCLASS\ntorch.distributed.checkpoint.LoadPlan(items: List[torch.distributed.checkpoint.planner.ReadItem], storage_data: Any = None, planner_data: Any = None)\n[SOURCE]\nCLASS\ntorch.distributed.checkpoint.ReadItem(type: torch.distributed.checkpoint.planner.LoadItemType, dest_index: torch.distributed.checkpoint.metadata.MetadataIndex, dest_offsets: torch.Size, storage_index: torch.distributed.checkpoint.metadata.MetadataIndex, storage_offsets: torch.Size, lengths: torch.Size)\n[SOURCE]\nCLASS\ntorch.distributed.checkpoint.SavePlanner\n[SOURCE]\n\nAbstract class defining the protocol used by save_state_dict to plan the save process.\n\nSavePlanners are stateful objects that can be used to customize the whole save process.\n\nSavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.\n\nA planner subclass can expect the following sequence of calls during save_state_dict:\n\nset_up_planner - called on all ranks.\n\nSignals the start of a checkpoint save.\n\ncreate_local_plan - called on all ranks.\n\nProcess the state_dict and produces a SavePlan that will be sent for global planning.\n\ncreate_global_plan - called on the coordinator rank only.\n\nTakes the SavePlan from all ranks and make any global decision.\n\nfinish_plan - called on all ranks.\n\nThis gives each rank a chance to adjust to global planning decisions.\n\nresolve_data - called multiple times on each rank\n\nLookups a value on the state_dict for the storage layer to write.\n\nUsers are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method.\n\nThere are 3 usual patterns of extension:\n\nRewriting state_dict. This is the simplest way to extend the save process as it doesn’t requite understanding the intrincacies of how SavePlan works:\n\n>>> class RenamePlanner(DefaultSavePlanner):\n>>>     def set_up_planner(self, state_dict, is_coordinator):\n>>>         # prefix all keys with `foo_``\n>>>         super().set_up_planner({\"foo_\" + k: v for k, v in state_dict.items()}, is_coordinator)\n\n\nModifying local plan and lookup in tandem. This is useful when fine control of how data is persisted\n\n>>> class FP16Planner(DefaultSavePlanner):\n>>>     def create_local_plan(self):\n>>>         plan = super().create_local_plan()\n>>>         for p in plan:\n>>>             if p.tensor_data is not None:\n>>>                 p.tensor_data.properties.dtype = torch.float16\n>>>         return plan\n>>>\n>>>     def resolve_data(self, write_item):\n>>>         item = super().resolve_data(write_item)\n>>>         return item if write_item.type == WriteItemType.BYTE_IO else item.to(torch.float16)\n\n\nUsing the global planning step to make central decisions that can’t be made individually by each rank\n\n>>> from itertools import islice\n>>> from dataclasses import replace\n>>> class DDPLoadBalancingPlanner(DefaultSavePlanner):\n>>>     # This uses the default local plan behavior of having all non-sharded writes in rank 0\n>>>     # This sample doesn't handle ShardedTensors\n>>>     def create_global_plan(self, all_plans):\n>>>         def chunk(it, size):\n>>>             it = iter(it)\n>>>         return list(iter(lambda: tuple(islice(it, size)), ()))\n>>>         all_plans = [\n>>>             replace(plan, items=items) for plan, items in\n>>>                 zip(all_plans, chunk(all_plans[0].items, len(all_plans)))\n>>>         ]\n>>>         return super().create_global_plan(all_plans)\n\n\nFinally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:\n\n>>> class SaveExtraDataPlanner(DefaultSavePlanner):\n>>>     def create_local_plan(self) -> SavePlan:\n>>>         plan = super().create_local_plan()\n>>>         return replace(plan, planner_data=\"per-rank-data\")\n>>>\n>>>     def create_global_plan(self, all_plans: List[SavePlan]) -> Tuple[List[SavePlan], Metadata]:\n>>>         global_plan, metadata = super().create_global_plan(all_plans)\n>>>         merged_data = [p.planner_data for p in global_plan]\n>>>         metadata = replace(metadata, planner_data=merged_data)\n>>>         return global_plan, metadata\n\nABSTRACT create_global_plan(all_plans)\n[SOURCE]\n\nCompute the global checkpoint plan and return the local plan of each rank.\n\nThis is called on the coordinator rank only.\n\nReturn type\n\nTuple[List[SavePlan], Metadata]\n\nABSTRACT create_local_plan()\n[SOURCE]\n\nCompute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.\n\nThis is called on all ranks.\n\nReturn type\n\nSavePlan\n\nABSTRACT finish_plan(new_plan)\n[SOURCE]\n\nMerge the plan created by create_local_plan and the result of create_global_plan.\n\nThis is called on all ranks.\n\nReturn type\n\nSavePlan\n\nABSTRACT resolve_data(write_item)\n[SOURCE]\n\nLookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it.\n\nCalled on each rank multiple times, at least once per WriteItem in the final SavePlan.\n\nThis method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.\n\nAny transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.\n\nWhen returning tensors, they can be on any device or format, they can be views too. It’s the storage layer responsibility to figure out how to save them.\n\nReturn type\n\nUnion[Tensor, BytesIO]\n\nABSTRACT set_up_planner(state_dict, is_coordinator)\n[SOURCE]\n\nInitialize this planner to save state_dict.\n\nImplementations should save those values as they won’t be provided lated in the save process.\n\nThis is called on all ranks.\n\nCLASS\ntorch.distributed.checkpoint.SavePlan(items: List[torch.distributed.checkpoint.planner.WriteItem], storage_data: Any = None, planner_data: Any = None)\n[SOURCE]\nCLASS\ntorch.distributed.checkpoint.WriteItem(index: torch.distributed.checkpoint.metadata.MetadataIndex, type: torch.distributed.checkpoint.planner.WriteItemType, tensor_data: Union[torch.distributed.checkpoint.planner.TensorWriteData, NoneType] = None)\n[SOURCE]\n\nWe provide a filesystem based storage layer:\n\nCLASS\ntorch.distributed.checkpoint.FileSystemReader(path)\n[SOURCE]\nCLASS\ntorch.distributed.checkpoint.FileSystemWriter(path, single_file_per_rank=True, sync_files=True, thread_count=1, per_thread_copy_ahead=10000000)\n[SOURCE]\n\nBasic implementation of StorageWriter using file IO.\n\nThis implementation makes the following assumptions and simplifications:\n\nThe checkpoint path is an empty or non-existing directory.\n\nFile creation is atomic\n\nThe checkpoint consist of one file per write request plus a .metadata file with the serialized metadata.\n\nWe provide default implementations of LoadPlanner and SavePlanner that can handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.\n\nCLASS\ntorch.distributed.checkpoint.DefaultSavePlanner(flatten_state_dict=True, flatten_sharded_tensors=True, dedup_replicated_tensors=True)\n[SOURCE]\nlookup_object(index)\n[SOURCE]\n\nThis is an extension from the planner interface to make it easy to extend the default planner\n\nReturn type\n\nAny\n\ntransform_object(write_item, object)\n[SOURCE]\n\nThis is an extension from the planner interface to make it easy to extend the default planner\n\nCLASS\ntorch.distributed.checkpoint.DefaultLoadPlanner(flatten_state_dict=True, flatten_sharded_tensors=True)\n[SOURCE]\n\nDefaultLoadPlanner that adds multiple features on top of LoadPlanner.\n\nIn particular it adds the following:\n\nflatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode\n\nlookup_tensor(index)\n[SOURCE]\n\nThis is an extension from the planner interface to make it easy to extend the default planner\n\nReturn type\n\nTensor\n\ntransform_tensor(read_item, tensor)\n[SOURCE]\n\nThis is an extension from the planner interface to make it easy to extend the default planner\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDistributed Checkpoint - torch.distributed.checkpoint\nload_state_dict()\nsave_state_dict()\nStorageReader\nStorageWriter\nLoadPlanner\nLoadPlan\nReadItem\nSavePlanner\nSavePlan\nWriteItem\nFileSystemReader\nFileSystemWriter\nDefaultSavePlanner\nDefaultLoadPlanner\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.compiler — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/torch.compiler.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.compiler\nShortcuts\nTORCH.COMPILER\n\ntorch.compiler is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is torch.compile.\n\ntorch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. torch.compile is written in Python and it marks the transition of PyTorch from C++ to Python.\n\ntorch.compile leverages the following underlying technologies:\n\nTorchDynamo (torch._dynamo) is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the torch.compiler namespace.\n\nTorchInductor is the default torch.compile deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through torch.compile possible. For NVIDIA and AMD GPUs, it leverages OpenAI Triton as the key building block.\n\nAOT Autograd captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor.\n\nNOTE\n\nIn some cases, the terms torch.compile, TorchDynamo, torch.compiler might be used interchangeably in this documentation.\n\nAs mentioned above, to run your workflows faster, torch.compile through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as inductor, TorchDynamo has a list of supported backends developed by our partners, which can be see by running torch.compiler.list_backends() each of which with its optional dependencies.\n\nSome of the most commonly used backends include:\n\nTraining & inference backends\n\nBackend\n\n\t\n\nDescription\n\n\n\n\ntorch.compile(m, backend=\"inductor\")\n\n\t\n\nUses the TorchInductor backend. Read more\n\n\n\n\ntorch.compile(m, backend=\"cudagraphs\")\n\n\t\n\nCUDA graphs with AOT Autograd. Read more\n\n\n\n\ntorch.compile(m, backend=\"ipex\")\n\n\t\n\nUses IPEX on CPU. Read more\n\n\n\n\ntorch.compile(m, backend=\"onnxrt\")\n\n\t\n\nUses ONNX Runtime for training on CPU/GPU. Read more\n\nInference-only backends\n\nBackend\n\n\t\n\nDescription\n\n\n\n\ntorch.compile(m, backend=\"tensorrt\")\n\n\t\n\nUses ONNX Runtime to run TensorRT for inference optimizations. Read more\n\n\n\n\ntorch.compile(m, backend=\"ipex\")\n\n\t\n\nUses IPEX for inference on CPU. Read more\n\n\n\n\ntorch.compile(m, backend=\"tvm\")\n\n\t\n\nUses Apache TVM for inference optimizations. Read more\n\nRead More\n\nGetting Started for PyTorch Users\n\nGetting Started\ntorch.compiler API reference\nPyTorch 2.0 Performance Dashboard\nTorchDynamo APIs for fine-grained tracing\nTorchInductor GPU Profiling\nProfiling to understand torch.compile performance\nFrequently Asked Questions\nPyTorch 2.0 Troubleshooting\n\nDeep Dive for PyTorch Developers\n\nTorchDynamo Deep Dive\nGuards Overview\nDynamic shapes\nPyTorch 2.0 NNModule Support\nBest Practices for Backends\nCUDAGraph Trees\nFake tensor\n\nHowTo for PyTorch Backend Vendors\n\nCustom Backends\nWriting Graph Transformations on ATen IR\nIRs\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.compiler\nRead More\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Tensor Parallelism - torch.distributed.tensor.parallel — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Tensor Parallelism - torch.distributed.tensor.parallel\nShortcuts\nTENSOR PARALLELISM - TORCH.DISTRIBUTED.TENSOR.PARALLEL\n\nTensor Parallelism(TP) is built on top of the PyTorch DistributedTensor (DTensor) and provides several parallelism styles: Rowwise, Colwise and Pairwise Parallelism.\n\nWARNING\n\nTensor Parallelism APIs are experimental and subject to change.\n\nThe entrypoint to parallelize your nn.Module using Tensor Parallelism is:\n\ntorch.distributed.tensor.parallel.parallelize_module(module, device_mesh, parallelize_plan, tp_mesh_dim=0)\n[SOURCE]\n\nThe API to apply Tensor Parallelism (TP) in PyTorch. We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains ParallelStyle, which indicates how user wants the module or sub_module to be parallelized.\n\nUser can also specify different parallel style per module fully qualified name (FQN). The API supports 2D parallelism natively by accepting an n-dimension device_mesh and users just need to specify the dimension where we perform tensor parallelism on.\n\nParameters\n\nmodule (nn.Module) – Module to be parallelized.\n\ndevice_mesh (DeviceMesh) – Object which describes the mesh topology of devices for the DTensor.\n\nparallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]]) – The plan used to parallelize the module. It can be either a ParallelStyle object which contains how we prepare input/output for Tensor Parallelism or it can be a dict of module FQN and its corresponding ParallelStyle object.\n\ntp_mesh_dim (int) – The dimension of device_mesh where we perform Tensor Parallelism on.\n\nReturns\n\nA nn.Module object parallelized.\n\nReturn type\n\nModule\n\nExample::\n>>> from torch.distributed.tensor.parallel import parallelize_module, PairwiseParallel\n>>>\n>>> # Define the module.\n>>> m = Model(...)\n>>> m = parallelize_module(m, PairwiseParallel())\n>>>\n\n\nWARNING\n\nPairwiseParallel comes with constraints for now. If you need finer granularity, you need to pass in a dict of module FQN and parallel style instead.\n\nTensor Parallelism supports the following parallel styles:\n\nCLASS\ntorch.distributed.tensor.parallel.style.RowwiseParallel(_prepare_input=<function make_input_shard_1d_last_dim>, _prepare_output=<function make_output_tensor>)\n[SOURCE]\n\nPartitioning the row of a module. We assume the input to be a sharded DTensor and output to be a torch.Tensor.\n\nCLASS\ntorch.distributed.tensor.parallel.style.ColwiseParallel(_prepare_input=<function make_input_replicate_1d>, _prepare_output=<function make_sharded_output_tensor>)\n[SOURCE]\n\nPartitioning the column of a tensor or module. We assume the input to be a replicated DTensor and output to be a sharded torch.Tensor.\n\nCLASS\ntorch.distributed.tensor.parallel.style.PairwiseParallel(_prepare_input=None, _prepare_output=None)\n[SOURCE]\n\nPairwiseParallel concatenate colwise and rowwise styles as a fixed pair like what Megatron-LM(https://arxiv.org/abs/1909.08053) is doing. We assume both input and output need to be replicate DTensors.\n\nWARNING\n\nPairwiseParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use PairwiseParallel only for even-number-layer MLP for now.\n\nWARNING\n\nSequence Parallelism are still in experimental and no evaluation has been done.\n\nCLASS\ntorch.distributed.tensor.parallel.style.SequenceParallel\n[SOURCE]\n\nSequenceParallel concatenate colwise and rowwise styles as a fixed pair together with sequence parallel like what Megatron-LM Sequence parallel (https://arxiv.org/pdf/2205.05198.pdf) is doing. We assume both input and output need to be sharded DTensors.\n\nWARNING\n\nSequenceParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use SequenceParallel only for even-number-layer MLP for now.\n\nSince Tensor Parallelism is built on top of DTensor, we need to specify the input and output placement of the module with DTensors so it can expectedly interacts with the module before and after. The followings are functions used for input/output preparation:\n\ntorch.distributed.tensor.parallel.style.make_input_replicate_1d(input, device_mesh=None)\n[SOURCE]\n\nReplicate input tensor over an 1-D device mesh. This function will be used in ParallelStyle.\n\nParameters\n\ninput (Union[torch.Tensor, DTensor]) – This input tensor will be replicated over the 1-D DeviceMesh.\n\ndevice_mesh (DeviceMesh, optional) – The 1-D device mesh where input will be replicated. If no DeviceMesh is passed and input is a DTensor, input.device_mesh will be used. If DeviceMesh is not 1-D, an exception will be thrown. Default: None\n\nReturns\n\nA DTensor replicated over device_mesh.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_input_reshard_replicate(input, device_mesh)\n[SOURCE]\n\nTo construct a Sharded DTensor from a tensor on different ranks and then convert to a replicate DTensor.\n\nParameters\n\ninput (torch.Tensor) – The input tensor on each rank which consists of a global DTensor sharded on dimension 0 over the 1-D DeviceMesh and then the sharded DTensor is converted to a replicate DTensor.\n\ndevice_mesh (DeviceMesh, optional) – The 1-D device mesh where input will be sharded. If DeviceMesh is not 1-D, an exception will be thrown. Default: None\n\nReturns\n\nA DTensor sharded on dimension 0 over device_mesh\n\nand then converted to replicate.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_input_shard_1d(input, device_mesh=None, dim=0)\n[SOURCE]\n\nShard input tensor on dim over an 1-D device mesh. This function will be used in ParallelStyle.\n\nParameters\n\ninput (Union[torch.Tensor, DTensor]) – Single tensor will be sharded on dimension dim over the 1-D DeviceMesh.\n\ndevice_mesh (DeviceMesh, optional) – The 1-D device mesh where input will be sharded. If no DeviceMesh is passed and input is a DTensor, input.device_mesh will be used. If DeviceMesh is not 1-D, an exception will be thrown. Default: None\n\ndim (int, optional) – The sharding dimension of input tensor. Default: 0\n\nReturns\n\nA DTensor sharded on dimension dim over device_mesh.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim(input, device_mesh=None)\n[SOURCE]\n\nWrapper func of make_input_shard_1d with dim = -1.\n\nParameters\n\ninput (Union[torch.Tensor, DTensor]) – This single tensor will be sharded on the last dimension over the 1-D DeviceMesh.\n\ndevice_mesh (DeviceMesh, optional) – The 1-D device mesh where input will be sharded. If no DeviceMesh is passed and input is a DTensor, input.device_mesh will be used. If DeviceMesh is not 1-D, an exception will be thrown. Default: None\n\nReturns\n\nA DTensor sharded on the last dimension over device_mesh.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_output_replicate_1d(output, device_mesh=None)\n[SOURCE]\n\nConvert Output DTensor to a replicated DTensor. This will be used in ParallelStyle.\n\nParameters\n\noutput (DTensor) – Output of module to be converted.\n\ndevice_mesh (DeviceMesh, optional) – Object needed to replicate the output and it needs to be a 1D device_mesh and we will throw exceptions if a non-1D device_mesh is passed in. If no device_mesh is passed in, we will reuse the one from output. Default: None\n\nReturns\n\nA DTensor object made replicate.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_output_reshard_tensor(output, device_mesh=None)\n[SOURCE]\n\nConvert Output DTensor to a sharded DTensor and return the local tensor.\n\nParameters\n\noutput (DTensor) – Output of module to be converted.\n\ndevice_mesh (DeviceMesh, optional) – Object needed to shard the output and it needs to be a 1D device_mesh and we will throw exceptions if a non-1D device_mesh is passed in. If no device_mesh is passed in, we will reuse the one from output. Default: None\n\nReturns\n\nA torch.Tensor object converted from output DTensor.\n\nReturn type\n\nTensor\n\ntorch.distributed.tensor.parallel.style.make_output_shard_1d(output, device_mesh=None, dim=0)\n[SOURCE]\n\nConvert Output DTensor to a sharded DTensor. This will be used in ParallelStyle.\n\nParameters\n\noutput (DTensor) – Output of module to be converted.\n\ndevice_mesh (DeviceMesh, optional) – Object needed to shard the output and it needs to be a 1D device_mesh and we will throw exceptions if a non-1D device_mesh is passed in. If no device_mesh is passed in, we will reuse the one from output. Default: None\n\ndim (int) – Sharding dim for output. Default: 0\n\nReturns\n\nA DTensor object sharded on the given dim.\n\nReturn type\n\nDTensor\n\ntorch.distributed.tensor.parallel.style.make_output_tensor(output, device_mesh=None)\n[SOURCE]\n\nConvert Output DTensor to a replicated DTensor first and then convert it to Tensor.\n\nParameters\n\noutput (DTensor) – Output of module to be converted.\n\ndevice_mesh (DeviceMesh, optional) – Object which is needed to replicate the output and it needs to be a 1D device_mesh and we will throw exceptions if a non-1D device_mesh is passed in. If no device_mesh is passed in, we will reuse the one from output. Default: None\n\nReturns\n\nA torch.Tensor object converted from output DTensor.\n\nReturn type\n\nTensor\n\nCurrently, there are some constraints which makes it hard for the MultiheadAttention module to work out of box for Tensor Parallelism, so we recommend users to try ColwiseParallel and RowwiseParallel for each parameter. There might be some code changes needed now since we are parallelizing on the head dim of the MultiheadAttention module.\n\nWe also support 2D parallelism, where we compose tensor parallelism with data parallelism. To integrate with FullyShardedDataParallel, users just need to call the following API explicitly:\n\ntorch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp()\n[SOURCE]\n\nThe API registers the extension which is needed for Tensor Parallelism (TP) to work with FullyShardedDataParallel (FSDP). We first parallelize parameters within one module or sub_modules based on a parallelize_plan and will let FSDP reshard the local tensor of distributed parameter which is essentially a DTensor.\n\nReturns\n\nA bool indicated whether extension registration succeeds or not.\n\nReturn type\n\nbool\n\nTo integrate with DistributedDataParallel, users just need to call the following API explicitly:\n\ntorch.distributed.tensor.parallel.ddp.pre_dp_module_transform(module)\n[SOURCE]\n\nEnable the composability between Tensor Parallelism (TP) and Data Parallelism(DP) in PyTorch when using DDP. We need to convert Parameters which are DTensors to local tensors before wrapping with data parallelism API. We then register two hooks, one for converting local tensors back to DTensor preforward and one to convert DTensors back to tensors after Forward. By integrating this way, we avoid any special handling of DTensor parameters by DDP and get DTensor’s gradients propagated back to DP, e.g. gradient buckets of DDP.\n\nFor now, this API only works with DistributedDataParallel. It will later support other DP methods such as FSDP.\n\nParameters\n\nmodule (nn.Module) – Module which has been applied TP on.\n\nExample::\n>>> from torch.distributed.tensor.parallel import parallelize_module, PairwiseParallel\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> from torch.distributed.tensor.parallel.ddp import pre_dp_module_transform\n>>>\n>>> # Define the module.\n>>> m = module(...)\n>>> parallelize_module(m, PairwiseParallel())\n>>> m = pre_dp_module_transform(m)\n>>> m = DDP(m)\n>>>\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nTensor Parallelism - torch.distributed.tensor.parallel\nparallelize_module()\nRowwiseParallel\nColwiseParallel\nPairwiseParallel\nSequenceParallel\nmake_input_replicate_1d()\nmake_input_reshard_replicate()\nmake_input_shard_1d()\nmake_input_shard_1d_last_dim()\nmake_output_replicate_1d()\nmake_output_reshard_tensor()\nmake_output_shard_1d()\nmake_output_tensor()\nenable_2d_with_fsdp()\npre_dp_module_transform()\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Distributed communication package - torch.distributed — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Distributed communication package - torch.distributed\nShortcuts\nDISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED\n\nNOTE\n\nPlease refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.\n\nBackends\n\ntorch.distributed supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.\n\nBackend\n\n\t\n\ngloo\n\n\t\n\nmpi\n\n\t\n\nnccl\n\n\n\n\nDevice\n\n\t\n\nCPU\n\n\t\n\nGPU\n\n\t\n\nCPU\n\n\t\n\nGPU\n\n\t\n\nCPU\n\n\t\n\nGPU\n\n\n\n\nsend\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nrecv\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nbroadcast\n\n\t\n\n✓\n\n\t\n\n✓\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nall_reduce\n\n\t\n\n✓\n\n\t\n\n✓\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nreduce\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nall_gather\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\ngather\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nscatter\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nreduce_scatter\n\n\t\n\n✘\n\n\t\n\n✘\n\n\t\n\n✘\n\n\t\n\n✘\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nall_to_all\n\n\t\n\n✘\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\n\n\n\nbarrier\n\n\t\n\n✓\n\n\t\n\n✘\n\n\t\n\n✓\n\n\t\n\n?\n\n\t\n\n✘\n\n\t\n\n✓\n\nBackends that come with PyTorch\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)\n\nNOTE\n\nAs of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of init_process_group() points to a file it must adhere to the following schema:\n\nLocal file system, init_method=\"file:///d:/tmp/some_file\"\n\nShared file system, init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"\n\nSame as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.\n\nWhich backend to use?\n\nIn the past, we were often asked: “which backend should I use?”.\n\nRule of thumb\n\nUse the NCCL backend for distributed GPU training\n\nUse the Gloo backend for distributed CPU training.\n\nGPU hosts with InfiniBand interconnect\n\nUse NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect.\n\nGPU hosts with Ethernet interconnect\n\nUse NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)\n\nCPU hosts with InfiniBand interconnect\n\nIf your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.\n\nCPU hosts with Ethernet interconnect\n\nUse Gloo, unless you have specific reasons to use MPI.\n\nCommon environment variables\nChoosing the network interface to use\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):\n\nNCCL_SOCKET_IFNAME, for example export NCCL_SOCKET_IFNAME=eth0\n\nGLOO_SOCKET_IFNAME, for example export GLOO_SOCKET_IFNAME=eth0\n\nIf you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.\n\nOther NCCL environment variables\n\nDebugging - in case of NCCL failure, you can set NCCL_DEBUG=INFO to print an explicit warning message as well as basic NCCL initialization information.\n\nYou may also use NCCL_DEBUG_SUBSYS to get more details about a specific aspect of NCCL. For example, NCCL_DEBUG_SUBSYS=COLL would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set NCCL_DEBUG_SUBSYS=GRAPH to inspect the detailed detection result and save as reference if further help from NCCL team is needed.\n\nPerformance tuning - NCCL performs automatic tuning based on its topology detection to save users’ tuning effort. On some socket-based systems, users may still try tuning NCCL_SOCKET_NTHREADS and NCCL_NSOCKS_PERTHREAD to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP.\n\nFor a full list of NCCL environment variables, please refer to NVIDIA NCCL’s official documentation\n\nBasics\n\nThe torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class torch.nn.parallel.DistributedDataParallel() builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.\n\nIn the single-machine synchronous case, torch.distributed or the torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other approaches to data-parallelism, including torch.nn.DataParallel():\n\nEach process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.\n\nEach process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.\n\nInitialization\n\nThe package needs to be initialized using the torch.distributed.init_process_group() function before calling any other methods. This blocks until all processes have joined.\n\ntorch.distributed.is_available()\n[SOURCE]\n\nReturns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS.\n\nReturn type\n\nbool\n\ntorch.distributed.init_process_group(backend=None, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='', pg_options=None)\n[SOURCE]\n\nInitializes the default distributed process group, and this will also initialize the distributed package.\n\nThere are 2 main ways to initialize a process group:\n\nSpecify store, rank, and world_size explicitly.\n\nSpecify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.\n\nIf neither is specified, init_method is assumed to be “env://”.\n\nParameters\n\nbackend (str or Backend, optional) – The backend to use. Depending on build-time configurations, valid values include mpi, gloo, nccl, and ucc. If the backend is not provided, then both a gloo and nccl backend will be created, see notes below for how multiple backends are managed. This field can be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. ucc backend is experimental.\n\ninit_method (str, optional) – URL specifying how to initialize the process group. Default is “env://” if no init_method or store is specified. Mutually exclusive with store.\n\nworld_size (int, optional) – Number of processes participating in the job. Required if store is specified.\n\nrank (int, optional) – Rank of the current process (it should be a number between 0 and world_size-1). Required if store is specified.\n\nstore (Store, optional) – Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method.\n\ntimeout (timedelta, optional) – Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. For ucc, blocking wait is supported similar to NCCL. However, async error handling is done differently since with UCC we have progress thread and not watch-dog thread.\n\ngroup_name (str, optional, deprecated) – Group name. This argument is ignored\n\npg_options (ProcessGroupOptions, optional) – process group options specifying what additional options need to be passed in during the construction of specific process groups. As of now, the only options we support is ProcessGroupNCCL.Options for the nccl backend, is_high_priority_stream can be specified so that the nccl backend can pick up high priority cuda streams when there’re compute kernels waiting.\n\nNOTE\n\nTo enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI.\n\nNOTE\n\nSupport for multiple backends is experimental. Currently when no backend is specified, both gloo and nccl backends will be created. The gloo backend will be used for collectives with CPU tensors and the nccl backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g. “cpu:gloo,cuda:custom_backend”.\n\ntorch.distributed.is_initialized()\n[SOURCE]\n\nChecking if the default process group has been initialized\n\nReturn type\n\nbool\n\ntorch.distributed.is_mpi_available()\n[SOURCE]\n\nChecks if the MPI backend is available.\n\nReturn type\n\nbool\n\ntorch.distributed.is_nccl_available()\n[SOURCE]\n\nChecks if the NCCL backend is available.\n\nReturn type\n\nbool\n\ntorch.distributed.is_gloo_available()\n[SOURCE]\n\nChecks if the Gloo backend is available.\n\nReturn type\n\nbool\n\ntorch.distributed.is_torchelastic_launched()\n[SOURCE]\n\nChecks whether this process was launched with torch.distributed.elastic (aka torchelastic). The existence of TORCHELASTIC_RUN_ID environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since TORCHELASTIC_RUN_ID maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..\n\nReturn type\n\nbool\n\nCurrently three initialization methods are supported:\n\nTCP initialization\n\nThere are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired world_size. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.\n\nNote that multicast address is not supported anymore in the latest distributed package. group_name is deprecated as well.\n\nimport torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n\nShared file-system initialization\n\nAnother initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired world_size. The URL should start with file:// and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next init_process_group() call on the same file path/name.\n\nNote that automatic rank assignment is not supported anymore in the latest distributed package and group_name is deprecated as well.\n\nWARNING\n\nThis method assumes that the file system supports locking using fcntl - most local systems and NFS support it.\n\nWARNING\n\nThis method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call init_process_group() multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call init_process_group() again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time init_process_group() is called.\n\nimport torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n\nEnvironment variable initialization\n\nThis method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:\n\nMASTER_PORT - required; has to be a free port on machine with rank 0\n\nMASTER_ADDR - required (except for rank 0); address of rank 0 node\n\nWORLD_SIZE - required; can be set either here, or in a call to init function\n\nRANK - required; can be set either here, or in a call to init function\n\nThe machine with rank 0 will be used to set up all connections.\n\nThis is the default method, meaning that init_method does not have to be specified (or can be env://).\n\nPost-Initialization\n\nOnce torch.distributed.init_process_group() was run, the following functions can be used. To check whether the process group has already been initialized use torch.distributed.is_initialized().\n\nCLASS\ntorch.distributed.Backend(name)\n[SOURCE]\n\nAn enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered backends.\n\nThe values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL.\n\nThis class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".\n\nNOTE\n\nThe entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.\n\nCLASSMETHOD register_backend(name, func, extended_api=False, devices=None)\n[SOURCE]\n\nRegisters a new backend with the given name and instantiating function.\n\nThis class method is used by 3rd party ProcessGroup extension to register new backends.\n\nParameters\n\nname (str) – Backend name of the ProcessGroup extension. It should match the one in init_process_group().\n\nfunc (function) – Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including store, rank, world_size, and timeout.\n\nextended_api (bool, optional) – Whether the backend supports extended argument structure. Default: False. If set to True, the backend will get an instance of c10d::DistributedBackendOptions, and a process group options object as defined by the backend implementation.\n\ndevice (str or list of str, optional) – device type this backend supports, e.g. “cpu”, “cuda”, etc. If None, assuming both “cpu” and “cuda”\n\nNOTE\n\nThis support of 3rd party backend is experimental and subject to change.\n\ntorch.distributed.get_backend(group=None)\n[SOURCE]\n\nReturns the backend of the given process group.\n\nParameters\n\ngroup (ProcessGroup, optional) – The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.\n\nReturns\n\nThe backend of the given process group as a lower case string.\n\nReturn type\n\nstr\n\ntorch.distributed.get_rank(group=None)\n[SOURCE]\n\nReturns the rank of the current process in the provided group or the default group if none was provided.\n\nRank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.\n\nParameters\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nReturns\n\nThe rank of the process group -1, if not part of the group\n\nReturn type\n\nint\n\ntorch.distributed.get_world_size(group=None)\n[SOURCE]\n\nReturns the number of processes in the current process group\n\nParameters\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nReturns\n\nThe world size of the process group -1, if not part of the group\n\nReturn type\n\nint\n\nDistributed Key-Value Store\n\nThe distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in torch.distributed.init_process_group() (by explicitly creating the store as an alternative to specifying init_method.) There are 3 choices for Key-Value Stores: TCPStore, FileStore, and HashStore.\n\nCLASS\ntorch.distributed.Store\n\nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore).\n\nCLASS\ntorch.distributed.TCPStore\n\nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection.\n\nParameters\n\nhost_name (str) – The hostname or IP Address the server store should run on.\n\nport (int) – The port on which the server store should listen for incoming requests.\n\nworld_size (int, optional) – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).\n\nis_master (bool, optional) – True when initializing the server store and False for client stores. Default is False.\n\ntimeout (timedelta, optional) – Timeout used by the store during initialization and for methods such as get() and wait(). Default is timedelta(seconds=300)\n\nwait_for_worker (bool, optional) – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.\n\nmulti_tenant (bool, optional) – If True, all TCPStore instances in the current process with the same host/port will use the same underlying TCPServer. Default is False.\n\nmaster_listen_fd (int, optional) – If specified, the underlying TCPServer will listen on this file descriptor, which must be a socket already bound to port. Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to port).\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n\nCLASS\ntorch.distributed.HashStore\n\nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.\n\nExample::\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n\nCLASS\ntorch.distributed.FileStore\n\nA store implementation that uses a file to store the underlying key-value pairs.\n\nParameters\n\nfile_name (str) – path of the file in which to store the key-value pairs\n\nworld_size (int, optional) – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).\n\nExample::\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n\nCLASS\ntorch.distributed.PrefixStore\n\nA wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.\n\nParameters\n\nprefix (str) – The prefix string that is prepended to each key before being inserted into the store.\n\nstore (torch.distributed.store) – A store object that forms the underlying key-value store.\n\ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) → None\n\nInserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.\n\nParameters\n\nkey (str) – The key to be added to the store.\n\nvalue (str) – The value associated with key to be added to the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) → bytes\n\nRetrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.\n\nParameters\n\nkey (str) – The function will return the value associated with this key.\n\nReturns\n\nValue associated with key if key is in the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) → int\n\nThe first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.\n\nParameters\n\nkey (str) – The key in the store whose counter will be incremented.\n\namount (int) – The quantity by which the counter will be incremented.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n\ntorch.distributed.Store.compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) → bytes\n\nInserts the key-value pair into the store based on the supplied key and performs comparison between expected_value and desired_value before inserting. desired_value will only be set if expected_value for the key already exists in the store or if expected_value is an empty string.\n\nParameters\n\nkey (str) – The key to be checked in the store.\n\nexpected_value (str) – The value associated with key to be checked before insertion.\n\ndesired_value (str) – The value associated with key to be added to the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")\n\ntorch.distributed.Store.wait(*args, **kwargs)\n\nOverloaded function.\n\nwait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -> None\n\nWaits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.\n\nParameters\n\nkeys (list) – List of keys on which to wait until they are set in the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n\n\nwait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -> None\n\nWaits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.\n\nParameters\n\nkeys (list) – List of keys on which to wait until they are set in the store.\n\ntimeout (timedelta) – Time to wait for the keys to be added before throwing an exception.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n\ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) → int\n\nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.\n\nWARNING\n\nWhen used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.\n\nReturns\n\nThe number of keys present in the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n\ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) → bool\n\nDeletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.\n\nWARNING\n\nThe delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.\n\nParameters\n\nkey (str) – The key to be deleted from the store\n\nReturns\n\nTrue if key was deleted, otherwise False.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n\ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) → None\n\nSets the store’s default timeout. This timeout is used during initialization and in wait() and get().\n\nParameters\n\ntimeout (timedelta) – timeout to be set in the store.\n\nExample::\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n\nGroups\n\nBy default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).\n\ntorch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None, use_local_synchronization=False)\n[SOURCE]\n\nCreates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\n\nWARNING\n\nUsing multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.\n\nParameters\n\nranks (list[int]) – List of ranks of group members. If None, will be set to all ranks. Default is None.\n\ntimeout (timedelta, optional) – Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set.\n\nbackend (str or Backend, optional) – The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If None is passed in, the backend corresponding to the default process group will be used. Default is None.\n\npg_options (ProcessGroupOptions, optional) – process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the nccl backend, is_high_priority_stream can be specified so that process group can pick up high priority cuda streams.\n\nuse_local_synchronization (bool, optional) – perform a group-local barrier at the end of the process group creation. This is different in that non-member ranks don’t need to call into API and don’t join the barrier.\n\nReturns\n\nA handle of distributed group that can be given to collective calls or None if the rank is not part of ranks.\n\nN.B. use_local_synchronization doesn’t work with MPI.\n\nN.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks don’t join the group barrier().\n\nN.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order.\n\ntorch.distributed.get_group_rank(group, global_rank)\n[SOURCE]\n\nTranslate a global rank into a group rank.\n\nglobal_rank must be part of group otherwise this raises RuntimeError.\n\nParameters\n\ngroup (ProcessGroup) – ProcessGroup to find the relative rank.\n\nglobal_rank (int) – Global rank to query.\n\nReturns\n\nGroup rank of global_rank relative to group\n\nReturn type\n\nint\n\nN.B. calling this function on the default process group returns identity\n\ntorch.distributed.get_global_rank(group, group_rank)\n[SOURCE]\n\nTranslate a group rank into a global rank.\n\ngroup_rank must be part of group otherwise this raises RuntimeError.\n\nParameters\n\ngroup (ProcessGroup) – ProcessGroup to find the global rank from.\n\ngroup_rank (int) – Group rank to query.\n\nReturns\n\nGlobal rank of group_rank relative to group\n\nReturn type\n\nint\n\nN.B. calling this function on the default process group returns identity\n\ntorch.distributed.get_process_group_ranks(group)\n[SOURCE]\n\nGet all ranks associated with group.\n\nParameters\n\ngroup (ProcessGroup) – ProcessGroup to get all ranks from.\n\nReturns\n\nList of global ranks ordered by group rank.\n\nPoint-to-point communication\ntorch.distributed.send(tensor, dst, group=None, tag=0)\n[SOURCE]\n\nSends a tensor synchronously.\n\nParameters\n\ntensor (Tensor) – Tensor to send.\n\ndst (int) – Destination rank. Destination rank should not be the same\n\nprocess. (as the rank of the current) –\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntag (int, optional) – Tag to match send with remote recv\n\ntorch.distributed.recv(tensor, src=None, group=None, tag=0)\n[SOURCE]\n\nReceives a tensor synchronously.\n\nParameters\n\ntensor (Tensor) – Tensor to fill with received data.\n\nsrc (int, optional) – Source rank. Will receive from any process if unspecified.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntag (int, optional) – Tag to match recv with remote send\n\nReturns\n\nSender rank -1, if not part of the group\n\nReturn type\n\nint\n\nisend() and irecv() return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:\n\nis_completed() - returns True if the operation has finished\n\nwait() - will block the process until the operation is finished. is_completed() is guaranteed to return True once it returns.\n\ntorch.distributed.isend(tensor, dst, group=None, tag=0)\n[SOURCE]\n\nSends a tensor asynchronously.\n\nWARNING\n\nModifying tensor before the request completes causes undefined behavior.\n\nWARNING\n\ntag is not supported with the NCCL backend.\n\nParameters\n\ntensor (Tensor) – Tensor to send.\n\ndst (int) – Destination rank.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntag (int, optional) – Tag to match send with remote recv\n\nReturns\n\nA distributed request object. None, if not part of the group\n\nReturn type\n\nWork\n\ntorch.distributed.irecv(tensor, src=None, group=None, tag=0)\n[SOURCE]\n\nReceives a tensor asynchronously.\n\nWARNING\n\ntag is not supported with the NCCL backend.\n\nParameters\n\ntensor (Tensor) – Tensor to fill with received data.\n\nsrc (int, optional) – Source rank. Will receive from any process if unspecified.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntag (int, optional) – Tag to match recv with remote send\n\nReturns\n\nA distributed request object. None, if not part of the group\n\nReturn type\n\nWork\n\ntorch.distributed.batch_isend_irecv(p2p_op_list)\n[SOURCE]\n\nSend or Receive a batch of tensors asynchronously and return a list of requests.\n\nProcess each of the operations in p2p_op_list and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.\n\nParameters\n\np2p_op_list – A list of point-to-point operations(type of each operator is torch.distributed.P2POp). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.\n\nReturns\n\nA list of distributed request objects returned by calling the corresponding op in the op_list.\n\nExamples\n\n>>> send_tensor = torch.arange(2) + 2 * rank\n>>> recv_tensor = torch.randn(2)\n>>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)\n>>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)\n>>> reqs = batch_isend_irecv([send_op, recv_op])\n>>> for req in reqs:\n>>>     req.wait()\n>>> recv_tensor\ntensor([2, 3])     # Rank 0\ntensor([0, 1])     # Rank 1\n\n\nNOTE\n\nNote that when this API is used with the NCCL PG backend, users must set the current GPU device with torch.cuda.set_device, otherwise it will lead to unexpected hang issues.\n\nIn addition, if this API is the first collective call in the group passed to dist.P2POp, all ranks of the group must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the group, batched P2P operations involving only a subset of ranks of the group are allowed.\n\nCLASS\ntorch.distributed.P2POp(op, tensor, peer, group=None, tag=0)\n[SOURCE]\n\nA class to build point-to-point operations for batch_isend_irecv.\n\nThis class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to batch_isend_irecv for point-to-point communications.\n\nParameters\n\nop (Callable) – A function to send data to or receive data from a peer process. The type of op is either torch.distributed.isend or torch.distributed.irecv.\n\ntensor (Tensor) – Tensor to send or receive.\n\npeer (int) – Destination or source rank.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntag (int, optional) – Tag to match send with recv.\n\nSynchronous and asynchronous collective operations\n\nEvery collective operation function supports the following two kinds of operations, depending on the setting of the async_op flag passed into the collective:\n\nSynchronous operation - the default mode, when async_op is set to False. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations.\n\nAsynchronous operation - when async_op is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:\n\nis_completed() - in the case of CPU collectives, returns True if completed. In the case of CUDA operations, returns True if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.\n\nwait() - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.\n\nget_future() - returns torch._C.Future object. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs, get_future() call might become redundant.\n\nExample\n\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n\n# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n\nCollective functions\ntorch.distributed.broadcast(tensor, src, group=None, async_op=False)\n[SOURCE]\n\nBroadcasts the tensor to the whole group.\n\ntensor must have the same number of elements in all processes participating in the collective.\n\nParameters\n\ntensor (Tensor) – Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise.\n\nsrc (int) – Source rank.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)\n[SOURCE]\n\nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.\n\nParameters\n\nobject_list (List[Any]) – List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes.\n\nsrc (int) – Source rank from which to broadcast object_list.\n\ngroup – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.\n\ndevice (torch.device, optional) – If not None, the objects are serialized and converted to tensors which are moved to the device before broadcasting. Default is None.\n\nReturns\n\nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.\n\nNOTE\n\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().\n\nNOTE\n\nNote that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.\n\nWARNING\n\nbroadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\n\nWARNING\n\nCalling broadcast_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using broadcast() instead.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> dist.broadcast_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n\ntorch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduces the tensor data across all machines in such a way that all get the final result.\n\nAfter the call tensor is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nParameters\n\ntensor (Tensor) – Input and output of the collective. The function operates in-place.\n\nop (optional) – One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\nExamples\n\n>>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n\n>>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n\ntorch.distributed.reduce(tensor, dst, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduces the tensor data across all machines.\n\nOnly the process with rank dst is going to receive the final result.\n\nParameters\n\ntensor (Tensor) – Input and output of the collective. The function operates in-place.\n\ndst (int) – Destination rank\n\nop (optional) – One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)\n[SOURCE]\n\nGathers tensors from the whole group in a list.\n\nComplex tensors are supported.\n\nParameters\n\ntensor_list (list[Tensor]) – Output list. It should contain correctly-sized tensors to be used for output of the collective.\n\ntensor (Tensor) – Tensor to be broadcast from current process.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\nExamples\n\n>>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n\n>>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n\ntorch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)\n[SOURCE]\n\nGather tensors from all ranks and put them in a single output tensor.\n\nParameters\n\noutput_tensor (Tensor) – Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of “concatenation”, see torch.cat(); (ii) a stack of all the input tensors along the primary dimension; for definition of “stack”, see torch.stack(). Examples below may better explain the supported output forms.\n\ninput_tensor (Tensor) – Tensor to be gathered from current rank. Different from the all_gather API, the input tensors in this API must have the same size across all ranks.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\nExamples\n\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor_in\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> # Output in concatenation form\n>>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([1, 2, 3, 4], device='cuda:0') # Rank 0\ntensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n>>> # Output in stack form\n>>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n>>> tensor_out2\ntensor([[1, 2],\n        [3, 4]], device='cuda:0') # Rank 0\ntensor([[1, 2],\n        [3, 4]], device='cuda:1') # Rank 1\n\n\nWARNING\n\nThe Gloo backend does not support this API.\n\ntorch.distributed.all_gather_object(object_list, obj, group=None)\n[SOURCE]\n\nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.\n\nParameters\n\nobject_list (list[Any]) – Output list. It should be correctly sized as the size of the group for this collective and will contain the output.\n\nobj (Any) – Pickable Python object to be broadcast from current process.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used. Default is None.\n\nReturns\n\nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.\n\nNOTE\n\nNote that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.\n\nNOTE\n\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().\n\nWARNING\n\nall_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\n\nWARNING\n\nCalling all_gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using all_gather() instead.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n\ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False)\n[SOURCE]\n\nGathers a list of tensors in a single process.\n\nParameters\n\ntensor (Tensor) – Input tensor.\n\ngather_list (list[Tensor], optional) – List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)\n\ndst (int, optional) – Destination rank (default is 0)\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)\n[SOURCE]\n\nGathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.\n\nParameters\n\nobj (Any) – Input object. Must be picklable.\n\nobject_gather_list (list[Any]) – Output list. On the dst rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be None on non-dst ranks. (default is None)\n\ndst (int, optional) – Destination rank. (default is 0)\n\ngroup – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.\n\nReturns\n\nNone. On the dst rank, object_gather_list will contain the output of the collective.\n\nNOTE\n\nNote that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.\n\nNOTE\n\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().\n\nWARNING\n\ngather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\n\nWARNING\n\nCalling gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using gather() instead.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n...     gather_objects[dist.get_rank()],\n...     output if dist.get_rank() == 0 else None,\n...     dst=0\n... )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n\ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False)\n[SOURCE]\n\nScatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the tensor argument.\n\nComplex tensors are supported.\n\nParameters\n\ntensor (Tensor) – Output tensor.\n\nscatter_list (list[Tensor]) – List of tensors to scatter (default is None, must be specified on the source rank)\n\nsrc (int) – Source rank (default is 0)\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\nNOTE\n\nNote that all Tensors in scatter_list must have the same size.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> tensor_size = 2\n>>> t_ones = torch.ones(tensor_size)\n>>> t_fives = torch.ones(tensor_size) * 5\n>>> output_tensor = torch.zeros(tensor_size)\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     # Only tensors, all of which must be the same size.\n>>>     scatter_list = [t_ones, t_fives]\n>>> else:\n>>>     scatter_list = None\n>>> dist.scatter(output_tensor, scatter_list, src=0)\n>>> # Rank i gets scatter_list[i]. For example, on rank 1:\n>>> output_tensor\ntensor([5., 5.])\n\ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None)\n[SOURCE]\n\nScatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.\n\nParameters\n\nscatter_object_output_list (List[Any]) – Non-empty list whose first element will store the object scattered to this rank.\n\nscatter_object_input_list (List[Any]) – List of input objects to scatter. Each object must be picklable. Only objects on the src rank will be scattered, and the argument can be None for non-src ranks.\n\nsrc (int) – Source rank from which to scatter scatter_object_input_list.\n\ngroup – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.\n\nReturns\n\nNone. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.\n\nNOTE\n\nNote that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.\n\nWARNING\n\nscatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\n\nWARNING\n\nCalling scatter_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using scatter() instead.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n\ntorch.distributed.reduce_scatter(output, input_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduces, then scatters a list of tensors to all processes in a group.\n\nParameters\n\noutput (Tensor) – Output tensor.\n\ninput_list (list[Tensor]) – List of tensors to reduce and scatter.\n\nop (optional) – One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n\ntorch.distributed.reduce_scatter_tensor(output, input, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduces, then scatters a tensor to all ranks in a group.\n\nParameters\n\noutput (Tensor) – Output tensor. It should have the same size across all ranks.\n\ninput (Tensor) – Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of “concatenation”, see torch.cat(). For definition of “stack”, see torch.stack().\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n\nExamples\n\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n>>> # Input in concatenation form\n>>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n>>> tensor_in\ntensor([0, 1, 2, 3], device='cuda:0') # Rank 0\ntensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n>>> # Input in stack form\n>>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n>>> tensor_in\ntensor([[0, 1],\n        [2, 3]], device='cuda:0') # Rank 0\ntensor([[0, 1],\n        [2, 3]], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n\n\nWARNING\n\nThe Gloo backend does not support this API.\n\ntorch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)\n[SOURCE]\n\nEach process splits input tensor and then scatters the split list to all processes in a group. Then concatenate the received tensors from all the processes in the group and return single output tensor.\n\nComplex tensors are supported.\n\nParameters\n\noutput (Tensor) – Gathered concatenated output tensor.\n\ninput (Tensor) – Input tensor to scatter.\n\noutput_split_sizes – (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of output tensor must divide equally by world_size.\n\ninput_split_sizes – (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of input tensor must divide equally by world_size.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n\nWARNING\n\nall_to_all_single is experimental and subject to change.\n\nExamples\n\n>>> input = torch.arange(4) + rank * 4\n>>> input\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\ntensor([8, 9, 10, 11])   # Rank 2\ntensor([12, 13, 14, 15]) # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\ntensor([2, 6, 10, 14])   # Rank 2\ntensor([3, 7, 11, 15])   # Rank 3\n\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = list(input.chunk(world_size))\n>>> gather_list  = list(output.chunk(world_size))\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n\n>>> # Another example with uneven split\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> output = ...\n>>> dist.all_to_all_single(output, input, output_splits, input_splits)\n>>> output\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n\ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)\n[SOURCE]\n\nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.\n\nComplex tensors are supported.\n\nParameters\n\noutput_tensor_list (list[Tensor]) – List of tensors to be gathered one per rank.\n\ninput_tensor_list (list[Tensor]) – List of tensors to scatter one per rank.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n\nWARNING\n\nall_to_all is experimental and subject to change.\n\nExamples\n\n>>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n\ntorch.distributed.barrier(group=None, async_op=False, device_ids=None)\n[SOURCE]\n\nSynchronizes all processes.\n\nThis collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().\n\nParameters\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\ndevice_ids ([int], optional) – List of device/GPU ids.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.monitored_barrier(group=None, timeout=None, wait_all_ranks=False)\n[SOURCE]\n\nSynchronizes all processes similar to torch.distributed.barrier, but takes a configurable timeout and is able to report ranks that did not pass this barrier within that timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\n\nThis collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application’s collective calls to check if any ranks are desynchronized.\n\nNOTE\n\nNote that this collective is only supported with the GLOO backend.\n\nParameters\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\ntimeout (datetime.timedelta, optional) – Timeout for monitored_barrier. If None, the default process group timeout will be used.\n\nwait_all_ranks (bool, optional) – Whether to collect all failed ranks or not. By default, this is False and monitored_barrier on rank 0 will throw on the first failed rank it encounters in order to fail fast. By setting wait_all_ranks=True monitored_barrier will collect all failed ranks and throw an error containing information about all failed ranks.\n\nReturns\n\nNone.\n\nExample::\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() != 1:\n>>>     dist.monitored_barrier() # Raises exception indicating that\n>>> # rank 1 did not call into monitored_barrier.\n>>> # Example with wait_all_ranks=True\n>>> if dist.get_rank() == 0:\n>>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n>>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n>>> # monitored_barrier.\n\nCLASS\ntorch.distributed.ReduceOp\n\nAn enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, BXOR, and PREMUL_SUM.\n\nBAND, BOR, and BXOR reductions are not available when using the NCCL backend.\n\nAVG divides values by the world size before summing across ranks. AVG is only available with the NCCL backend, and only for NCCL versions 2.10 or later.\n\nPREMUL_SUM multiplies inputs by a given scalar locally before reduction. PREMUL_SUM is only available with the NCCL backend, and only available for NCCL versions 2.11 or later. Users are supposed to use torch.distributed._make_nccl_premul_sum.\n\nAdditionally, MAX, MIN and PRODUCT are not supported for complex tensors.\n\nThe values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc.\n\nThis class does not support __members__ property.\n\nCLASS\ntorch.distributed.reduce_op\n\nDeprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX.\n\nReduceOp is recommended to use instead.\n\nProfiling Collective Communication\n\nNote that you can use torch.profiler (recommended, only available after 1.8.1) or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo, nccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\n\nimport torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n\n\nPlease refer to the profiler documentation for a full overview of profiler features.\n\nMulti-GPU collective functions\n\nWARNING\n\nThe multi-GPU functions will be deprecated. If you must use them, please revisit our documentation later.\n\nIf you have more than one GPU on each node, when using the NCCL and Gloo backend, broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() all_gather_multigpu() and reduce_scatter_multigpu() support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.\n\nFor example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:\n\nCode running on Node 0\n\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n\n\nCode running on Node 1\n\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n\n\nAfter the call, all 16 tensors on the two nodes will have the all-reduced value of 16\n\ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0)\n[SOURCE]\n\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\ntensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU\n\nOnly nccl and gloo backend are currently supported tensors should only be GPU tensors\n\nParameters\n\ntensor_list (List[Tensor]) – Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function.\n\nsrc (int) – Source rank.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nsrc_tensor (int, optional) – Source tensor rank within tensor_list\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.all_reduce_multigpu(tensor_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.\n\nAfter the call, all tensor in tensor_list is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nOnly nccl and gloo backend is currently supported tensors should only be GPU tensors\n\nParameters\n\ntensor_list (List[Tensor]) – List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function.\n\nop (optional) – One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.reduce_multigpu(tensor_list, dst, op=<RedOpType.SUM: 0>, group=None, async_op=False, dst_tensor=0)\n[SOURCE]\n\nReduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU\n\nOnly the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result.\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nParameters\n\ntensor_list (List[Tensor]) – Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function.\n\ndst (int) – Destination rank\n\nop (optional) – One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\ndst_tensor (int, optional) – Destination tensor rank within tensor_list\n\nReturns\n\nAsync work handle, if async_op is set to True. None, otherwise\n\ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False)\n[SOURCE]\n\nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nComplex tensors are supported.\n\nParameters\n\noutput_tensor_lists (List[List[Tensor]]) –\n\nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i].\n\nNote that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j]\n\nAlso note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.\n\ninput_tensor_list (List[Tensor]) – List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n\ntorch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n[SOURCE]\n\nReduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.\n\nEach tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.\n\nParameters\n\noutput_tensor_list (List[Tensor]) –\n\nOutput tensors (on different GPUs) to receive the result of the operation.\n\nNote that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.\n\ninput_tensor_lists (List[List[Tensor]]) –\n\nInput lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i].\n\nNote that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j]\n\nAlso note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.\n\ngroup (ProcessGroup, optional) – The process group to work on. If None, the default process group will be used.\n\nasync_op (bool, optional) – Whether this op should be an async op.\n\nReturns\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n\nThird-party backends\n\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party backends are decided by their own implementations.\n\nThe new backend derives from c10d::ProcessGroup and registers the backend name and the instantiating interface through torch.distributed.Backend.register_backend() when imported.\n\nWhen manually importing this backend and invoking torch.distributed.init_process_group() with the corresponding backend name, the torch.distributed package runs on the new backend.\n\nWARNING\n\nThe support of third-party backend is experimental and subject to change.\n\nLaunch utility\n\nThe torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training.\n\ntorch.distributed.launch is a module that spawns up multiple distributed training processes on each of the training nodes.\n\nWARNING\n\nThis module is going to be deprecated in favor of torchrun.\n\nThe utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.\n\nIn both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (--nproc-per-node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1).\n\nHow to use this module:\n\nSingle-Node multi-process distributed training\n\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n\n\nMulti-Node multi-process distributed training: (e.g. two nodes)\n\nNode 1: (IP: 192.168.1.1, and has a free port: 1234)\n\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=0 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n\n\nNode 2:\n\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=1 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n\n\nTo look up what optional arguments this module offers:\n\npython -m torch.distributed.launch --help\n\n\nImportant Notices:\n\n1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.\n\n2. In your training program, you must parse the command-line argument: --local-rank=LOCAL_PROCESS_RANK, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\n\nParsing the local_rank argument\n\n>>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local-rank\", type=int)\n>>> args = parser.parse_args()\n\n\nSet your device to local rank using either\n\n>>> torch.cuda.set_device(args.local_rank)  # before your code runs\n\n\nor\n\n>>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n>>>    ...\n\n\n3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that init_method=env://. Other init methods (e.g. tcp://) may work, but env:// is the one that is officially supported by this module.\n\n>>> torch.distributed.init_process_group(backend='YOUR BACKEND',\n>>>                                      init_method='env://')\n\n\n4. In your training program, you can either use regular distributed functions or use torch.nn.parallel.DistributedDataParallel() module. If your training program uses GPUs for training and you would like to use torch.nn.parallel.DistributedDataParallel() module, here is how to configure it.\n\n>>> model = torch.nn.parallel.DistributedDataParallel(model,\n>>>                                                   device_ids=[args.local_rank],\n>>>                                                   output_device=args.local_rank)\n\n\nPlease ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [args.local_rank], and output_device needs to be args.local_rank in order to use this utility\n\n5. Another way to pass local_rank to the subprocesses via environment variable LOCAL_RANK. This behavior is enabled when you launch the script with --use-env=True. You must adjust the subprocess example above to replace args.local_rank with os.environ['LOCAL_RANK']; the launcher will not pass --local-rank when you specify this flag.\n\nWARNING\n\nlocal_rank is NOT globally unique: it is only unique per process on a machine. Thus, don’t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don’t do this correctly.\n\nSpawn utility\n\nThe Multiprocessing package - torch.multiprocessing package also provides a spawn function in torch.multiprocessing.spawn(). This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.\n\nFor references on how to use it, please refer to PyTorch example - ImageNet implementation\n\nNote that this function requires Python 3.4 or higher.\n\nDebugging torch.distributed applications\n\nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. torch.distributed provides a suite of tools to help debug training applications in a self-serve fashion:\n\nMonitored Barrier\n\nAs of v1.10, torch.distributed.monitored_barrier() exists as an alternative to torch.distributed.barrier() which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into torch.distributed.monitored_barrier() within the provided timeout. torch.distributed.monitored_barrier() implements a host-side barrier using send/recv communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into torch.distributed.monitored_barrier() (in practice this could be due to an application bug or hang in a previous collective):\n\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=())\n\n\nThe following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\n\nRuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n Original exception:\n[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n\nTORCH_DISTRIBUTED_DEBUG\n\nWith TORCH_CPP_LOG_LEVEL=INFO, the environment variable TORCH_DISTRIBUTED_DEBUG can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. TORCH_DISTRIBUTED_DEBUG can be set to either OFF (default), INFO, or DETAIL depending on the debugging level required. Please note that the most verbose option, DETAIL may impact the application performance and thus should only be used when debugging issues.\n\nSetting TORCH_DISTRIBUTED_DEBUG=INFO will result in additional debug logging when models trained with torch.nn.parallel.DistributedDataParallel() are initialized, and TORCH_DISTRIBUTED_DEBUG=DETAIL will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\n\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=())\n\n\nThe following logs are rendered at initialization time:\n\nI0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\nbroadcast_buffers: 1\nbucket_cap_bytes: 26214400\nfind_unused_parameters: 0\ngradient_as_bucket_view: 0\nis_multi_device_module: 0\niteration: 0\nnum_parameter_tensors: 2\noutput_device: 0\nrank: 0\ntotal_parameter_size_bytes: 440\nworld_size: 2\nbackend_name: nccl\nbucket_sizes: 440\ncuda_visible_devices: N/A\ndevice_ids: 0\ndtypes: float\nmaster_addr: localhost\nmaster_port: 29501\nmodule_name: TwoLinLayerNet\nnccl_async_error_handling: N/A\nnccl_blocking_wait: N/A\nnccl_debug: WARN\nnccl_ib_timeout: N/A\nnccl_nthreads: N/A\nnccl_socket_ifname: N/A\ntorch_distributed_debug: INFO\n\n\nThe following logs are rendered during runtime (when TORCH_DISTRIBUTED_DEBUG=DETAIL is set):\n\nI0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 40838608\n Avg backward compute time: 5983335\nAvg backward comm. time: 4326421\n Avg backward comm/comp overlap time: 4207652\nI0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 42850427\n Avg backward compute time: 3885553\nAvg backward comm. time: 2357981\n Avg backward comm/comp overlap time: 2234674\n\n\nIn addition, TORCH_DISTRIBUTED_DEBUG=INFO enhances crash logging in torch.nn.parallel.DistributedDataParallel() due to unused parameters in the model. Currently, find_unused_parameters=True must be passed into torch.nn.parallel.DistributedDataParallel() initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as torch.nn.parallel.DistributedDataParallel() does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, torch.nn.parallel.DistributedDataParallel() will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify loss to be instead computed as loss = output[1], then TwoLinLayerNet.a does not receive a gradient in the backwards pass, and thus results in DDP failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\n\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\nlue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\nParameters which did not receive grad for rank 0: a.weight\nParameter indices which did not receive grad for rank 0: 0\n\n\nSetting TORCH_DISTRIBUTED_DEBUG=DETAIL will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP allreduce). This is done by creating a wrapper process group that wraps all process groups returned by torch.distributed.init_process_group() and torch.distributed.new_group() APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a torch.distributed.monitored_barrier(), which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into torch.distributed.all_reduce():\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=())\n\n\nWith the NCCL backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables TORCH_DISTRIBUTED_DEBUG=DETAIL and reruns the application, the following error message reveals the root cause:\n\nwork = default_pg.allreduce([tensor], opts)\nRuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n[ torch.LongTensor{1} ]\n\n\nNOTE\n\nFor fine-grained control of the debug level during runtime the functions torch.distributed.set_debug_level(), torch.distributed.set_debug_level_from_env(), and torch.distributed.get_debug_level() can also be used.\n\nIn addition, TORCH_DISTRIBUTED_DEBUG=DETAIL can be used in conjunction with TORCH_SHOW_CPP_STACKTRACES=1 to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use c10d collective calls backed by process groups created with the torch.distributed.init_process_group() and torch.distributed.new_group() APIs.\n\nLogging\n\nIn addition to explicit debugging support via torch.distributed.monitored_barrier() and TORCH_DISTRIBUTED_DEBUG, the underlying C++ library of torch.distributed also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of TORCH_CPP_LOG_LEVEL and TORCH_DISTRIBUTED_DEBUG environment variables.\n\nTORCH_CPP_LOG_LEVEL\n\n\t\n\nTORCH_DISTRIBUTED_DEBUG\n\n\t\n\nEffective Log Level\n\n\n\n\nERROR\n\n\t\n\nignored\n\n\t\n\nError\n\n\n\n\nWARNING\n\n\t\n\nignored\n\n\t\n\nWarning\n\n\n\n\nINFO\n\n\t\n\nignored\n\n\t\n\nInfo\n\n\n\n\nINFO\n\n\t\n\nINFO\n\n\t\n\nDebug\n\n\n\n\nINFO\n\n\t\n\nDETAIL\n\n\t\n\nTrace (a.k.a. All)\n\nDistributed has a custom Exception type derived from RuntimeError called torch.distributed.DistBackendError. This exception is thrown when a backend-specific error occurs. For example, if the NCCL backend is used and the user attempts to use a GPU that is not available to the NCCL library.\n\nCLASS\ntorch.distributed.DistBackendError\n\nException raised when a backend error occurs in distributed\n\nWARNING\n\nThe DistBackendError exception type is an experimental feature is subject to change.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDistributed communication package - torch.distributed\nBackends\nBasics\nInitialization\nPost-Initialization\nDistributed Key-Value Store\nGroups\nPoint-to-point communication\nSynchronous and asynchronous collective operations\nCollective functions\nProfiling Collective Communication\nMulti-GPU collective functions\nThird-party backends\nLaunch utility\nSpawn utility\nDebugging torch.distributed applications\nLogging\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Distributed Optimizers — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.optim.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Distributed Optimizers\nShortcuts\nDISTRIBUTED OPTIMIZERS\n\nWARNING\n\nDistributed optimizer is not currently supported when using CUDA tensors\n\ntorch.distributed.optim exposes DistributedOptimizer, which takes a list of remote parameters (RRef) and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer Base class to apply the gradients on each worker.\n\nCLASS\ntorch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs)\n[SOURCE]\n\nDistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.\n\nThis class uses get_gradients() in order to retrieve the gradients for specific parameters.\n\nConcurrent calls to step(), either from the same or different clients, will be serialized on each worker – as each worker’s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.\n\nDistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow the recipe in PyTorch tutorials to enable TorchScript support for your own custom optimizers.\n\nParameters\n\noptimizer_class (optim.Optimizer) – the class of optimizer to instantiate on each worker.\n\nparams_rref (list[RRef]) – list of RRefs to local or remote parameters to optimize.\n\nargs – arguments to pass to the optimizer constructor on each worker.\n\nkwargs – arguments to pass to the optimizer constructor on each worker.\n\nExample::\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n\nstep(context_id)\n[SOURCE]\n\nPerforms a single optimization step.\n\nThis will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.\n\nParameters\n\ncontext_id – the autograd context id for which we should run the optimizer step.\n\nCLASS\ntorch.distributed.optim.PostLocalSGDOptimizer(optim, averager)\n[SOURCE]\n\nWraps an arbitrary torch.optim.Optimizer and runs post-local SGD, This optimizer runs local optimizer at every step. After the warm-up stage, it averages parameters periodically afer the local optimizer is applied.\n\nParameters\n\noptim (Optimizer) – The local optimizer.\n\naverager (ModelAverager) – A model averager instance to run post-localSGD algorithm.\n\nExample:\n\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.distributed.algorithms.model_averaging.averagers as averagers\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import PostLocalSGDOptimizer\n>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (\n>>>   PostLocalSGDState,\n>>>   post_localSGD_hook,\n>>> )\n>>>\n>>> model = nn.parallel.DistributedDataParallel(\n>>>    module, device_ids=[rank], output_device=rank\n>>> )\n>>>\n>>> # Register a post-localSGD communication hook.\n>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)\n>>> model.register_comm_hook(state, post_localSGD_hook)\n>>>\n>>> # Create a post-localSGD optimizer that wraps a local optimizer.\n>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.\n>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n>>> opt = PostLocalSGDOptimizer(\n>>>     optim=local_optim,\n>>>     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)\n>>> )\n>>>\n>>> # In the first 100 steps, DDP runs global gradient averaging at every step.\n>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n>>> for step in range(0, 200):\n>>>    opt.zero_grad()\n>>>    loss = loss_fn(output, labels)\n>>>    loss.backward()\n>>>    opt.step()\n\nload_state_dict(state_dict)\n[SOURCE]\n\nThis is the same as torch.optim.Optimizer load_state_dict(), but also restores model averager’s step value to the one saved in the provided state_dict.\n\nIf there is no \"step\" entry in state_dict, it will raise a warning and initialize the model averager’s step to 0.\n\nstate_dict()\n[SOURCE]\n\nThis is the same as torch.optim.Optimizer state_dict(), but adds an extra entry to record model averager’s step to the checkpoint to ensure reload does not cause unnecessary warm up again.\n\nstep()\n[SOURCE]\n\nPerforms a single optimization step (parameter update).\n\nCLASS\ntorch.distributed.optim.ZeroRedundancyOptimizer(params, optimizer_class, process_group=None, parameters_as_bucket_view=False, overlap_with_ddp=False, **defaults)\n[SOURCE]\n\nThis class wraps an arbitrary optim.Optimizer and shards its states across ranks in the group as described by ZeRO. The local optimizer instance in each rank is only responsible for updating approximately 1 / world_size parameters and hence only needs to keep 1 / world_size optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. ZeroRedundancyOptimizer can be used in conjunction with torch.nn.parallel.DistributedDataParallel to reduce per-rank peak memory consumption.\n\nZeroRedundancyOptimizer uses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.\n\nParameters\n\nparams (Iterable) – an Iterable of torch.Tensor s or dict s giving all parameters, which will be sharded across ranks.\n\nKeyword Arguments\n\noptimizer_class (torch.nn.Optimizer) – the class of the local optimizer.\n\nprocess_group (ProcessGroup, optional) – torch.distributed ProcessGroup (default: dist.group.WORLD initialized by torch.distributed.init_process_group()).\n\nparameters_as_bucket_view (bool, optional) – if True, parameters are packed into buckets to speed up communication, and param.data fields point to bucket views at different offsets; if False, each individual parameter is communicated separately, and each params.data stays intact (default: False).\n\noverlap_with_ddp (bool, optional) – if True, step() is overlapped with DistributedDataParallel ‘s gradient synchronization; this requires (1) either a functional optimizer for the optimizer_class argument or one with a functional equivalent and (2) registering a DDP communication hook constructed from one of the functions in ddp_zero_hook.py; parameters are packed into buckets matching those in DistributedDataParallel, meaning that the parameters_as_bucket_view argument is ignored. If False, step() runs disjointly after the backward pass (per normal). (default: False)\n\n**defaults – any trailing arguments, which are forwarded to the local optimizer.\n\nExample:\n\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()\n\n\nWARNING\n\nCurrently, ZeroRedundancyOptimizer requires that all of the passed-in parameters are the same dense type.\n\nWARNING\n\nIf you pass overlap_with_ddp=True, be wary of the following: Given the way that overlapping DistributedDataParallel with ZeroRedundancyOptimizer is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if static_graph=False or static_graph=True, respectively. This is because it needs information about the gradient bucketing strategy used by DistributedDataParallel, which is not finalized until the second forward pass if static_graph=False or until the third forward pass if static_graph=True. To adjust for this, one option is to prepend dummy inputs.\n\nWARNING\n\nZeroRedundancyOptimizer is experimental and subject to change.\n\nadd_param_group(param_group)\n[SOURCE]\n\nAdd a parameter group to the Optimizer ‘s param_groups.\n\nThis can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the Optimizer as training progresses.\n\nParameters\n\nparam_group (dict) – specifies the parameters to be optimized and group-specific optimization options.\n\nWARNING\n\nThis method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.\n\nconsolidate_state_dict(to=0)\n[SOURCE]\n\nConsolidate a list of state_dict s (one per rank) on the target rank.\n\nParameters\n\nto (int) – the rank that receives the optimizer states (default: 0).\n\nRaises\n\nRuntimeError – if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.\n\nWARNING\n\nThis needs to be called on all ranks.\n\njoin_hook(**kwargs)\n[SOURCE]\n\nReturns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.\n\nGradients must be properly set before this hook is called.\n\nParameters\n\nkwargs (dict) – a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.\n\nThis hook does not support any keyword arguments; i.e. kwargs is unused.\n\nload_state_dict(state_dict)\n[SOURCE]\n\nLoad the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.\n\nParameters\n\nstate_dict (dict) – optimizer state; should be an object returned from a call to state_dict().\n\nRaises\n\nRuntimeError – if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.\n\nstate_dict()\n[SOURCE]\n\nReturns the last global optimizer state known to this rank.\n\nRaises\n\nRuntimeError – if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt; or if this method is called without a preceding call to consolidate_state_dict().\n\nReturn type\n\nDict[str, Any]\n\nstep(closure=None, **kwargs)\n[SOURCE]\n\nPerforms a single optimizer step and syncs parameters across all ranks.\n\nParameters\n\nclosure (Callable) – a closure that re-evaluates the model and returns the loss; optional for most optimizers.\n\nReturns\n\nOptional loss depending on the underlying local optimizer.\n\nReturn type\n\nOptional[float]\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDistributed Optimizers\nDistributedOptimizer\nPostLocalSGDOptimizer\nZeroRedundancyOptimizer\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "FullyShardedDataParallel — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/fsdp.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > FullyShardedDataParallel\nShortcuts\nFULLYSHARDEDDATAPARALLEL\nCLASS\ntorch.distributed.fsdp.FullyShardedDataParallel(module, process_group=None, sharding_strategy=None, cpu_offload=None, auto_wrap_policy=None, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, mixed_precision=None, ignored_modules=None, param_init_fn=None, device_id=None, sync_module_states=False, forward_prefetch=False, limit_all_gathers=True, use_orig_params=False, ignored_states=None)\n[SOURCE]\n\nA wrapper for sharding module parameters across data parallel workers. This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. FullyShardedDataParallel is commonly shortened to FSDP.\n\nExample:\n\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> torch.cuda.set_device(device_id)\n>>> sharded_module = FSDP(my_module)\n>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\n>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))\n>>> loss = x.sum()\n>>> loss.backward()\n>>> optim.step()\n\n\nWARNING\n\nThe optimizer must be initialized after the module has been wrapped with FSDP since FSDP will shard and transform the module’s parameters in a way that may not preserve the original parameter variables. Thus, the previously initialized optimizer may have stale references to the parameters.\n\nWARNING\n\nIf the destination CUDA device has ID dev_id, either (1) module should already be placed on that device, (2) the device should be set using torch.cuda.set_device(dev_id), or (3) dev_id should be passed into the device_id constructor argument. This FSDP instance’s compute device will be that destination device. For (1) and (3), the FSDP initialization always occurs on GPU. For (2), the FSDP initialization happens on module ‘s current device, which may be CPU.\n\nWARNING\n\nFSDP currently does not support gradient accumulation outside no_sync() when using CPU offloading. Trying to do so yields incorrect results since FSDP will use the newly-reduced gradient instead of accumulating with any existing gradient.\n\nWARNING\n\nChanging the original parameter variable names after construction will lead to undefined behavior.\n\nWARNING\n\nPassing in the sync_module_states=True flag requires module to be on GPU or to use the device_id argument to specify a CUDA device that FSDP will move module to in the FSDP constructor. This is because sync_module_states=True requires GPU communication.\n\nWARNING\n\nAs of PyTorch 1.12, FSDP only offers limited support for shared parameters (for example, setting one Linear layer’s weight to another’s). In particular, modules that share parameters must be wrapped as part of the same FSDP unit. If enhanced shared parameter support is needed for your use case, please ping https://github.com/pytorch/pytorch/issues/77724\n\nWARNING\n\nFSDP has some constraints on freezing parameters (i.e. setting param.requires_grad=False). For use_orig_params=False, each FSDP instance must manage parameters that are all frozen or all non-frozen. For use_orig_params=True, FSDP supports mixing frozen and non-frozen, but we recommend not doing so since then the gradient memory usage will be higher than expected (namely, equivalent to not freezing those parameters). This means that ideally, frozen parameters should be isolated into their own nn.Module s and wrapped separately with FSDP.\n\nNOTE\n\nAttempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule’s parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit.\n\nNOTE\n\nFSDP moves input tensors to the forward method to the GPU compute device, so the user does not need to manually move them from CPU.\n\nWARNING\n\nThe user should not modify the parameters between forward and backward without using the summon_full_params() context since the modifications may not persist. Moreover, for use_orig_params=False, accessing the original parameters between forward and backward may raise an illegal memory access.\n\nWARNING\n\nFor use_orig_params=True, ShardingStrategy.SHARD_GRAD_OP exposes the unsharded parameters, not the sharded parameters, after forward since it does not free the unsharded ones, unlike ShardingStrategy.FULL_SHARD. One caveat is that, since gradients are always sharded or None, ShardingStrategy.SHARD_GRAD_OP will not expose the sharded gradients with the unsharded parameters after forward. If you want to inspect the gradients, try summon_full_params() with with_grads=True.\n\nWARNING\n\nFSDP replaces managed modules’ parameters with torch.Tensor views during forward and backward computation for autograd-related reasons. If your module’s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP’s newly created views, and autograd will not work correctly.\n\nNOTE\n\nWith limit_all_gathers=True, you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.\n\nNOTE\n\nWhen using sharding_strategy=ShardingStrategy.HYBRID_SHARD with the sharding process group being intra-node and the replication process group being inter-node, setting NCCL_CROSS_NIC=1 can help improve the all-reduce times over the replication process group for some cluster setups.\n\nParameters\n\nmodule (nn.Module) – This is the module to be wrapped with FSDP.\n\nprocess_group (Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]) – This is the process group over which the model is sharded and thus the one used for FSDP’s all-gather and reduce-scatter collective communications. If None, then FSDP uses the default process group. For hybrid sharding strategies such as ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of process groups, representing the groups over which to shard and replicate, respectively. If None, then FSDP constructs process groups for the user to shard intra-node and replicate inter-node. (Default: None)\n\nsharding_strategy (Optional[ShardingStrategy]) – This configures the sharding strategy, which may trade off memory saving and communication overhead. See ShardingStrategy for details. (Default: FULL_SHARD)\n\ncpu_offload (Optional[CPUOffload]) – This configures CPU offloading. If this is set to None, then no CPU offloading happens. See CPUOffload for details. (Default: None)\n\nauto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy]]) –\n\nThis specifies a policy to apply FSDP to submodules of module, which is needed for communication and computation overlap and thus affects performance. If None, then FSDP only applies to module, and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts ModuleWrapPolicy directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments module: nn.Module, recurse: bool, and nonwrapped_numel: int and should return a bool specifying whether the passed-in module should have FSDP applied if recurse=False or if the traversal should continue into the module’s subtree if recurse=True. Users may add additional arguments to the callable. The size_based_auto_wrap_policy in torch.distributed.fsdp.wrap.py gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.\n\nExample:\n\n>>> def custom_auto_wrap_policy(\n>>>     module: nn.Module,\n>>>     recurse: bool,\n>>>     nonwrapped_numel: int,\n>>>     # Additional custom arguments\n>>>     min_num_params: int = int(1e8),\n>>> ) -> bool:\n>>>     return nonwrapped_numel >= min_num_params\n>>> # Configure a custom `min_num_params`\n>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))\n\n\nbackward_prefetch (Optional[BackwardPrefetch]) – This configures explicit backward prefetching of all-gathers. If None, then FSDP does not backward prefetch, and there is no communication and computation overlap in the backward pass. See BackwardPrefetch for details. (Default: BACKWARD_PRE)\n\nmixed_precision (Optional[MixedPrecision]) – This configures native mixed precision for FSDP. If this is set to None, then no mixed precision is used. Otherwise, parameter, buffer, and gradient reduction dtypes can be set. See MixedPrecision for details. (Default: None)\n\nignored_modules (Optional[Iterable[torch.nn.Module]]) – Modules whose own parameters and child modules’ parameters and buffers are ignored by this instance. None of the modules directly in ignored_modules should be FullyShardedDataParallel instances, and any child modules that are already-constructed FullyShardedDataParallel instances will not be ignored if they are nested under this instance. This argument may be used to avoid sharding specific parameters at module granularity when using an auto_wrap_policy or if parameters’ sharding is not managed by FSDP. (Default: None)\n\nparam_init_fn (Optional[Callable[[nn.Module], None]]) –\n\nA Callable[torch.nn.Module] -> None that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via is_meta and either applies param_init_fn if specified or calls nn.Module.reset_parameters() otherwise. For both cases, the implementation should only initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX’s (https://github.com/pytorch/torchdistX) deferred_init() API, where the deferred modules are initialized by calling param_init_fn if specified or torchdistX’s default materialize_module() otherwise. If param_init_fn is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.\n\nExample:\n\n>>> module = MyModule(device=\"meta\")\n>>> def my_init_fn(module: nn.Module):\n>>>     # E.g. initialize depending on the module type\n>>>     ...\n>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n>>> print(next(fsdp_model.parameters()).device) # current CUDA device\n>>> # With torchdistX\n>>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n>>> # Will initialize via deferred_init.materialize_module().\n>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)\n\n\ndevice_id (Optional[Union[int, torch.device]]) – An int or torch.device giving the CUDA device on which FSDP initialization takes place, including the module initialization if needed and the parameter sharding. This should be specified to improve initialization speed if module is on CPU. If the default CUDA device was set (e.g. via torch.cuda.set_device), then the user may pass torch.cuda.current_device to this. (Default: None)\n\nsync_module_states (bool) – If True, then each FSDP module will broadcast module parameters and buffers from rank 0 to ensure that they are replicated across ranks (adding communication overhead to this constructor). This can help load state_dict checkpoints via load_state_dict in a memory efficient way. See FullStateDictConfig for an example of this. (Default: False)\n\nforward_prefetch (bool) – If True, then FSDP explicitly prefetches the next forward-pass all-gather before the current forward computation. This is only useful for CPU-bound workloads, in which case issuing the next all-gather earlier may improve overlap. This should only be used for static-graph models since the prefetching follows the first iteration’s execution order. (Default: False)\n\nlimit_all_gathers (bool) – If True, then FSDP explicitly synchronizes the CPU thread to ensure GPU memory usage from only two consecutive FSDP instances (the current instance running computation and the next instance whose all-gather is prefetched). If False, then FSDP allows the CPU thread to issue all-gathers without any extra synchronization. (Default: True) We often refer to this feature as the “rate limiter”. This flag should only be set to False for specific CPU-bound workloads with low memory pressure in which case the CPU thread can aggressively issue all kernels without concern for the GPU memory usage.\n\nuse_orig_params (bool) – Setting this to True has FSDP use module ‘s original parameters. FSDP exposes those original parameters to the user via nn.Module.named_parameters() instead of FSDP’s internal FlatParameter s. This means that the optimizer step runs on the original parameters, enabling per-original-parameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded FlatParameter, respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size-0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. True is required to use torch.compile(). Setting this to False exposes FSDP’s internal FlatParameter s to the user via nn.Module.named_parameters(). (Default: False)\n\nignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]) – Ignored parameters or modules that will not be managed by this FSDP instance, meaning that the parameters are not sharded and their gradients are not reduced across ranks. This argument unifies with the existing ignored_modules argument, and we may deprecate ignored_modules soon. For backward compatibility, we keep both ignored_states and ignored_modules`, but FSDP only allows one of them to be specified as not None.\n\napply(fn)\n[SOURCE]\n\nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).\n\nCompared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.\n\nParameters\n\nfn (Module -> None) – function to be applied to each submodule\n\nReturns\n\nself\n\nReturn type\n\nModule\n\nclip_grad_norm_(max_norm, norm_type=2.0)\n[SOURCE]\n\nClips the gradient norm of all parameters. The norm is computed over all parameters’ gradients as viewed as a single vector, and the gradients are modified in-place.\n\nParameters\n\nmax_norm (float or int) – max norm of the gradients\n\nnorm_type (float or int) – type of the used p-norm. Can be 'inf' for infinity norm.\n\nReturns\n\nTotal norm of the parameters (viewed as a single vector).\n\nReturn type\n\nTensor\n\nNOTE\n\nIf every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().\n\nNOTE\n\nIf at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.\n\nNOTE\n\nThe total norm returned will have the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm’s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm’s dtype will be FP32.\n\nWARNING\n\nThis needs to be called on all ranks since it uses collective communications.\n\nSTATIC flatten_sharded_optim_state_dict(sharded_optim_state_dict, model, optim)\n[SOURCE]\n\nThe API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.\n\nParameters\n\nsharded_optim_state_dict (Dict[str, Any]) – Optimizer state dict corresponding to the unflattened parameters and holding the sharded optimizer state.\n\nmodel (torch.nn.Module) – Refer to shard_full_optim_state_dict().\n\noptim (torch.optim.Optimizer) – Optimizer for model ‘s parameters.\n\nReturns\n\nRefer to shard_full_optim_state_dict().\n\nReturn type\n\nDict[str, Any]\n\nforward(*args, **kwargs)\n[SOURCE]\n\nRuns the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.\n\nReturn type\n\nAny\n\nSTATIC fsdp_modules(module, root_only=False)\n[SOURCE]\n\nReturns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.\n\nParameters\n\nmodule (torch.nn.Module) – Root module, which may or may not be an FSDP module.\n\nroot_only (bool) – Whether to return only FSDP root modules. (Default: False)\n\nReturns\n\nFSDP modules that are nested in the input module.\n\nReturn type\n\nList[FullyShardedDataParallel]\n\nSTATIC full_optim_state_dict(model, optim, optim_input=None, rank0_only=True, group=None)\n[SOURCE]\n\nConsolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.\n\nWARNING\n\nThis needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.\n\nWARNING\n\nUnlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.\n\nNOTE\n\nLike in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().\n\nParameters\n\nmodel (torch.nn.Module) – Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim.\n\noptim (torch.optim.Optimizer) – Optimizer for model ‘s parameters.\n\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer optim representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None)\n\nrank0_only (bool) – If True, saves the populated dict only on rank 0; if False, saves it on all ranks. (Default: True)\n\ngroup (dist.ProcessGroup) – Model’s process group or None if using the default process group. (Default: None)\n\nReturns\n\nA dict containing the optimizer state for model ‘s original unflattened parameters and including keys “state” and “param_groups” following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC get_state_dict_type(module)\n[SOURCE]\n\nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.\n\nReturns\n\nA StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.\n\nRaises\n\nAssertionError` if the StateDictSettings for differen –\n\nFSDP submodules differ. –\n\nReturn type\n\nStateDictSettings\n\nPROPERTY module: MODULE\n\nReturns the wrapped module (like DistributedDataParallel).\n\nnamed_buffers(*args, **kwargs)\n[SOURCE]\n\nOverrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.\n\nReturn type\n\nIterator[Tuple[str, Tensor]]\n\nnamed_parameters(*args, **kwargs)\n[SOURCE]\n\nOverrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.\n\nReturn type\n\nIterator[Tuple[str, Parameter]]\n\nno_sync()\n[SOURCE]\n\nA context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.\n\nNOTE\n\nThis likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.\n\nNOTE\n\nWhen used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.\n\nReturn type\n\nGenerator\n\nSTATIC optim_state_dict(model, optim, optim_state_dict=None, group=None)\n[SOURCE]\n\nTransforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.\n\nFor full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM.\n\nFor sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory.\n\nFor local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).\n\nExample:\n\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\nParameters\n\nmodel (torch.nn.Module) – Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim.\n\noptim (torch.optim.Optimizer) – Optimizer for model ‘s parameters.\n\noptim_state_dict (Dict[str, Any]) – the target optimizer state_dict to transform. If the value is None, optim.state_dict() will be used. ( Default: None)\n\ngroup (dist.ProcessGroup) – Model’s process group across which parameters are sharded or None if using the default process group. ( Default: None)\n\nReturns\n\nA dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC optim_state_dict_to_load(model, optim, optim_state_dict, is_named_optimizer=False, load_directly=False, group=None)\n[SOURCE]\n\nGiven a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel.\n\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\nParameters\n\nmodel (torch.nn.Module) – Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim.\n\noptim (torch.optim.Optimizer) – Optimizer for model ‘s parameters.\n\noptim_state_dict (Dict[str, Any]) – The optimizer states to be loaded.\n\nis_named_optimizer (bool) – Is this optimizer a NamedOptimizer or KeyedOptimizer. Only set to True if optim is TorchRec’s KeyedOptimizer or torch.distributed’s NamedOptimizer.\n\nload_directly (bool) – If this is set to True, this API will also call optim.load_state_dict(result) before returning the result. Otherwise, users are responsible to call optim.load_state_dict() (Default: False)\n\ngroup (dist.ProcessGroup) – Model’s process group across which parameters are sharded or None if using the default process group. ( Default: None)\n\nReturn type\n\nDict[str, Any]\n\nregister_comm_hook(state, hook)\n[SOURCE]\n\nRegisters a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.\n\nWARNING\n\nFSDP communication hook should be registered before running an initial forward pass and only once.\n\nParameters\n\nstate (object) –\n\nPassed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.\n\nhook (Callable) – Callable, which has one of the following signatures: 1) hook: Callable[torch.Tensor] -> None: This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns None; 2) hook: Callable[torch.Tensor, torch.Tensor] -> None: This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns None. Callables with signature 1 are expected to handle gradient communication for a NO_SHARD case. Callables with signature 2 are expected to handle gradient communication for sharded cases.\n\nSTATIC rekey_optim_state_dict(optim_state_dict, optim_state_key_type, model, optim_input=None, optim=None)\n[SOURCE]\n\nRe-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.\n\nTo re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model:\n\n>>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd)\n\n\nTo re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:\n\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd)\n\nReturns\n\nThe optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC scatter_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None, group=None)\n[SOURCE]\n\nScatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict().\n\nExample:\n\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd)\n\n\nNOTE\n\nBoth shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.\n\nParameters\n\nfull_optim_state_dict (Optional[Dict[str, Any]]) – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state if on rank 0; the argument is ignored on nonzero ranks.\n\nmodel (torch.nn.Module) – Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict.\n\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None)\n\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None)\n\ngroup (dist.ProcessGroup) – Model’s process group or None if using the default process group. (Default: None)\n\nReturns\n\nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC set_state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)\n[SOURCE]\n\nSet the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.\n\nNOTE\n\nThis API should be called for only the top-level (root) module.\n\nNOTE\n\nThis API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:\n\nExample:\n\n>>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n\nParameters\n\nmodule (torch.nn.Module) – Root module.\n\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\n\nstate_dict_config (Optional[StateDictConfig]) – the configuration for the target state_dict_type.\n\nReturns\n\nA StateDictSettings that include the previous state_dict type and configuration for the module.\n\nReturn type\n\nStateDictSettings\n\nSTATIC shard_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None)\n[SOURCE]\n\nShards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank’s part of the optimizer state. The first argument should be the return value of full_optim_state_dict().\n\nExample:\n\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd)\n\n\nNOTE\n\nBoth shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.\n\nParameters\n\nfull_optim_state_dict (Dict[str, Any]) – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state.\n\nmodel (torch.nn.Module) – Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict.\n\noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) – Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None)\n\noptim (Optional[torch.optim.Optimizer]) – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None)\n\nReturns\n\nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC sharded_optim_state_dict(model, optim, group=None)\n[SOURCE]\n\nThe API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):.\n\nFor the detailed usage, refer to full_optim_state_dict().\n\nWARNING\n\nThe returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.\n\nReturn type\n\nDict[str, Any]\n\nSTATIC state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)\n[SOURCE]\n\nA context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail.\n\nExample:\n\n>>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict()\n\nParameters\n\nmodule (torch.nn.Module) – Root module.\n\nstate_dict_type (StateDictType) – the desired state_dict_type to set.\n\nstate_dict_config (Optional[StateDictConfig]) – the model state_dict configuration for the target state_dict_type.\n\noptim_state_dict_config (Optional[OptimStateDictConfig]) – the optimizer state_dict configuration for the target state_dict_type.\n\nReturn type\n\nGenerator\n\nSTATIC summon_full_params(module, recurse=True, writeback=True, rank0_only=False, offload_to_cpu=False, with_grads=False)\n[SOURCE]\n\nA context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.\n\nNOTE\n\nThis can be used on inner FSDPs.\n\nNOTE\n\nThis can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.\n\nNOTE\n\nParameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.\n\nNOTE\n\nThe full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.\n\nNOTE\n\nThis method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.\n\nWARNING\n\nNote that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\n\nWARNING\n\nNote that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.\n\nParameters\n\nrecurse (bool, Optional) – recursively summon all params for nested FSDP instances (default: True).\n\nwriteback (bool, Optional) – if False, modifications to params are discarded after the context manager exits; disabling this can be slightly more efficient (default: True)\n\nrank0_only (bool, Optional) – if True, full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting rank0_only=True with writeback=True is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\n\noffload_to_cpu (bool, Optional) – If True, full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or NO_SHARD config). It is recommended to use offload_to_cpu with rank0_only=True to avoid redundant copies of model parameters being offloaded to the same CPU memory.\n\nwith_grads (bool, Optional) – If True, gradients are also unsharded with the parameters. Currently, this is only supported when passing use_orig_params=True to the FSDP constructor and offload_to_cpu=False to this method. (Default: False)\n\nReturn type\n\nGenerator\n\nCLASS\ntorch.distributed.fsdp.BackwardPrefetch(value)\n[SOURCE]\n\nThis configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.\n\nBACKWARD_PRE: This enables the most overlap but increases memory usage the most. This prefetches the next set of parameters before the current set of parameters’ gradient computation. This overlaps the next all-gather and the current gradient computation, and at the peak, it holds the current set of parameters, next set of parameters, and current set of gradients in memory.\n\nBACKWARD_POST: This enables less overlap but requires less memory usage. This prefetches the next set of parameters after the current set of parameters’ gradient computation. This overlaps the current reduce-scatter and the next gradient computation, and it frees the current set of parameters before allocating memory for the next set of parameters, only holding the next set of parameters and current set of gradients in memory at the peak.\n\nFSDP’s backward_prefetch argument accepts None, which disables the backward prefetching altogether. This has no overlap and does not increase memory usage. In general, we do not recommend this setting since it may degrade throughput significantly.\n\nFor more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders.\n\nCLASS\ntorch.distributed.fsdp.ShardingStrategy(value)\n[SOURCE]\n\nThis specifies the sharding strategy to be used for distributed training by FullyShardedDataParallel.\n\nFULL_SHARD: Parameters, gradients, and optimizer states are sharded. For the parameters, this strategy unshards (via all-gather) before the forward, reshards after the forward, unshards before the backward computation, and reshards after the backward computation. For gradients, it synchronizes and shards them (via reduce-scatter) after the backward computation. The sharded optimizer states are updated locally per rank.\n\nSHARD_GRAD_OP: Gradients and optimizer states are sharded during computation, and additionally, parameters are sharded outside computation. For the parameters, this strategy unshards before the forward, does not reshard them after the forward, and only reshards them after the backward computation. The sharded optimizer states are updated locally per rank. Inside no_sync(), the parameters are not resharded after the backward computation.\n\nNO_SHARD: Parameters, gradients, and optimizer states are not sharded but instead replicated across ranks similar to PyTorch’s DistributedDataParallel API. For gradients, this strategy synchronizes them (via all-reduce) after the backward computation. The unsharded optimizer states are updated locally per rank.\n\nHYBRID_SHARD: Apply FULL_SHARD within a node, and replicate parameters across nodes. This results in reduced communication volume as expensive all-gathers and reduce-scatters are only done within a node, which can be more performant for medium -sized models.\n\n_HYBRID_SHARD_ZERO2: Apply SHARD_GRAD_OP within a node, and replicate parameters across nodes. This is like HYBRID_SHARD, except this may provide even higher throughput since the unsharded parameters are not freed after the forward pass, saving the all-gathers in the pre-backward.\n\nCLASS\ntorch.distributed.fsdp.MixedPrecision(param_dtype=None, reduce_dtype=None, buffer_dtype=None, keep_low_precision_grads=False, cast_forward_inputs=False, cast_root_forward_inputs=True, _module_classes_to_ignore=(<class 'torch.nn.modules.batchnorm._BatchNorm'>, ))\n[SOURCE]\n\nThis configures FSDP-native mixed precision training.\n\nVariables\n\nparam_dtype (Optional[torch.dtype]) – This specifies the dtype for model parameters during forward and backward and thus the dtype for forward and backward computation. Outside forward and backward, the sharded parameters are kept in full precision (e.g. for the optimizer step), and for model checkpointing, the parameters are always saved in full precision. (Default: None)\n\nreduce_dtype (Optional[torch.dtype]) – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is None but param_dtype is not None, then this takes on the param_dtype value, still running gradient reduction in low precision. This is permitted to differ from param_dtype, e.g. to force gradient reduction to run in full precision. (Default: None)\n\nbuffer_dtype (Optional[torch.dtype]) – This specifies the dtype for buffers. FSDP does not shard buffers. Rather, FSDP casts them to buffer_dtype in the first forward pass and keeps them in that dtype thereafter. For model checkpointing, the buffers are saved in full precision except for LOCAL_STATE_DICT. (Default: None)\n\nkeep_low_precision_grads (bool) – If False, then FSDP upcasts gradients to full precision after the backward pass in preparation for the optimizer step. If True, then FSDP keeps the gradients in the dtype used for gradient reduction, which can save memory if using a custom optimizer that supports running in low precision. (Default: False)\n\ncast_forward_inputs (bool) – If True, then this FSDP module casts its forward args and kwargs to param_dtype. This is to ensure that parameter and input dtypes match for forward computation, as required by many ops. This may need to be set to True when only applying mixed precision to some but not all FSDP modules, in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default: False)\n\ncast_root_forward_inputs (bool) – If True, then the root FSDP module casts its forward args and kwargs to param_dtype, overriding the value of cast_forward_inputs. For non-root FSDP modules, this does not do anything. (Default: True)\n\n_module_classes_to_ignore (Sequence[Type[torch.nn.modules.module.Module]]) – (Sequence[Type[nn.Module]]): This specifies module classes to ignore for mixed precision when using an auto_wrap_policy: Modules of these classes will have FSDP applied to them separately with mixed precision disabled (meaning that the final FSDP construction would deviate from the specified policy). If auto_wrap_policy is not specified, then this does not do anything. This API is experimental and subject to change. (Default: (_BatchNorm,))\n\nNOTE\n\nThis API is experimental and subject to change.\n\nNOTE\n\nOnly floating point tensors are cast to their specified dtypes.\n\nNOTE\n\nIn summon_full_params, parameters are forced to full precision, but buffers are not.\n\nNOTE\n\nLayer norm and batch norm accumulate in float32 even when their inputs are in a low precision like float16 or bfloat16. Disabling FSDP’s mixed precision for those norm modules only means that the affine parameters are kept in float32. However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.\n\nNOTE\n\nBy default, if the user passes a model with any _BatchNorm modules and specifies an auto_wrap_policy, then the batch norm modules will have FSDP applied to them separately with mixed precision disabled. See the _module_classes_to_ignore argument.\n\nNOTE\n\nMixedPrecision has cast_root_forward_inputs=True and cast_forward_inputs=False by default. For the root FSDP instance, its cast_root_forward_inputs takes precedence over its cast_forward_inputs. For non-root FSDP instances, their cast_root_forward_inputs values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same MixedPrecision configuration and only needs to cast inputs to the param_dtype at the beginning of the model’s forward pass.\n\nNOTE\n\nFor nested FSDP instances with different MixedPrecision configurations, we recommend setting individual cast_forward_inputs values to configure casting inputs or not before each instance’s forward. In such a case, since the casts happen before each FSDP instance’s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different MixedPrecision configuration.\n\nExample:\n\n>>> model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))\n>>> model[1] = FSDP(\n>>>     model[1],\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.float16, cast_forward_inputs=True),\n>>> )\n>>> model = FSDP(\n>>>     model,\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, cast_forward_inputs=True),\n>>> )\n\n\nThe above shows a working example. On the other hand, if model[1] were replaced with model[0], meaning that the submodule using different MixedPrecision ran its forward first, then model[1] would incorrectly see float16 activations instead of bfloat16 ones.\n\nCLASS\ntorch.distributed.fsdp.CPUOffload(offload_params=False)\n[SOURCE]\n\nThis configures CPU offloading.\n\nVariables\n\noffload_params (bool) – This specifies whether to offload parameters to CPU when not involved in computation. If True, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU.\n\nCLASS\ntorch.distributed.fsdp.StateDictConfig(offload_to_cpu=False, use_dtensor=False)\n[SOURCE]\n\nStateDictConfig is the base class for all state_dict configuration classes. Users should instantiate a child class (e.g. FullStateDictConfig) in order to configure settings for the corresponding state_dict type supported by FSDP.\n\nVariables\n\noffload_to_cpu (bool) – If True, then FSDP offloads the state dict values to CPU, and if False, then FSDP keeps them on GPU. (Default: False)\n\nuse_dtensor (bool) – If True, then FSDP saves the state dict values as DTensor if the value is sharded, and if False, then FSDP saves them as ShardedTensor. (Default: False)\n\nCLASS\ntorch.distributed.fsdp.FullStateDictConfig(offload_to_cpu=False, use_dtensor=False, rank0_only=False)\n[SOURCE]\n\nFullStateDictConfig is a config class meant to be used with StateDictType.FULL_STATE_DICT. We recommend enabling both offload_to_cpu=True and rank0_only=True when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the state_dict_type() context manager as follows:\n\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> fsdp = FSDP(model, auto_wrap_policy=...)\n>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):\n>>>     state = fsdp.state_dict()\n>>>     # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:\n>>> model = model_fn() # Initialize model on CPU in preparation for wrapping with FSDP\n>>> if dist.get_rank() == 0:\n>>>     # Load checkpoint only on rank 0 to avoid memory redundancy\n>>>     state_dict = torch.load(\"my_checkpoint.pt\")\n>>>     model.load_state_dict(state_dict)\n>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument\n>>> # communicates loaded checkpoint states from rank 0 to rest of the world.\n>>> fsdp = FSDP(model, device_id=torch.cuda.current_device(), auto_wrap_policy=..., sync_module_states=True)\n>>> # After this point, all ranks have FSDP model with loaded checkpoint.\n\nVariables\n\nrank0_only (bool) – If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)\n\nCLASS\ntorch.distributed.fsdp.ShardedStateDictConfig(offload_to_cpu: bool = False, use_dtensor: bool = False)\n[SOURCE]\nCLASS\ntorch.distributed.fsdp.LocalStateDictConfig(offload_to_cpu: bool = False, use_dtensor: bool = False)\n[SOURCE]\nCLASS\ntorch.distributed.fsdp.OptimStateDictConfig(offload_to_cpu=True, use_dtensor=False)\n[SOURCE]\n\nOptimStateDictConfig is the base class for all optim_state_dict configuration classes. Users should instantiate a child class (e.g. FullOptimStateDictConfig) in order to configure settings for the corresponding optim_state_dict type supported by FSDP.\n\nVariables\n\noffload_to_cpu (bool) – If True, then FSDP offloads the state dict’s tensor values to CPU, and if False, then FSDP keeps them on the original device (which is GPU unless parameter CPU offloading is enabled). (Default: True)\n\nuse_dtensor (bool) – If True, then FSDP saves the state dict values as DTensor if the value is sharded, and if False, then FSDP saves them as ShardedTensor. (Default: False)\n\nCLASS\ntorch.distributed.fsdp.FullOptimStateDictConfig(offload_to_cpu=True, use_dtensor=False, rank0_only=False)\n[SOURCE]\nVariables\n\nrank0_only (bool) – If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)\n\nCLASS\ntorch.distributed.fsdp.ShardedOptimStateDictConfig(offload_to_cpu: bool = True, use_dtensor: bool = False)\n[SOURCE]\nCLASS\ntorch.distributed.fsdp.LocalOptimStateDictConfig(offload_to_cpu: bool = False, use_dtensor: bool = False)\n[SOURCE]\nCLASS\ntorch.distributed.fsdp.StateDictSettings(state_dict_type: torch.distributed.fsdp.api.StateDictType, state_dict_config: torch.distributed.fsdp.api.StateDictConfig, optim_state_dict_config: torch.distributed.fsdp.api.OptimStateDictConfig)\n[SOURCE]\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nFullyShardedDataParallel\nFullyShardedDataParallel\nBackwardPrefetch\nShardingStrategy\nMixedPrecision\nCPUOffload\nStateDictConfig\nFullStateDictConfig\nShardedStateDictConfig\nLocalStateDictConfig\nOptimStateDictConfig\nFullOptimStateDictConfig\nShardedOptimStateDictConfig\nLocalOptimStateDictConfig\nStateDictSettings\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Generic Join Context Manager — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Generic Join Context Manager\nShortcuts\nGENERIC JOIN CONTEXT MANAGER\n\nThe generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: Join, Joinable, and JoinHook. For a tutorial, see Distributed Training with Uneven Inputs Using the Join Context Manager.\n\nCLASS\ntorch.distributed.algorithms.Join(joinables, enable=True, throw_on_early_termination=False, **kwargs)\n[SOURCE]\n\nThis class defines the generic join context manager, which allows custom hooks to be called after a process joins. These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to JoinHook for details about the hook definition.\n\nWARNING\n\nThe context manager requires each participating Joinable to call the method notify_join_context() before its own per- iteration collective communications to ensure correctness.\n\nWARNING\n\nThe context manager requires that all process_group attributes in the JoinHook objects are the same. If there are multiple JoinHook objects, then the device of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if throw_on_early_termination is enabled, both of which using an all- reduce.\n\nParameters\n\njoinables (List[Joinable]) – a list of the participating Joinable s; their hooks are iterated over in the given order.\n\nenable (bool) – a flag enabling uneven input detection; setting to False disables the context manager’s functionality and should only be set when the user knows the inputs will not be uneven (default: True).\n\nthrow_on_early_termination (bool) – a flag controlling whether to throw an exception upon detecting uneven inputs (default: False).\n\nExample:\n\n>>> import os\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.multiprocessing as mp\n>>> import torch.nn.parallel.DistributedDataParallel as DDP\n>>> import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO\n>>> from torch.distributed.algorithms.join import Join\n>>>\n>>> # On each spawned worker\n>>> def worker(rank):\n>>>     dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>     model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n>>>     optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)\n>>>     # Rank 1 gets one more input than rank 0\n>>>     inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]\n>>>     with Join([model, optim]):\n>>>         for input in inputs:\n>>>             loss = model(input).sum()\n>>>             loss.backward()\n>>>             optim.step()\n>>>     # All ranks reach here without hanging/erroring\n\nSTATIC notify_join_context(joinable)\n[SOURCE]\n\nNotifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.\n\nThis method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel.\n\nOnly the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.\n\nParameters\n\njoinable (Joinable) – the Joinable object calling this method.\n\nReturns\n\nAn async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise.\n\nCLASS\ntorch.distributed.algorithms.Joinable\n[SOURCE]\n\nThis defines an abstract base class for joinable classes. A joinable class (inheriting from Joinable) should implement join_hook(), which returns a JoinHook instance, in addition to join_device() and join_process_group() that return device and process group information, respectively.\n\nABSTRACT PROPERTY join_device: DEVICE\n\nReturns the device from which to perform collective communications needed by the join context manager implementation itself.\n\nABSTRACT join_hook(**kwargs)\n[SOURCE]\n\nReturns a JoinHook instance for the given Joinable.\n\nParameters\n\nkwargs (dict) – a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.\n\nReturn type\n\nJoinHook\n\nABSTRACT PROPERTY join_process_group: ANY\n\nReturns the process group for the collective communications needed by the join context manager itself.\n\nCLASS\ntorch.distributed.algorithms.JoinHook\n[SOURCE]\n\nThis defines a join hook, which provides two entry points in the join context manager: a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.\n\nTo implement a join hook for the generic join context manager, define a class that inherits from JoinHook and override main_hook() and post_hook() as appropriate.\n\nmain_hook()\n[SOURCE]\n\nThis hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step).\n\npost_hook(is_last_joiner)\n[SOURCE]\n\nThis hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.\n\nParameters\n\nis_last_joiner (bool) – True if the rank is one of the last to join; False otherwise.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nGeneric Join Context Manager\nJoin\nJoinable\nJoinHook\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Torch Distributed Elastic — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/distributed.elastic.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Torch Distributed Elastic\nShortcuts\nTORCH DISTRIBUTED ELASTIC\n\nMakes distributed PyTorch fault-tolerant and elastic.\n\nGet Started\n\nUsage\n\nQuickstart\nTrain script\nExamples\nDocumentation\n\nAPI\n\ntorchrun (Elastic Launch)\nElastic Agent\nMultiprocessing\nError Propagation\nRendezvous\nExpiration Timers\nMetrics\nEvents\n\nAdvanced\n\nCustomization\n\nPlugins\n\nTorchElastic Kubernetes\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nTorch Distributed Elastic\nGet Started\nDocumentation\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.export — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/export.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.export\nShortcuts\nTORCH.EXPORT\n\nWARNING\n\nThis feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.\n\nOverview\n\ntorch.export.export() takes an arbitrary Python callable (a torch.nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.\n\nimport torch\nfrom torch.export import export\n\ndef f(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: torch.export.ExportedProgram = export(\n    f, args=example_args\n)\nprint(exported_program)\n\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 10], arg1_1: f32[10, 10]):\n            # code: a = torch.sin(x)\n            sin: f32[10, 10] = torch.ops.aten.sin.default(arg0_1);\n\n            # code: b = torch.cos(y)\n            cos: f32[10, 10] = torch.ops.aten.cos.default(arg1_1);\n\n            # code: return a + b\n            add: f32[10, 10] = torch.ops.aten.add.Tensor(sin, cos);\n            return (add,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[],\n        buffers=[],\n        user_inputs=['arg0_1', 'arg1_1'],\n        user_outputs=['add'],\n        inputs_to_parameters={},\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: []\n\n\ntorch.export produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here (coming soon!).\n\nSoundness: It is guaranteed to be a sound representation of the original program, and maintains the same calling conventions of the original program.\n\nNormalized: There are no Python semantics within the graph. Submodules from the original programs are inlined to form one fully flattened computational graph.\n\nDefined Operator Set: The graph produced contains only a small defined Core ATen IR opset and registered custom operators.\n\nGraph properties: The graph is purely functional, meaning it does not contain operations with side effects such as mutations or aliasing. It does not mutate any intermediate values, parameters, or buffers.\n\nMetadata: The graph contains metadata captured during tracing, such as a stacktrace from user’s code.\n\nUnder the hood, torch.export leverages the following latest technologies:\n\nTorchDynamo (torch._dynamo) is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code.\n\nAOT Autograd provides a functionalized PyTorch graph and ensures the graph is decomposed/lowered to the small defined Core ATen operator set.\n\nTorch FX (torch.fx) is the underlying representation of the graph, allowing flexible Python-based transformations.\n\nExisting frameworks\n\ntorch.compile() also utilizes the same PT2 stack as torch.export, but is slightly different:\n\nJIT vs. AOT: torch.compile() is a JIT compiler whereas which is not intended to be used to produce compiled artifacts outside of deployment.\n\nPartial vs. Full Graph Capture: When torch.compile() runs into an untraceable part of a model, it will “graph break” and fall back to running the program in the eager Python runtime. In comparison, torch.export aims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Since torch.export produces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages.\n\nUsability tradeoff: Since torch.compile() is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible. torch.export will instead require users to provide more information or rewrite their code to make it traceable.\n\nCompared to torch.fx.symbolic_trace(), torch.export traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, torch.export keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, torch.export is expected to work on more user programs, and produce lower-level graphs (at the torch.ops.aten operator level). Note that users can still use torch.fx.symbolic_trace() as a preprocessing step before torch.export.\n\nCompared to torch.jit.script(), torch.export does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).\n\nCompared to torch.jit.trace(), torch.export is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.\n\nExporting a PyTorch Model\nAn Example\n\nThe main entrypoint is through torch.export.export(), which takes a callable (torch.nn.Module, function, or method) and sample inputs, and captures the computation graph into an torch.export.ExportedProgram. An example:\n\nimport torch\nfrom torch.export import export\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[16, 3, 3, 3], arg1_1: f32[16], arg2_1: f32[1, 3, 256, 256], arg3_1: f32[1, 16, 256, 256]):\n\n            # code: a = self.conv(x)\n            convolution: f32[1, 16, 256, 256] = torch.ops.aten.convolution.default(\n                arg2_1, arg0_1, arg1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1\n            );\n\n            # code: a.add_(constant)\n            add: f32[1, 16, 256, 256] = torch.ops.aten.add.Tensor(convolution, arg3_1);\n\n            # code: return self.maxpool(self.relu(a))\n            relu: f32[1, 16, 256, 256] = torch.ops.aten.relu.default(add);\n            max_pool2d_with_indices = torch.ops.aten.max_pool2d_with_indices.default(\n                relu, [3, 3], [3, 3]\n            );\n            getitem: f32[1, 16, 85, 85] = max_pool2d_with_indices[0];\n            return (getitem,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=['L__self___conv.weight', 'L__self___conv.bias'],\n        buffers=[],\n        user_inputs=['arg2_1', 'arg3_1'],\n        user_outputs=['getitem'],\n        inputs_to_parameters={\n            'arg0_1': 'L__self___conv.weight',\n            'arg1_1': 'L__self___conv.bias',\n        },\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: []\n\n\nInspecting the ExportedProgram, we can note the following:\n\nThe torch.fx.Graph contains the computation graph of the original program, along with records of the original code for easy debugging.\n\nThe graph contains only torch.ops.aten operators found in the Core ATen IR opset and custom operators, and is fully functional, without any inplace operators such as torch.add_.\n\nThe parameters (weight and bias to conv) are lifted as inputs to the graph, resulting in no get_attr nodes in the graph, which previously existed in the result of torch.fx.symbolic_trace().\n\nThe torch.export.ExportGraphSignature models the input and output signature, along with specifying which inputs are parameters.\n\nThe resulting shape and dtype of tensors produced by each node in the graph is noted. For example, the convolution node will result in a tensor of dtype torch.float32 and shape (1, 16, 256, 256).\n\nExpressing Dynamism\n\nBy default torch.export will trace the program assuming all input shapes are static, and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be marked dynamic using the torch.export.dynamic_dim() API, and passed into torch.export.export() through the constraints argument. An example:\n\nimport torch\nfrom torch.export import export, dynamic_dim\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\nconstraints = [\n    # First dimension of each input is a dynamic batch size\n    dynamic_dim(example_args[0], 0),\n    dynamic_dim(example_args[1], 0),\n    # The dynamic batch size between the inputs are equal\n    dynamic_dim(example_args[0], 0) == dynamic_dim(example_args[1], 0),\n]\n\nexported_program: torch.export.ExportedProgram = export(\n  M(), args=example_args, constraints=constraints\n)\nprint(exported_program)\n\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[32, 64], arg1_1: f32[32], arg2_1: f32[64, 128], arg3_1: f32[64], arg4_1: f32[32], arg5_1: f32[s0, 64], arg6_1: f32[s0, 128]):\n\n            # code: out1 = self.branch1(x1)\n            permute: f32[64, 32] = torch.ops.aten.permute.default(arg0_1, [1, 0]);\n            addmm: f32[s0, 32] = torch.ops.aten.addmm.default(arg1_1, arg5_1, permute);\n            relu: f32[s0, 32] = torch.ops.aten.relu.default(addmm);\n\n            # code: out2 = self.branch2(x2)\n            permute_1: f32[128, 64] = torch.ops.aten.permute.default(arg2_1, [1, 0]);\n            addmm_1: f32[s0, 64] = torch.ops.aten.addmm.default(arg3_1, arg6_1, permute_1);\n            relu_1: f32[s0, 64] = torch.ops.aten.relu.default(addmm_1);  addmm_1 = None\n\n            # code: return (out1 + self.buffer, out2)\n            add: f32[s0, 32] = torch.ops.aten.add.Tensor(relu, arg4_1);\n            return (add, relu_1)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[\n            'branch1.0.weight',\n            'branch1.0.bias',\n            'branch2.0.weight',\n            'branch2.0.bias',\n        ],\n        buffers=['L__self___buffer'],\n        user_inputs=['arg5_1', 'arg6_1'],\n        user_outputs=['add', 'relu_1'],\n        inputs_to_parameters={\n            'arg0_1': 'branch1.0.weight',\n            'arg1_1': 'branch1.0.bias',\n            'arg2_1': 'branch2.0.weight',\n            'arg3_1': 'branch2.0.bias',\n        },\n        inputs_to_buffers={'arg4_1': 'L__self___buffer'},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {s0: RangeConstraint(min_val=2, max_val=9223372036854775806)}\n    Equality constraints: [(InputDim(input_name='arg5_1', dim=0), InputDim(input_name='arg6_1', dim=0))]\n\n\nSome additional things to note:\n\nThrough the torch.export.dynamic_dim() API, we specified the first dimension of each input to be dynamic. Looking at the inputs arg5_1 and arg6_1, they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs. s0 is a symbol representing that this dimension can be a range of values.\n\nexported_program.range_constraints describes the ranges of each symbol appearing in the graph. In this case, we see that s0 has the range [2, inf]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. See The 0/1 Specialization Problem for an in-depth discussion of this topic.\n\nexported_program.equality_constraints describes which dimensions are required to be equal. Since we specified in the constraints that the first dimension of each argument is equivalent, (dynamic_dim(example_args[0], 0) == dynamic_dim(example_args[1], 0)), we see in the equality constraints the tuple specifying that arg5_1 dimension 0 and arg6_1 dimension 0 are equal.\n\nSerialization\n\nTo save the ExportedProgram, users can use the torch.export.save() and torch.export.load() APIs. A convention is to save the ExportedProgram using a .pt2 file extension.\n\nAn example:\n\nimport torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), torch.randn(5))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n\nSpecialization\nInput shapes\n\nAs mentioned before, by default, torch.export will trace the program specializing on the input tensors’ shapes, unless a dimension is specified as dynamic via the torch.export.dynamic_dim() API. This means that if there exists shape-dependent control flow, torch.export will specialize on the branch that is being taken with the given sample inputs. For example:\n\nimport torch\nfrom torch.export import export\n\ndef fn(x):\n    if x.shape[0] > 5:\n        return x + 1\n    else:\n        return x - 1\n\nexample_inputs = (torch.rand(10, 2),)\nexported_program = export(fn, example_inputs)\nprint(exported_program)\n\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 2]):\n            add: f32[10, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            return (add,)\n\n\nThe conditional of (x.shape[0] > 5) does not appear in the ExportedProgram because the example inputs have the static shape of (10, 2). Since torch.export specializes on the inputs’ static shapes, the else branch (x - 1) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, torch.export.dynamic_dim() will need to be used to specify the dimension of the input tensor (x.shape[0]) to be dynamic, and the source code will need to be rewritten.\n\nNon-tensor inputs\n\ntorch.export also specializes the traced graph based on the values of inputs that are not torch.Tensor, such as int, float, bool, and str. However, we will likely change this in the near future to not specialize on inputs of primitive types.\n\nFor example:\n\nimport torch\nfrom torch.export import export\n\ndef fn(x: torch.Tensor, const: int, times: int):\n    for i in range(times):\n        x = x + const\n    return x\n\nexample_inputs = (torch.rand(2, 2), 1, 3)\nexported_program = export(fn, example_inputs)\nprint(exported_program)\n\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[2, 2], arg1_1, arg2_1):\n            add: f32[2, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            add_1: f32[2, 2] = torch.ops.aten.add.Tensor(add, 1);\n            add_2: f32[2, 2] = torch.ops.aten.add.Tensor(add_1, 1);\n            return (add_2,)\n\n\nBecause integers are specialized, the torch.ops.aten.add.Tensor operations are all computed with the inlined constant 1, rather than arg1_1. Additionally, the times iterator used in the for loop is also “inlined” in the graph through the 3 repeated torch.ops.aten.add.Tensor calls, and the input arg2_1 is never used.\n\nLimitations of torch.export\nGraph Breaks\n\nAs torch.export is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of torch.compile, an unsupported operation will cause a “graph break” and the unsupported operation will be run with default Python evaluation. In contrast, torch.export will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.\n\nWhen a graph break is encountered, ExportDB is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.\n\nData/Shape-Dependent Control Flow\n\nGraph breaks can also be encountered on data-dependent control flow (if x.shape[0] > 2) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators (coming soon!).\n\nData-Dependent Accesses\n\nData dependent behavior such as using the value inside of a tensor to construct another tensor, or using the value of a tensor to slice into another tensor, is also something the tracer cannot fully determine. Users will need to rewrite their code using the inline constraint APIs torch.export.constrain_as_size() and torch.export.constrain_as_value().\n\nMissing Meta Kernels for Operators\n\nWhen tracing, a META implementation (or “meta kernel”) is required for all operators. This is used to reason about the input/output shapes for this operator.\n\nNote that the official API for registering custom meta kernels for custom ops is currently undergoing development. While the final API is being refined, you can refer to the documentation here.\n\nIn the unfortunate case where your model uses an ATen operator that is does not have a meta kernel implementation yet, please file an issue.\n\nRead More\n\nAdditional Links for Export Users\n\nWriting Graph Transformations on ATen IR\nIRs\nExportDB\n\nDeep Dive for PyTorch Developers\n\nTorchDynamo Deep Dive\nDynamic shapes\nFake tensor\nAPI Reference\ntorch.export.export(f, args, kwargs=None, *, constraints=None)\n[SOURCE]\n\nexport() takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional Core ATen Operator Set and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.\n\nSoundness Guarantee\n\nWhile tracing, export() takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output ExportedProgram is considered valid only when these assumptions hold true.\n\nThere are 2 types of assumptions made during tracing\n\nShapes (not values) of input tensors.\n\nRanges (lower and upper bound) of values extracted from intermediate tensors via .item() or direct indexing.\n\nAll assumptions must be validated at graph capture time for export() to succeed. Specifically:\n\nAssumptions on static shapes of input tensors are automatically validated without additional effort.\n\nAssumptions on dynamic shape of input tensors require explicit Input Constraint constructed with dynamic_dim() APIs\n\nAssumptions on range of intermediate values require explicit Inline Constraint, constructed use constrain_as_size() and constraint_as_value() APIs.\n\nIf any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example export() would suggest following code for input constraints:\n\ndef specify_constraints(x):\n    return [\n        # x:\n        dynamic_dim(x, 0) <= 5,\n    ]\n\n\nThis example means the program requires the dim 0 of input x to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into constraints argument.\n\nParameters\n\nf (Callable) – The callable to trace.\n\nargs (Tuple[Any, ...]) – Example positional inputs.\n\nkwargs (Optional[Dict[str, Any]]) – Optional example keyword inputs.\n\nconstraints (Optional[List[Constraint]]) – An optional list of constraints on the dynamic arguments that specify their possible range of shapes. By default, shapes of input torch.Tensors are assumed to be static. If an input torch.Tensor is expected to have dynamic shapes, please use dynamic_dim() to define Constraint objects that specify the dynamics and the possible range of shapes. See dynamic_dim() docstring for examples on how to use it.\n\nReturns\n\nAn ExportedProgram containing the traced callable.\n\nReturn type\n\nExportedProgram\n\nAcceptable input/output types\n\nAcceptable types of inputs (for args and kwargs) and outputs include:\n\nPrimitive types, i.e. torch.Tensor, int, float, bool and str.\n\n(Nested) Data structures comprising of dict, list, tuple, namedtuple and OrderedDict containing all above types.\n\ntorch.export.dynamic_dim(t, index)\n[SOURCE]\n\ndynamic_dim() constructs a Constraint object that describes the dynamism of a dimension index of tensor t. Constraint objects should be passed to constraints argument of export().\n\nParameters\n\nt (torch.Tensor) – Example input tensor that have dynamic dimension size(s)\n\nindex (int) – Index of dynamic dimension\n\nReturns\n\nA Constraint object that describes shape dynamism. It can be passed to export() so that export() does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.\n\nSpecifically dynamic_dim() can be used to express following types of dynamism.\n\nSize of a dimension is dynamic and unbounded:\n\nt0 = torch.rand(2, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size rather than always being static size 2\nconstraints = [dynamic_dim(t0, 0)]\nep = export(fn, (t0, t1), constraints=constraints)\n\n\nSize of a dimension is dynamic with a lower bound:\n\nt0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size with a lower bound of 5 (inclusive)\n# Second dimension of t1 can be dynamic size with a lower bound of 2 (exclusive)\nconstraints = [\n    dynamic_dim(t0, 0) >= 5,\n    dynamic_dim(t1, 1) > 2,\n]\nep = export(fn, (t0, t1), constraints=constraints)\n\n\nSize of a dimension is dynamic with an upper bound:\n\nt0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size with a upper bound of 16 (inclusive)\n# Second dimension of t1 can be dynamic size with a upper bound of 8 (exclusive)\nconstraints = [\n    dynamic_dim(t0, 0) <= 16,\n    dynamic_dim(t1, 1) < 8,\n]\nep = export(fn, (t0, t1), constraints=constraints)\n\n\nSize of a dimension is dynamic and it is always equal to size of another dynamic dimension:\n\nt0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# Sizes of second dimension of t0 and first dimension are always equal\nconstraints = [\n    dynamic_dim(t0, 1) == dynamic_dim(t1, 0),\n]\nep = export(fn, (t0, t1), constraints=constraints)\n\n\nMix and match all types above as long as they do not express conflicting requirements\n\ntorch.export.constrain_as_size(symbol, min=None, max=None)\n[SOURCE]\n\nHint export() about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.\n\nParameters\n\nsymbol – Intermediate scalar value (int-only now) to apply range constraint on.\n\nmin (Optional[int]) – Minimum possible value of given symbol (inclusive)\n\nmax (Optional[int]) – Maximum possible value of given symbol (inclusive)\n\nReturns\n\nNone\n\nFor example, following program can not be traced soundly wihout using constrain_as_size() to give export() a hint about shape ranges:\n\ndef fn(x):\n    d = x.max().item()\n    return torch.ones(v)\n\n\nexport() would give following error:\n\ntorch._dynamo.exc.Unsupported: guard on data-dependent symbolic int/float\n\n\nAssuming the actual range of d can be between [3, 10], you can add a call to constrain_as_size() in the source code like this:\n\ndef fn(x):\n    d = x.max().item()\n    torch.export.constrain_as_size(d, min=3, max=10)\n    return torch.ones(d)\n\n\nWith the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:\n\ngraph():\n    %arg0_1 := placeholder[target=arg0_1]\n\n    # d = x.max().item()\n    %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))\n    %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))\n\n    # Asserting 3 <= d <= 10\n    %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 3))\n    %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))\n    %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [3, 10].))\n    %le := call_function[target=operator.le](args = (%_local_scalar_dense, 10))\n    %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))\n    %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [3, 10].))\n    %sym_constrain_range_for_size := call_function[target=torch.ops.aten.sym_constrain_range_for_size.default](\n        args = (%_local_scalar_dense,), kwargs = {min: 3, max: 10})\n\n    # Constructing new tensor with d\n    %full := call_function[target=torch.ops.aten.full.default](\n        args = ([%_local_scalar_dense], 1),\n        kwargs = {dtype: torch.float32, layout: torch.strided, device: cpu, pin_memory: False})\n\n    ......\n\n\nWARNING\n\nif your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed\n\ntorch.export.constrain_as_value(symbol, min=None, max=None)\n[SOURCE]\n\nHint export() about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.\n\nWARNING\n\n(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call constrain_as_size() instead.)\n\nParameters\n\nsymbol – Intermediate scalar value (int-only now) to apply range constraint on.\n\nmin (Optional[int]) – Minimum possible value of given symbol (inclusive)\n\nmax (Optional[int]) – Maximum possible value of given symbol (inclusive)\n\nReturns\n\nNone\n\nFor example, following program can not be traced soundly:\n\ndef fn(x):\n    v = x.max().item()\n    if v > 1024:\n        return x\n    else:\n        return x * 2\n\n\nv is a data-dependent value, which is assumed to have a range of (-inf, inf). export() a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus export() would give following error:\n\ntorch._dynamo.exc.UserError: Consider annotating your code using\ntorch.export.constrain_as_size() or torch.export().constrain_as_value() APIs.\nIt appears that you're trying to get a value out of symbolic int/float whose value\nis data-dependent (and thus we do not know the true value.)  The expression we were\ntrying to evaluate is f0 > 1024 (unhinted: f0 > 1024).\n\n\nAssuming the actual range of v can be between [10, 200], you can add a call to constrain_as_value() in the source code like this:\n\ndef fn(x):\n    v = x.max().item()\n\n    # Give export() a hint\n    torch.export.constrain_as_value(v, min=10, max=200)\n\n    if v > 1024:\n        return x\n    else:\n        return x * 2\n\n\nWith the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:\n\ngraph():\n    %arg0_1 := placeholder[target=arg0_1]\n\n    # v = x.max().item()\n    %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))\n    %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))\n\n    # Asserting 10 <= v <= 200\n    %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 10))\n    %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))\n    %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [10, 200].))\n    %le := call_function[target=operator.le](args = (%_local_scalar_dense, 200))\n    %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))\n    %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [10, 200].))\n    %sym_constrain_range := call_function[target=torch.ops.aten.sym_constrain_range.default](\n        args = (%_local_scalar_dense,), kwargs = {min: 10, max: 200})\n\n    # Always taking `else` branch to multiply elements `x` by 2 due to hints above\n    %mul := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 2), kwargs = {})\n    return (mul,)\n\ntorch.export.save(ep, f, *, extra_files=None, opset_version=None)\n[SOURCE]\n\nWARNING\n\nUnder active development, saved files may not be usable in newer versions of PyTorch.\n\nSaves an ExportedProgram to a file-like object. It can then be loaded using the Python API torch.export.load.\n\nParameters\n\nep (ExportedProgram) – The exported program to save.\n\nf (Union[str, pathlib.Path, io.BytesIO) – A file-like object (has to implement write and flush) or a string containing a file name.\n\nextra_files (Optional[Dict[str, Any]]) – Map from filename to contents which will be stored as part of f.\n\nopset_version (Optional[Dict[str, int]]) – A map of opset names to the version of this opset\n\nExample:\n\nimport torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nep = torch.export.export(MyModule(), torch.randn(5))\n\n# Save to file\ntorch.export.save(ep, 'exported_program.pt2')\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.export.save(ep, buffer)\n\n# Save with extra files\nextra_files = {'foo.txt': b'bar'}\ntorch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n\ntorch.export.load(f, *, extra_files=None, expected_opset_version=None)\n[SOURCE]\n\nWARNING\n\nUnder active development, saved files may not be usable in newer versions of PyTorch.\n\nLoads an ExportedProgram previously saved with torch.export.save.\n\nParameters\n\nep (ExportedProgram) – The exported program to save.\n\nf (Union[str, pathlib.Path, io.BytesIO) – A file-like object (has to implement write and flush) or a string containing a file name.\n\nextra_files (Optional[Dict[str, Any]]) – The extra filenames given in this map would be loaded and their content would be stored in the provided map.\n\nexpected_opset_version (Optional[Dict[str, int]]) – A map of opset names to expected opset versions\n\nReturns\n\nAn ExportedProgram object\n\nReturn type\n\nExportedProgram\n\nExample:\n\nimport torch\nimport io\n\n# Load ExportedProgram from file\nep = torch.export.load('exported_program.pt2')\n\n# Load ExportedProgram from io.BytesIO object\nwith open('exported_program.pt2', 'rb') as f:\n    buffer = io.BytesIO(f.read())\nbuffer.seek(0)\nep = torch.export.load(buffer)\n\n# Load with extra files.\nextra_files = {'foo.txt': ''}  # values will be replaced with data\nep = torch.export.load('exported_program.pt2', extra_files=extra_files)\nprint(extra_files['foo.txt'])\n\nCLASS\ntorch.export.Constraint(*args, **kwargs)\n[SOURCE]\n\nWARNING\n\nDo not construct Constraint directly, use dynamic_dim() instead.\n\nThis represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range.\n\nCLASS\ntorch.export.ExportedProgram(root, graph, graph_signature, call_spec, state_dict, range_constraints, equality_constraints, module_call_graph, example_inputs=None)\n[SOURCE]\n\nPackage of a program from export(). It contains an torch.fx.Graph that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.\n\nYou can call an ExportedProgram like the original callable traced by export() with the same calling convention.\n\nTo perform transformations on the graph, use .module property to access an torch.fx.GraphModule. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use export() again to construct a correct ExportedProgram.\n\nmodule()\n[SOURCE]\n\nReturns a self contained GraphModule with all the parameters/buffers inlined.\n\nReturn type\n\nModule\n\nCLASS\ntorch.export.ExportBackwardSignature(gradients_to_parameters: Dict[str, str], gradients_to_user_inputs: Dict[str, str], loss_output: str)\n[SOURCE]\nCLASS\ntorch.export.ExportGraphSignature(parameters, buffers, user_inputs, user_outputs, inputs_to_parameters, inputs_to_buffers, buffers_to_mutate, backward_signature, assertion_dep_token=None)\n[SOURCE]\n\nExportGraphSignature models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.\n\nExport Graph is functional and does not access “states” like parameters or buffers within the graph via getattr nodes. Instead, export() gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.\n\nThe ordering of all inputs and outputs are:\n\nInputs = [*parameters_buffers, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs]\n\n\ne.g. If following module is exported:\n\nclass CustomModule(nn.Module):\n    def __init__(self):\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer('my_buffer1', torch.tensor(3.0))\n        self.register_buffer('my_buffer2', torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0) # In-place addition\n\n        return output\n\n\nResulting Graph would be:\n\ngraph():\n    %arg0_1 := placeholder[target=arg0_1]\n    %arg1_1 := placeholder[target=arg1_1]\n    %arg2_1 := placeholder[target=arg2_1]\n    %arg3_1 := placeholder[target=arg3_1]\n    %arg4_1 := placeholder[target=arg4_1]\n    %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})\n    %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})\n    %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})\n    %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})\n    %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})\n    return (add_tensor_2, add_tensor_1)\n\n\nResulting ExportGraphSignature would be:\n\nExportGraphSignature(\n    # Indicates that there is one parameter named `my_parameter`\n    parameters=['L__self___my_parameter'],\n\n    # Indicates that there are two buffers, `my_buffer1` and `my_buffer2`\n    buffers=['L__self___my_buffer1', 'L__self___my_buffer2'],\n\n    # Indicates that the nodes `arg3_1` and `arg4_1` in produced graph map to\n    # original user inputs, ie. x1 and x2\n    user_inputs=['arg3_1', 'arg4_1'],\n\n    # Indicates that the node `add_tensor_1` maps to output of original program\n    user_outputs=['add_tensor_1'],\n\n    # Indicates that there is one parameter (self.my_parameter) captured,\n    # its name is now mangled to be `L__self___my_parameter`, which is now\n    # represented by node `arg0_1` in the graph.\n    inputs_to_parameters={'arg0_1': 'L__self___my_parameter'},\n\n    # Indicates that there are two buffers (self.my_buffer1, self.my_buffer2) captured,\n    # their name are now mangled to be `L__self___my_my_buffer1` and `L__self___my_buffer2`.\n    # They are now represented by nodes `arg1_1` and `arg2_1` in the graph.\n    inputs_to_buffers={'arg1_1': 'L__self___my_buffer1', 'arg2_1': 'L__self___my_buffer2'},\n\n    # Indicates that one buffer named `L__self___my_buffer2` is mutated during execution,\n    # its new value is output from the graph represented by the node named `add_tensor_2`\n    buffers_to_mutate={'add_tensor_2': 'L__self___my_buffer2'},\n\n    # Backward graph not captured\n    backward_signature=None,\n\n    # Work in progress feature, please ignore now.\n    assertion_dep_token=None\n)\n\nCLASS\ntorch.export.ArgumentKind(value)\n[SOURCE]\n\nAn enumeration.\n\nCLASS\ntorch.export.ArgumentSpec(kind: torch.export.ArgumentKind, value: Any)\n[SOURCE]\nCLASS\ntorch.export.ModuleCallSignature(inputs: List[torch.export.ArgumentSpec], outputs: List[torch.export.ArgumentSpec], in_spec: torch.utils._pytree.TreeSpec, out_spec: torch.utils._pytree.TreeSpec)\n[SOURCE]\nCLASS\ntorch.export.ModuleCallEntry(fqn: str, signature: Union[torch.export.ModuleCallSignature, NoneType] = None)\n[SOURCE]\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.export\nOverview\nExporting a PyTorch Model\nLimitations of torch.export\nRead More\nAPI Reference\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.backends — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/backends.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.backends\nShortcuts\nTORCH.BACKENDS\n\ntorch.backends controls the behavior of various backends that PyTorch supports.\n\nThese backends include:\n\ntorch.backends.cpu\n\ntorch.backends.cuda\n\ntorch.backends.cudnn\n\ntorch.backends.mps\n\ntorch.backends.mkl\n\ntorch.backends.mkldnn\n\ntorch.backends.openmp\n\ntorch.backends.opt_einsum\n\ntorch.backends.xeon\n\ntorch.backends.cpu\ntorch.backends.cpu.get_cpu_capability()\n[SOURCE]\n\nReturns cpu capability as a string value.\n\nPossible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”\n\nReturn type\n\nstr\n\ntorch.backends.cuda\ntorch.backends.cuda.is_built()\n[SOURCE]\n\nReturns whether PyTorch is built with CUDA support. Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.\n\ntorch.backends.cuda.matmul.allow_tf32\n\nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.\n\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction\n\nA bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.\n\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction\n\nA bool that controls whether reduced precision reductions are allowed with bf16 GEMMs.\n\ntorch.backends.cuda.cufft_plan_cache\n\ncufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i’s cache via torch.backends.cuda.cufft_plan_cache[i].\n\ntorch.backends.cuda.cufft_plan_cache.size\n\nA readonly int that shows the number of plans currently in a cuFFT plan cache.\n\ntorch.backends.cuda.cufft_plan_cache.max_size\n\nA int that controls the capacity of a cuFFT plan cache.\n\ntorch.backends.cuda.cufft_plan_cache.clear()\n\nClears a cuFFT plan cache.\n\ntorch.backends.cuda.preferred_linalg_library(backend=None)\n[SOURCE]\n\nWARNING\n\nThis flag is experimental and subject to change.\n\nWhen PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.\n\nIf “cusolver” is set then cuSOLVER will be used wherever possible.\n\nIf “magma” is set then MAGMA will be used wherever possible.\n\nIf “default” (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available.\n\nWhen no input is given, this function returns the currently preferred library.\n\nUser may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect for your application’s inputs.\n\nCurrently supported linalg operators:\n\ntorch.linalg.inv()\n\ntorch.linalg.inv_ex()\n\ntorch.linalg.cholesky()\n\ntorch.linalg.cholesky_ex()\n\ntorch.cholesky_solve()\n\ntorch.cholesky_inverse()\n\ntorch.linalg.lu_factor()\n\ntorch.linalg.lu()\n\ntorch.linalg.lu_solve()\n\ntorch.linalg.qr()\n\ntorch.linalg.eigh()\n\ntorch.linalg.eighvals()\n\ntorch.linalg.svd()\n\ntorch.linalg.svdvals()\n\nReturn type\n\n_LinalgBackend\n\nCLASS\ntorch.backends.cuda.SDPBackend(value)\n[SOURCE]\n\nEnum class for the scaled dot product attention backends.\n\nWARNING\n\nThis class is in beta and subject to change.\n\nThis class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h\n\ntorch.backends.cuda.flash_sdp_enabled()\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nReturns whether flash scaled dot product attention is enabled or not.\n\ntorch.backends.cuda.enable_mem_efficient_sdp(enabled)\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nEnables or disables memory efficient scaled dot product attention.\n\ntorch.backends.cuda.mem_efficient_sdp_enabled()\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nReturns whether memory efficient scaled dot product attention is enabled or not.\n\ntorch.backends.cuda.enable_flash_sdp(enabled)\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nEnables or disables flash scaled dot product attention.\n\ntorch.backends.cuda.math_sdp_enabled()\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nReturns whether math scaled dot product attention is enabled or not.\n\ntorch.backends.cuda.enable_math_sdp(enabled)\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nEnables or disables math scaled dot product attention.\n\ntorch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True)\n[SOURCE]\n\nWARNING\n\nThis flag is beta and subject to change.\n\nThis context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.\n\ntorch.backends.cudnn\ntorch.backends.cudnn.version()\n[SOURCE]\n\nReturns the version of cuDNN\n\ntorch.backends.cudnn.is_available()\n[SOURCE]\n\nReturns a bool indicating if CUDNN is currently available.\n\ntorch.backends.cudnn.enabled\n\nA bool that controls whether cuDNN is enabled.\n\ntorch.backends.cudnn.allow_tf32\n\nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.\n\ntorch.backends.cudnn.deterministic\n\nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms().\n\ntorch.backends.cudnn.benchmark\n\nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n\ntorch.backends.cudnn.benchmark_limit\n\nA int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.\n\ntorch.backends.mps\ntorch.backends.mps.is_available()\n[SOURCE]\n\nReturns a bool indicating if MPS is currently available.\n\nReturn type\n\nbool\n\ntorch.backends.mps.is_built()\n[SOURCE]\n\nReturns whether PyTorch is built with MPS support. Note that this doesn’t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.\n\nReturn type\n\nbool\n\ntorch.backends.mkl\ntorch.backends.mkl.is_available()\n[SOURCE]\n\nReturns whether PyTorch is built with MKL support.\n\nCLASS\ntorch.backends.mkl.verbose(enable)\n[SOURCE]\n\nOn-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.\n\nimport torch\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n\nParameters\n\nlevel – Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing\n\ntorch.backends.mkldnn\ntorch.backends.mkldnn.is_available()\n[SOURCE]\n\nReturns whether PyTorch is built with MKL-DNN support.\n\nCLASS\ntorch.backends.mkldnn.verbose(level)\n[SOURCE]\n\nOn-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.\n\nimport torch\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n\nParameters\n\nlevel – Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation\n\ntorch.backends.openmp\ntorch.backends.openmp.is_available()\n[SOURCE]\n\nReturns whether PyTorch is built with OpenMP support.\n\ntorch.backends.opt_einsum\ntorch.backends.opt_einsum.is_available()\n[SOURCE]\n\nReturns a bool indicating if opt_einsum is currently available.\n\nReturn type\n\nbool\n\ntorch.backends.opt_einsum.get_opt_einsum()\n[SOURCE]\n\nReturns the opt_einsum package if opt_einsum is currently available, else None.\n\nReturn type\n\nAny\n\ntorch.backends.opt_einsum.enabled\n\nA :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.\n\nIf opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.\n\ntorch.backends.opt_einsum.strategy\n\nA :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal” strategies are also supported. Note that the “optimal” strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum’s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n\ntorch.backends.xeon\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.backends\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.mps\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.xeon\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.cuda — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/cuda.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.cuda\nShortcuts\nTORCH.CUDA\n\nThis package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.\n\nIt is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA.\n\nCUDA semantics has more details about working with CUDA.\n\nStreamContext\n\n\t\n\nContext-manager that selects a given stream.\n\n\n\n\ncan_device_access_peer\n\n\t\n\nChecks if peer access between two devices is possible.\n\n\n\n\ncurrent_blas_handle\n\n\t\n\nReturns cublasHandle_t pointer to current cuBLAS handle\n\n\n\n\ncurrent_device\n\n\t\n\nReturns the index of a currently selected device.\n\n\n\n\ncurrent_stream\n\n\t\n\nReturns the currently selected Stream for a given device.\n\n\n\n\ndefault_stream\n\n\t\n\nReturns the default Stream for a given device.\n\n\n\n\ndevice\n\n\t\n\nContext-manager that changes the selected device.\n\n\n\n\ndevice_count\n\n\t\n\nReturns the number of GPUs available.\n\n\n\n\ndevice_of\n\n\t\n\nContext-manager that changes the current device to that of given object.\n\n\n\n\nget_arch_list\n\n\t\n\nReturns list CUDA architectures this library was compiled for.\n\n\n\n\nget_device_capability\n\n\t\n\nGets the cuda capability of a device.\n\n\n\n\nget_device_name\n\n\t\n\nGets the name of a device.\n\n\n\n\nget_device_properties\n\n\t\n\nGets the properties of a device.\n\n\n\n\nget_gencode_flags\n\n\t\n\nReturns NVCC gencode flags this library was compiled with.\n\n\n\n\nget_sync_debug_mode\n\n\t\n\nReturns current value of debug mode for cuda synchronizing operations.\n\n\n\n\ninit\n\n\t\n\nInitialize PyTorch's CUDA state.\n\n\n\n\nipc_collect\n\n\t\n\nForce collects GPU memory after it has been released by CUDA IPC.\n\n\n\n\nis_available\n\n\t\n\nReturns a bool indicating if CUDA is currently available.\n\n\n\n\nis_initialized\n\n\t\n\nReturns whether PyTorch's CUDA state has been initialized.\n\n\n\n\nmemory_usage\n\n\t\n\nReturns the percent of time over the past sample period during which global (device) memory was being read or written.\n\n\n\n\nset_device\n\n\t\n\nSets the current device.\n\n\n\n\nset_stream\n\n\t\n\nSets the current stream.This is a wrapper API to set the stream.\n\n\n\n\nset_sync_debug_mode\n\n\t\n\nSets the debug mode for cuda synchronizing operations.\n\n\n\n\nstream\n\n\t\n\nWrapper around the Context-manager StreamContext that selects a given stream.\n\n\n\n\nsynchronize\n\n\t\n\nWaits for all kernels in all streams on a CUDA device to complete.\n\n\n\n\nutilization\n\n\t\n\nReturns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.\n\n\n\n\ntemperature\n\n\t\n\nReturns the average temperature of the GPU sensor in Degrees C (Centigrades)\n\n\n\n\npower_draw\n\n\t\n\nReturns the average power draw of the GPU sensor in mW (MilliWatts)\n\n\n\n\nclock_rate\n\n\t\n\nReturns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.\n\n\n\n\nOutOfMemoryError\n\n\t\n\nException raised when CUDA is out of memory\n\nRandom Number Generator\n\nget_rng_state\n\n\t\n\nReturns the random number generator state of the specified GPU as a ByteTensor.\n\n\n\n\nget_rng_state_all\n\n\t\n\nReturns a list of ByteTensor representing the random number states of all devices.\n\n\n\n\nset_rng_state\n\n\t\n\nSets the random number generator state of the specified GPU.\n\n\n\n\nset_rng_state_all\n\n\t\n\nSets the random number generator state of all devices.\n\n\n\n\nmanual_seed\n\n\t\n\nSets the seed for generating random numbers for the current GPU.\n\n\n\n\nmanual_seed_all\n\n\t\n\nSets the seed for generating random numbers on all GPUs.\n\n\n\n\nseed\n\n\t\n\nSets the seed for generating random numbers to a random number for the current GPU.\n\n\n\n\nseed_all\n\n\t\n\nSets the seed for generating random numbers to a random number on all GPUs.\n\n\n\n\ninitial_seed\n\n\t\n\nReturns the current random seed of the current GPU.\n\nCommunication collectives\n\ncomm.broadcast\n\n\t\n\nBroadcasts a tensor to specified GPU devices.\n\n\n\n\ncomm.broadcast_coalesced\n\n\t\n\nBroadcasts a sequence tensors to the specified GPUs.\n\n\n\n\ncomm.reduce_add\n\n\t\n\nSums tensors from multiple GPUs.\n\n\n\n\ncomm.scatter\n\n\t\n\nScatters tensor across multiple GPUs.\n\n\n\n\ncomm.gather\n\n\t\n\nGathers tensors from multiple GPU devices.\n\nStreams and events\n\nStream\n\n\t\n\nWrapper around a CUDA stream.\n\n\n\n\nExternalStream\n\n\t\n\nWrapper around an externally allocated CUDA stream.\n\n\n\n\nEvent\n\n\t\n\nWrapper around a CUDA event.\n\nGraphs (beta)\n\nis_current_stream_capturing\n\n\t\n\nReturns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\n\n\n\n\ngraph_pool_handle\n\n\t\n\nReturns an opaque token representing the id of a graph memory pool.\n\n\n\n\nCUDAGraph\n\n\t\n\nWrapper around a CUDA graph.\n\n\n\n\ngraph\n\n\t\n\nContext-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.\n\n\n\n\nmake_graphed_callables\n\n\t\n\nAccepts callables (functions or nn.Modules) and returns graphed versions.\n\nMemory management\n\nempty_cache\n\n\t\n\nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n\n\n\n\nlist_gpu_processes\n\n\t\n\nReturns a human-readable printout of the running processes and their GPU memory use for a given device.\n\n\n\n\nmem_get_info\n\n\t\n\nReturns the global free and total GPU memory for a given device using cudaMemGetInfo.\n\n\n\n\nmemory_stats\n\n\t\n\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\n\n\n\nmemory_summary\n\n\t\n\nReturns a human-readable printout of the current memory allocator statistics for a given device.\n\n\n\n\nmemory_snapshot\n\n\t\n\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\n\n\n\nmemory_allocated\n\n\t\n\nReturns the current GPU memory occupied by tensors in bytes for a given device.\n\n\n\n\nmax_memory_allocated\n\n\t\n\nReturns the maximum GPU memory occupied by tensors in bytes for a given device.\n\n\n\n\nreset_max_memory_allocated\n\n\t\n\nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device.\n\n\n\n\nmemory_reserved\n\n\t\n\nReturns the current GPU memory managed by the caching allocator in bytes for a given device.\n\n\n\n\nmax_memory_reserved\n\n\t\n\nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\n\n\n\nset_per_process_memory_fraction\n\n\t\n\nSet memory fraction for a process.\n\n\n\n\nmemory_cached\n\n\t\n\nDeprecated; see memory_reserved().\n\n\n\n\nmax_memory_cached\n\n\t\n\nDeprecated; see max_memory_reserved().\n\n\n\n\nreset_max_memory_cached\n\n\t\n\nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.\n\n\n\n\nreset_peak_memory_stats\n\n\t\n\nResets the \"peak\" stats tracked by the CUDA memory allocator.\n\n\n\n\ncaching_allocator_alloc\n\n\t\n\nPerforms a memory allocation using the CUDA memory allocator.\n\n\n\n\ncaching_allocator_delete\n\n\t\n\nDeletes memory allocated using the CUDA memory allocator.\n\n\n\n\nget_allocator_backend\n\n\t\n\nReturns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.\n\n\n\n\nCUDAPluggableAllocator\n\n\t\n\nCUDA memory allocator loaded from a so file.\n\n\n\n\nchange_current_allocator\n\n\t\n\nChanges the currently used memory allocator to be the one provided.\n\nNVIDIA Tools Extension (NVTX)\n\nnvtx.mark\n\n\t\n\nDescribe an instantaneous event that occurred at some point.\n\n\n\n\nnvtx.range_push\n\n\t\n\nPushes a range onto a stack of nested range span.\n\n\n\n\nnvtx.range_pop\n\n\t\n\nPops a range off of a stack of nested range spans.\n\nJiterator (beta)\n\njiterator._create_jit_fn\n\n\t\n\nCreate a jiterator-generated cuda kernel for an elementwise op.\n\n\n\n\njiterator._create_multi_output_jit_fn\n\n\t\n\nCreate a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.\n\nStream Sanitizer (prototype)\n\nCUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.cuda\nRandom Number Generator\nCommunication collectives\nStreams and events\nGraphs (beta)\nMemory management\nNVIDIA Tools Extension (NVTX)\nJiterator (beta)\nStream Sanitizer (prototype)\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.mps — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/mps.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.mps\nShortcuts\nTORCH.MPS\n\nThis package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See https://developer.apple.com/documentation/metalperformanceshaders for more details.\n\nsynchronize\n\n\t\n\nWaits for all kernels in all streams on a MPS device to complete.\n\n\n\n\nget_rng_state\n\n\t\n\nReturns the random number generator state as a ByteTensor.\n\n\n\n\nset_rng_state\n\n\t\n\nSets the random number generator state.\n\n\n\n\nmanual_seed\n\n\t\n\nSets the seed for generating random numbers.\n\n\n\n\nseed\n\n\t\n\nSets the seed for generating random numbers to a random number.\n\n\n\n\nempty_cache\n\n\t\n\nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.\n\n\n\n\nset_per_process_memory_fraction\n\n\t\n\nSet memory fraction for limiting process's memory allocation on MPS device.\n\n\n\n\ncurrent_allocated_memory\n\n\t\n\nReturns the current GPU memory occupied by tensors in bytes.\n\n\n\n\ndriver_allocated_memory\n\n\t\n\nReturns total GPU memory allocated by Metal driver for the process in bytes.\n\nMPS Profiler\n\nprofiler.start\n\n\t\n\nStart OS Signpost tracing from MPS backend.\n\n\n\n\nprofiler.stop\n\n\t\n\nStops generating OS Signpost tracing from MPS backend.\n\n\n\n\nprofiler.profile\n\n\t\n\nContext Manager to enabling generating OS Signpost tracing from MPS backend.\n\nMPS Event\n\nevent.Event\n\n\t\n\nWrapper around an MPS event.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.mps\nMPS Profiler\nMPS Event\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Understanding CUDA Memory Usage — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/torch_cuda_memory.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Understanding CUDA Memory Usage\nShortcuts\nUNDERSTANDING CUDA MEMORY USAGE\n\nTo debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory at any point in time, and optionally record the history of allocation events that led up to that snapshot.\n\nThe generated snapshots can then be drag and dropped onto the interactiver viewer hosted at pytorch.org/memory_viz which can be used to explore the snapshot.\n\nGENERATING A SNAPSHOT\n\nThe common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:\n\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n\nUSING THE VISUALIZER\n\nOpen pytorch.org/memory_viz and drag/drop the pickled snapshot file into the visualizer. The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.\n\nActive Memory Timeline\n\nThe Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations. Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to render fewer allocations and improve performance when there is a lot of data.\n\nAllocator State History\n\nThe Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occured, such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why an allocation failed even though reserved memory still exists.\n\nThe stack trace information also reports the address at which an allocation occured. The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the “_0”th time this address was allocated. This unique string can be looked up in the Active Memory Timeline and searched in the Active State History to examine the memory state when a tensor was allocated or freed.\n\nSNAPSHOT API REFERENCE\ntorch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None)\n[SOURCE]\n\nEnables recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in torch.cuda.memory._snapshot().\n\nIn addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events.\n\nUse torch.cuda.memory._snapshot() to retrieve this information, and the tools in _memory_viz.py to visualize snapshots.\n\nThe Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues.\n\nC++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth.\n\nParameters\n\nenabled (Literal[None, \"state\", \"all\"], optional) – None, disable recording memory history. “state”, keep information for currenly allocated memory. “all”, additionally keep a history of all alloc/free calls. Defaults to “all”.\n\ncontext (Literal[None, \"state\", \"alloc\", \"all\"], optional) – None, Do not record any tracebacks. “state”, Record tracebacks for currently allocated memory. “alloc”, additionally keep tracebacks for alloc calls. “all”, additionally keep tracebacks for free calls. Defaults to “all”.\n\nstacks (Literal[\"python\", \"all\"], optional) – “python”, include Python, TorchScript, and inductor frames in tracebacks “all”, additionally include C++ frames Defaults to “all”.\n\nmax_entries (int, optional) – Keep a maximum of max_entries alloc/free events in the recorded history recorded.\n\ntorch.cuda.memory._snapshot(device=None)\n[SOURCE]\n\nSaves a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure.\n\nclass Snapshot(TypedDict):\n    segments : List[Segment]\n    device_traces: List[List[TraceEntry]]\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal['small', 'large'] # 'large' (>1MB)\n    allocated_size: int # size of memory in use\n    active_size: int # size of memory in use or in active_awaiting_free state\n    blocks : List[Block]\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int # size requested during malloc, may be smaller than\n                        # size due to rounding\n    address: int\n    state: Literal['active_allocated', # used by a tensor\n                'active_awaiting_free', # waiting for another stream to finish using\n                                        # this, then it will become free\n                'inactive',] # free for reuse\n    frames: List[Frame] # stack trace from where the allocation occurred\n\nclass Frame(TypedDict):\n        filename: str\n        line: int\n        name: str\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n    'alloc'  # memory allocated\n    'free_requested', # the allocated received a call to free memory\n    'free_completed', # the memory that was requested to be freed is now\n                    # able to be used in future allocation calls\n    'segment_alloc', # the caching allocator ask cudaMalloc for more memory\n                    # and added it as a segment in its cache\n    'segment_free',  # the caching allocator called cudaFree to return memory\n                    # to cuda possibly trying free up memory to\n                    # allocate more segments or because empty_caches was called\n    'oom',          # the allocator threw an OOM exception. 'size' is\n                    # the requested number of bytes that did not succeed\n    'snapshot'      # the allocator generated a memory snapshot\n                    # useful to coorelate a previously taken\n                    # snapshot with this trace\n    ]\n    addr: int # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int # only present for OOM, the amount of\n                    # memory cuda still reports to be free\n\nReturns\n\nThe Snapshot dictionary object\n\ntorch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle')\n[SOURCE]\n\nSaves a pickled version of the torch.memory._snapshot() dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz\n\nParameters\n\nfilename (str, optional) – Name of the file to create. Defaults to “dump_snapshot.pickle”.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nActive Memory Timeline\nAllocator State History\nSnapshot API Reference\n_record_memory_history()\n_snapshot()\n_dump_snapshot()\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.cpu — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/cpu.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.cpu\nShortcuts\nTORCH.CPU\n\nThis package implements abstractions found in torch.cuda to facilitate writing device-agnostic code.\n\ncurrent_stream\n\n\t\n\nReturns the currently selected Stream for a given device.\n\n\n\n\nis_available\n\n\t\n\nReturns a bool indicating if CPU is currently available.\n\n\n\n\nsynchronize\n\n\t\n\nWaits for all kernels in all streams on the CPU device to complete.\n\n\n\n\nstream\n\n\t\n\nWrapper around the Context-manager StreamContext that selects a given stream.\n\n\n\n\ndevice_count\n\n\t\n\nReturns number of CPU devices (not cores).\n\n\n\n\nStreamContext\n\n\t\n\nContext-manager that selects a given stream.\n\nStreams and events\n\nStream\n\n\t\n\nN.B.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.cpu\nStreams and events\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Automatic differentiation package - torch.autograd — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/autograd.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Automatic differentiation package - torch.autograd\nShortcuts\nAUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD\n\ntorch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).\n\nbackward\n\n\t\n\nComputes the sum of gradients of given tensors with respect to graph leaves.\n\n\n\n\ngrad\n\n\t\n\nComputes and returns the sum of gradients of outputs with respect to the inputs.\n\nForward-mode Automatic Differentiation\n\nWARNING\n\nThis API is in beta. Even though the function signatures are very unlikely to change, improved operator coverage is planned before we consider this stable.\n\nPlease see the forward-mode AD tutorial for detailed steps on how to use this API.\n\nforward_ad.dual_level\n\n\t\n\nContext-manager that enables forward AD.\n\n\n\n\nforward_ad.make_dual\n\n\t\n\nAssociates a tensor value with a forward gradient, the tangent, to create a \"dual tensor\", which is used to compute forward AD gradients.\n\n\n\n\nforward_ad.unpack_dual\n\n\t\n\nUnpacks a \"dual tensor\" to get both its Tensor value and its forward AD gradient.\n\nFunctional higher level API\n\nWARNING\n\nThis API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.\n\nThis section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.\n\nThis API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).\n\nfunctional.jacobian\n\n\t\n\nFunction that computes the Jacobian of a given function.\n\n\n\n\nfunctional.hessian\n\n\t\n\nFunction that computes the Hessian of a given scalar function.\n\n\n\n\nfunctional.vjp\n\n\t\n\nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.\n\n\n\n\nfunctional.jvp\n\n\t\n\nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.\n\n\n\n\nfunctional.vhp\n\n\t\n\nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.\n\n\n\n\nfunctional.hvp\n\n\t\n\nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.\n\nLocally disabling gradient computation\n\nSee Locally disabling gradient computation for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see Locally disabling gradient computation for a list of functions that can be used to locally disable gradients.\n\nDefault gradient layouts\n\nWhen a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows.\n\nIf param.grad is initially None:\n\nIf param’s memory is non-overlapping and dense, .grad is created with strides matching param (thus matching param’s layout).\n\nOtherwise, .grad is created with rowmajor-contiguous strides.\n\nIf param already has a non-sparse .grad attribute:\n\nIf create_graph=False, backward() accumulates into .grad in-place, which preserves its strides.\n\nIf create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad’s strides.\n\nThe default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts.\n\nIn fact, resetting all .grads to None before each accumulation phase, e.g.:\n\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n\n\nsuch that they’re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks.\n\nManual gradient layouts\n\nIf you need manual control over .grad’s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True.\n\nIn-place operations on Tensors\n\nSupporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\n\nIn-place correctness checks\n\nAll Tensor s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.\n\nVariable (deprecated)\n\nWARNING\n\nThe Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed:\n\nVariable(tensor) and Variable(tensor, requires_grad) still work as expected, but they return Tensors instead of Variables.\n\nvar.data is the same thing as tensor.data.\n\nMethods such as var.backward(), var.detach(), var.register_hook() now work on tensors with the same method names.\n\nIn addition, one can now create tensors with requires_grad=True using factory methods such as torch.randn(), torch.zeros(), torch.ones(), and others like the following:\n\nautograd_tensor = torch.randn((2, 3, 4), requires_grad=True)\n\nTensor autograd functions\n\ntorch.Tensor.grad\n\n\t\n\nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.\n\n\n\n\ntorch.Tensor.requires_grad\n\n\t\n\nIs True if gradients need to be computed for this Tensor, False otherwise.\n\n\n\n\ntorch.Tensor.is_leaf\n\n\t\n\nAll Tensors that have requires_grad which is False will be leaf Tensors by convention.\n\n\n\n\ntorch.Tensor.backward([gradient, ...])\n\n\t\n\nComputes the gradient of current tensor wrt graph leaves.\n\n\n\n\ntorch.Tensor.detach\n\n\t\n\nReturns a new Tensor, detached from the current graph.\n\n\n\n\ntorch.Tensor.detach_\n\n\t\n\nDetaches the Tensor from the graph that created it, making it a leaf.\n\n\n\n\ntorch.Tensor.register_hook(hook)\n\n\t\n\nRegisters a backward hook.\n\n\n\n\ntorch.Tensor.register_post_accumulate_grad_hook(hook)\n\n\t\n\nRegisters a backward hook that runs after grad accumulation.\n\n\n\n\ntorch.Tensor.retain_grad()\n\n\t\n\nEnables this Tensor to have their grad populated during backward().\n\nFunction\nCLASS\ntorch.autograd.Function(*args, **kwargs)\n[SOURCE]\n\nBase class to create custom autograd.Function\n\nTo create a custom autograd.Function, subclass this class and implement the forward() and backward() static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call forward() directly.\n\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using torch.autograd.gradcheck().\n\nSee Extending torch.autograd for more details on how to use this class.\n\nExamples:\n\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\n\nFunction.forward\n\n\t\n\nThis function is to be overridden by all subclasses.\n\n\n\n\nFunction.backward\n\n\t\n\nDefines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).\n\n\n\n\nFunction.jvp\n\n\t\n\nDefines a formula for differentiating the operation with forward mode automatic differentiation.\n\n\n\n\nFunction.vmap\n\n\t\n\nDefines a rule for the behavior of this autograd.Function underneath torch.vmap().\n\nContext method mixins\n\nWhen creating a new Function, the following methods are available to ctx.\n\nfunction.FunctionCtx.mark_dirty\n\n\t\n\nMarks given tensors as modified in an in-place operation.\n\n\n\n\nfunction.FunctionCtx.mark_non_differentiable\n\n\t\n\nMarks outputs as non-differentiable.\n\n\n\n\nfunction.FunctionCtx.save_for_backward\n\n\t\n\nSaves given tensors for a future call to backward().\n\n\n\n\nfunction.FunctionCtx.set_materialize_grads\n\n\t\n\nSets whether to materialize grad tensors.\n\nNumerical gradient checking\n\ngradcheck\n\n\t\n\nCheck gradients computed via small finite differences against analytical gradients wrt tensors in inputs that are of floating point or complex type and with requires_grad=True.\n\n\n\n\ngradgradcheck\n\n\t\n\nCheck gradients of gradients computed via small finite differences against analytical gradients wrt tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True.\n\nProfiler\n\nAutograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using profile. nvprof based (registers both CPU and GPU activity) using emit_nvtx. and vtune profiler based using emit_itt.\n\nCLASS\ntorch.autograd.profiler.profile(enabled=True, *, use_cuda=False, use_device=None, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True, use_mtia=False, experimental_config=None)\n[SOURCE]\n\nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks\n\nParameters\n\nenabled (bool, optional) – Setting this to False makes this context manager a no-op.\n\nuse_cuda (bool, optional) – Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.\n\nrecord_shapes (bool, optional) – If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.\n\nwith_flops (bool, optional) – If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operator’s input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.\n\nprofile_memory (bool, optional) – track tensor memory allocation/deallocation.\n\nwith_stack (bool, optional) – record source information (file and line number) for the ops.\n\nwith_modules (bool) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.\n\nuse_kineto (bool, optional) – experimental, enable profiling with Kineto profiler.\n\nuse_cpu (bool, optional) – profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling.\n\nexperimental_config (_ExperimentalConfig) – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\n\nExample\n\n>>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>>         y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n\n\nprofiler.profile.export_chrome_trace\n\n\t\n\nExports an EventList as a Chrome tracing tools file.\n\n\n\n\nprofiler.profile.key_averages\n\n\t\n\nAverages all function events over their keys.\n\n\n\n\nprofiler.profile.self_cpu_time_total\n\n\t\n\nReturns total time spent on CPU obtained as a sum of all self times across all the events.\n\n\n\n\nprofiler.profile.total_average\n\n\t\n\nAverages all events.\n\nCLASS\ntorch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False)\n[SOURCE]\n\nContext manager that makes every autograd operation emit an NVTX range.\n\nIt is useful when running the program under nvprof:\n\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n\n\nUnfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.\n\nParameters\n\nenabled (bool, optional) – Setting enabled=False makes this context manager a no-op. Default: True.\n\nrecord_shapes (bool, optional) – If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: False\n\nExample\n\n>>> with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n\n\nForward-backward correlation\n\nWhen viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates.\n\nDuring the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function.\n\nAny functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass.\n\nDouble-backward\n\nIf, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function’s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass.\n\nCLASS\ntorch.autograd.profiler.emit_itt(enabled=True, record_shapes=False)\n[SOURCE]\n\nContext manager that makes every autograd operation emit an ITT range.\n\nIt is useful when running the program under Intel(R) VTune Profiler:\n\nvtune <--vtune-flags> <regular command here>\n\n\nThe Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI.\n\nParameters\n\nenabled (bool, optional) – Setting enabled=False makes this context manager a no-op. Default: True.\n\nrecord_shapes (bool, optional) – If record_shapes=True, the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: False\n\nExample\n\n>>> with torch.autograd.profiler.emit_itt():\n...     model(x)\n\n\nprofiler.load_nvprof\n\n\t\n\nOpens an nvprof trace file and parses autograd annotations.\n\nAnomaly detection\nCLASS\ntorch.autograd.detect_anomaly(check_nan=True)\n[SOURCE]\n\nContext-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\nRunning the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.\n\nIf check_nan is True, any backward computation that generate “nan” value will raise an error. Default True.\n\nWARNING\n\nThis mode should be enabled only for debugging as the different tests will slow down your program execution.\n\nExample\n\n>>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n\nCLASS\ntorch.autograd.set_detect_anomaly(mode, check_nan=True)\n[SOURCE]\n\nContext-manager that sets the anomaly detection for the autograd engine on or off.\n\nset_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.\n\nSee detect_anomaly above for details of the anomaly detection behaviour.\n\nParameters\n\nmode (bool) – Flag whether to enable anomaly detection (True), or disable (False).\n\ncheck_nan (bool) – Flag whether to raise an error when the backward generate “nan”\n\nAutograd graph\n\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during the backward pass.\n\nThe grad_fn attribute of a torch.Tensor holds a torch.autograd.graph.Node if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or None otherwise.\n\ngraph.Node.name\n\n\t\n\nReturns the name.\n\n\n\n\ngraph.Node.metadata\n\n\t\n\nReturns the metadata.\n\n\n\n\ngraph.Node.next_functions\n\n\t\n\n\n\n\ngraph.Node.register_hook\n\n\t\n\nRegisters a backward hook.\n\n\n\n\ngraph.Node.register_prehook\n\n\t\n\nRegisters a backward pre-hook.\n\nSome operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the grad_fn and can be accessed. For example:\n\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n\n\nYou can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see Hooks for saved tensors.\n\nCLASS\ntorch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook)\n[SOURCE]\n\nContext-manager that sets a pair of pack / unpack hooks for saved tensors.\n\nUse this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.\n\nIn that context, the pack_hook function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using save_for_backward() but also those recorded by a PyTorch-defined operation). The output of pack_hook is then stored in the computation graph instead of the original tensor.\n\nThe unpack_hook is called when the saved tensor needs to be accessed, namely when executing torch.Tensor.backward() or torch.autograd.grad(). It takes as argument the packed object returned by pack_hook and should return a tensor which has the same content as the original tensor (passed as input to the corresponding pack_hook).\n\nThe hooks should have the following signatures:\n\npack_hook(tensor: Tensor) -> Any\n\nunpack_hook(Any) -> Tensor\n\nwhere the return value of pack_hook is a valid input to unpack_hook.\n\nIn general, you want unpack_hook(pack_hook(t)) to be equal to t in terms of value, size, dtype and device.\n\nExample:\n\n>>> def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x\n>>>\n>>> def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n>>>\n>>> a = torch.ones(5, requires_grad=True)\n>>> b = torch.ones(5, requires_grad=True) * 2\n>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n>>> y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n\n\nWARNING\n\nPerforming an inplace operation on the input to either hooks may lead to undefined behavior.\n\nWARNING\n\nOnly one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied.\n\nCLASS\ntorch.autograd.graph.save_on_cpu(pin_memory=False, device_type='cuda')\n[SOURCE]\n\nContext-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.\n\nWhen performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.\n\nUse this context-manager to trade compute for GPU memory usage (e.g. when your model doesn’t fit in GPU memory during training).\n\nParameters\n\npin_memory (bool) – If True tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to False. Also see Use pinned memory buffers.\n\nExample:\n\n>>> a = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> b = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> c = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>>\n>>> def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n>>>\n>>> y = f(a, b, c)\n>>> del a, b, c  # for illustration only\n>>> # the content of a, b, and prod_2 are still alive on GPU\n>>> # the content of prod_1 and c only live on CPU\n>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n>>> # all intermediary tensors are released (deleted) after the call to backward\n\nCLASS\ntorch.autograd.graph.disable_saved_tensors_hooks(error_message)\n[SOURCE]\n\nContext-manager that disables the saved tensors default hooks feature.\n\nUseful for if you are creating a feature that does not work with saved tensors default hooks.\n\nParameters\n\nerror_message (str) – When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised.\n\nExample:\n\n>>> message = \"saved tensors default hooks are disabled\"\n>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass\n\nCLASS\ntorch.autograd.graph.register_multi_grad_hook(tensors, fn)\n[SOURCE]\n\nRegisters a multi-grad backward hook.\n\nThe hook will be called after gradients with respect to every tensor in tensors have been computed. If a tensor is in tensors but is not part of the graph, or if a tensor is not needed to compute the gradients for any inputs specified for the current .backward() or .grad() call, this tensor will be ignored and the hook will not wait for its gradient to be computed.\n\nAfter every non-ignored tensor’s gradient has been computed, fn will be called with those gradients. None will be passed for tensors that did not have their gradients computed.\n\nThe hook should not modify its arguments.\n\nThis function returns a handle with a method handle.remove() that removes the hook.\n\nNOTE\n\nSee Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.\n\nExample:\n\n>>> import torch\n>>>\n>>> a = torch.rand(2, 3, requires_grad=True)\n>>> b = torch.rand(2, 3, requires_grad=True)\n>>> c = a * b\n>>> d = a * b\n>>>\n>>> def fn(grads):\n...     print([g is not None for g in grads])\n...\n>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n>>>\n>>> c.sum().backward(retain_graph=True)\n[True, True, True, False]\n>>> c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n>>>\n\nCLASS\ntorch.autograd.graph.allow_mutation_on_saved_tensors\n[SOURCE]\n\nContext manager under which mutating tensors saved for backward is allowed\n\nUnder this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it’s used during backward.\n\nTo ensure the correct behavior, both the forward and backward should be run under the same context manager.\n\nReturns\n\nAn _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.\n\nExample:\n\n>>> import torch\n>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nAutomatic differentiation package - torch.autograd\nForward-mode Automatic Differentiation\nFunctional higher level API\nLocally disabling gradient computation\nDefault gradient layouts\nIn-place operations on Tensors\nVariable (deprecated)\nTensor autograd functions\nFunction\nContext method mixins\nNumerical gradient checking\nProfiler\nAnomaly detection\nAutograd graph\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.library — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/library.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.library\nShortcuts\nTORCH.LIBRARY\n\nPython operator registration API provides capabilities for extending PyTorch’s core library of operators with user defined operators. Currently, this can be done in two ways:\n\nCreating new libraries\n\nLets you to register new operators and kernels for various backends and functionalities by specifying the appropriate dispatch keys. For example,\n\nConsider registering a new operator add in your newly created namespace foo. You can access this operator using the torch.ops API and calling into by calling torch.ops.foo.add. You can also access specific registered overloads by calling torch.ops.foo.add.{overload_name}.\n\nIf you registered a new kernel for the CUDA dispatch key for this operator, then your custom defined function will be called for CUDA tensor inputs.\n\nThis can be done by creating Library class objects of \"DEF\" kind.\n\nExtending existing C++ libraries (e.g., aten)\n\nLets you register kernels for existing operators corresponding to various backends and functionalities by specifying the appropriate dispatch keys.\n\nThis may come in handy to fill up spotty operator support for a feature implemented through a dispatch key. For example.,\n\nYou can add operator support for Meta Tensors (by registering function to the Meta dispatch key).\n\nThis can be done by creating Library class objects of \"IMPL\" kind.\n\nA tutorial that walks you through some examples on how to use this API is available on Google Colab.\n\nWARNING\n\nDispatcher is a complicated PyTorch concept and having a sound understanding of Dispatcher is crucial to be able to do anything advanced with this API. This blog post is a good starting point to learn about Dispatcher.\n\nCLASS\ntorch.library.Library(ns, kind, dispatch_key='')\n[SOURCE]\n\nA class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.\n\nTo create a library to override operators in an existing library (with name ns), set the kind to “IMPL”. To create a new library (with name ns) to register new operators, set the kind to “DEF”. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to “FRAGMENT”.\n\nParameters\n\nns – library name\n\nkind – “DEF”, “IMPL” (default: “IMPL”), “FRAGMENT”\n\ndispatch_key – PyTorch dispatch key (default: “”)\n\ndefine(schema, alias_analysis='')\n[SOURCE]\n\nDefines a new operator and its semantics in the ns namespace.\n\nParameters\n\nschema – function schema to define a new operator.\n\nalias_analysis (optional) – Indicates if the aliasing properties of the operator arguments can be inferred from the schema (default behavior) or not (“CONSERVATIVE”).\n\nReturns\n\nname of the operator as inferred from the schema.\n\nExample::\n>>> my_lib = Library(\"foo\", \"DEF\")\n>>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n\nimpl(op_name, fn, dispatch_key='')\n[SOURCE]\n\nRegisters the function implementation for an operator defined in the library.\n\nParameters\n\nop_name – operator name (along with the overload) or OpOverload object.\n\nfn – function that’s the operator implementation for the input dispatch key or fallthrough_kernel() to register a fallthrough.\n\ndispatch_key – dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.\n\nExample::\n>>> my_lib = Library(\"aten\", \"IMPL\")\n>>> def div_cpu(self, other):\n>>>     return self * (1 / other)\n>>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n\ntorch.library.fallthrough_kernel()\n[SOURCE]\n\nA dummy function to pass to Library.impl in order to register a fallthrough.\n\nWe have also added some function decorators to make it convenient to register functions for operators:\n\ntorch.library.impl()\n\ntorch.library.define()\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.library\nLibrary\nfallthrough_kernel()\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/torch.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch\nShortcuts\nTORCH\n\nThe torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.\n\nTensors\n\nis_tensor\n\n\t\n\nReturns True if obj is a PyTorch tensor.\n\n\n\n\nis_storage\n\n\t\n\nReturns True if obj is a PyTorch storage object.\n\n\n\n\nis_complex\n\n\t\n\nReturns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.\n\n\n\n\nis_conj\n\n\t\n\nReturns True if the input is a conjugated tensor, i.e. its conjugate bit is set to True.\n\n\n\n\nis_floating_point\n\n\t\n\nReturns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.\n\n\n\n\nis_nonzero\n\n\t\n\nReturns True if the input is a single element tensor which is not equal to zero after type conversions.\n\n\n\n\nset_default_dtype\n\n\t\n\nSets the default floating point dtype to d.\n\n\n\n\nget_default_dtype\n\n\t\n\nGet the current default floating point torch.dtype.\n\n\n\n\nset_default_device\n\n\t\n\nSets the default torch.Tensor to be allocated on device.\n\n\n\n\nset_default_tensor_type\n\n\t\n\nSets the default torch.Tensor type to floating point tensor type t.\n\n\n\n\nnumel\n\n\t\n\nReturns the total number of elements in the input tensor.\n\n\n\n\nset_printoptions\n\n\t\n\nSet options for printing.\n\n\n\n\nset_flush_denormal\n\n\t\n\nDisables denormal floating numbers on CPU.\n\nCreation Ops\n\nNOTE\n\nRandom sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.\n\ntensor\n\n\t\n\nConstructs a tensor with no autograd history (also known as a \"leaf tensor\", see Autograd mechanics) by copying data.\n\n\n\n\nsparse_coo_tensor\n\n\t\n\nConstructs a sparse tensor in COO(rdinate) format with specified values at the given indices.\n\n\n\n\nsparse_csr_tensor\n\n\t\n\nConstructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices.\n\n\n\n\nsparse_csc_tensor\n\n\t\n\nConstructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given ccol_indices and row_indices.\n\n\n\n\nsparse_bsr_tensor\n\n\t\n\nConstructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given crow_indices and col_indices.\n\n\n\n\nsparse_bsc_tensor\n\n\t\n\nConstructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given ccol_indices and row_indices.\n\n\n\n\nasarray\n\n\t\n\nConverts obj to a tensor.\n\n\n\n\nas_tensor\n\n\t\n\nConverts data into a tensor, sharing data and preserving autograd history if possible.\n\n\n\n\nas_strided\n\n\t\n\nCreate a view of an existing torch.Tensor input with specified size, stride and storage_offset.\n\n\n\n\nfrom_numpy\n\n\t\n\nCreates a Tensor from a numpy.ndarray.\n\n\n\n\nfrom_dlpack\n\n\t\n\nConverts a tensor from an external library into a torch.Tensor.\n\n\n\n\nfrombuffer\n\n\t\n\nCreates a 1-dimensional Tensor from an object that implements the Python buffer protocol.\n\n\n\n\nzeros\n\n\t\n\nReturns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n\n\n\n\nzeros_like\n\n\t\n\nReturns a tensor filled with the scalar value 0, with the same size as input.\n\n\n\n\nones\n\n\t\n\nReturns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.\n\n\n\n\nones_like\n\n\t\n\nReturns a tensor filled with the scalar value 1, with the same size as input.\n\n\n\n\narange\n\n\t\n\nReturns a 1-D tensor of size \n⌈\nend\n−\nstart\nstep\n⌉\n⌈\nstep\nend−start\n\t​\n\n⌉ with values from the interval [start, end) taken with common difference step beginning from start.\n\n\n\n\nrange\n\n\t\n\nReturns a 1-D tensor of size \n⌊\nend\n−\nstart\nstep\n⌋\n+\n1\n⌊\nstep\nend−start\n\t​\n\n⌋+1 with values from start to end with step step.\n\n\n\n\nlinspace\n\n\t\n\nCreates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.\n\n\n\n\nlogspace\n\n\t\n\nCreates a one-dimensional tensor of size steps whose values are evenly spaced from \nbase\nstart\nbase\nstart\n to \nbase\nend\nbase\nend\n, inclusive, on a logarithmic scale with base base.\n\n\n\n\neye\n\n\t\n\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\n\n\n\nempty\n\n\t\n\nReturns a tensor filled with uninitialized data.\n\n\n\n\nempty_like\n\n\t\n\nReturns an uninitialized tensor with the same size as input.\n\n\n\n\nempty_strided\n\n\t\n\nCreates a tensor with the specified size and stride and filled with undefined data.\n\n\n\n\nfull\n\n\t\n\nCreates a tensor of size size filled with fill_value.\n\n\n\n\nfull_like\n\n\t\n\nReturns a tensor with the same size as input filled with fill_value.\n\n\n\n\nquantize_per_tensor\n\n\t\n\nConverts a float tensor to a quantized tensor with given scale and zero point.\n\n\n\n\nquantize_per_channel\n\n\t\n\nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.\n\n\n\n\ndequantize\n\n\t\n\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\n\n\n\ncomplex\n\n\t\n\nConstructs a complex tensor with its real part equal to real and its imaginary part equal to imag.\n\n\n\n\npolar\n\n\t\n\nConstructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.\n\n\n\n\nheaviside\n\n\t\n\nComputes the Heaviside step function for each element in input.\n\nIndexing, Slicing, Joining, Mutating Ops\n\nadjoint\n\n\t\n\nReturns a view of the tensor conjugated and with the last two dimensions transposed.\n\n\n\n\nargwhere\n\n\t\n\nReturns a tensor containing the indices of all non-zero elements of input.\n\n\n\n\ncat\n\n\t\n\nConcatenates the given sequence of seq tensors in the given dimension.\n\n\n\n\nconcat\n\n\t\n\nAlias of torch.cat().\n\n\n\n\nconcatenate\n\n\t\n\nAlias of torch.cat().\n\n\n\n\nconj\n\n\t\n\nReturns a view of input with a flipped conjugate bit.\n\n\n\n\nchunk\n\n\t\n\nAttempts to split a tensor into the specified number of chunks.\n\n\n\n\ndsplit\n\n\t\n\nSplits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.\n\n\n\n\ncolumn_stack\n\n\t\n\nCreates a new tensor by horizontally stacking the tensors in tensors.\n\n\n\n\ndstack\n\n\t\n\nStack tensors in sequence depthwise (along third axis).\n\n\n\n\ngather\n\n\t\n\nGathers values along an axis specified by dim.\n\n\n\n\nhsplit\n\n\t\n\nSplits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.\n\n\n\n\nhstack\n\n\t\n\nStack tensors in sequence horizontally (column wise).\n\n\n\n\nindex_add\n\n\t\n\nSee index_add_() for function description.\n\n\n\n\nindex_copy\n\n\t\n\nSee index_add_() for function description.\n\n\n\n\nindex_reduce\n\n\t\n\nSee index_reduce_() for function description.\n\n\n\n\nindex_select\n\n\t\n\nReturns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.\n\n\n\n\nmasked_select\n\n\t\n\nReturns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.\n\n\n\n\nmovedim\n\n\t\n\nMoves the dimension(s) of input at the position(s) in source to the position(s) in destination.\n\n\n\n\nmoveaxis\n\n\t\n\nAlias for torch.movedim().\n\n\n\n\nnarrow\n\n\t\n\nReturns a new tensor that is a narrowed version of input tensor.\n\n\n\n\nnarrow_copy\n\n\t\n\nSame as Tensor.narrow() except this returns a copy rather than shared storage.\n\n\n\n\nnonzero\n\n\t\n\n\n\n\npermute\n\n\t\n\nReturns a view of the original tensor input with its dimensions permuted.\n\n\n\n\nreshape\n\n\t\n\nReturns a tensor with the same data and number of elements as input, but with the specified shape.\n\n\n\n\nrow_stack\n\n\t\n\nAlias of torch.vstack().\n\n\n\n\nselect\n\n\t\n\nSlices the input tensor along the selected dimension at the given index.\n\n\n\n\nscatter\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_()\n\n\n\n\ndiagonal_scatter\n\n\t\n\nEmbeds the values of the src tensor into input along the diagonal elements of input, with respect to dim1 and dim2.\n\n\n\n\nselect_scatter\n\n\t\n\nEmbeds the values of the src tensor into input at the given index.\n\n\n\n\nslice_scatter\n\n\t\n\nEmbeds the values of the src tensor into input at the given dimension.\n\n\n\n\nscatter_add\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_add_()\n\n\n\n\nscatter_reduce\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_reduce_()\n\n\n\n\nsplit\n\n\t\n\nSplits the tensor into chunks.\n\n\n\n\nsqueeze\n\n\t\n\nReturns a tensor with all specified dimensions of input of size 1 removed.\n\n\n\n\nstack\n\n\t\n\nConcatenates a sequence of tensors along a new dimension.\n\n\n\n\nswapaxes\n\n\t\n\nAlias for torch.transpose().\n\n\n\n\nswapdims\n\n\t\n\nAlias for torch.transpose().\n\n\n\n\nt\n\n\t\n\nExpects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n\n\n\n\ntake\n\n\t\n\nReturns a new tensor with the elements of input at the given indices.\n\n\n\n\ntake_along_dim\n\n\t\n\nSelects values from input at the 1-dimensional indices from indices along the given dim.\n\n\n\n\ntensor_split\n\n\t\n\nSplits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.\n\n\n\n\ntile\n\n\t\n\nConstructs a tensor by repeating the elements of input.\n\n\n\n\ntranspose\n\n\t\n\nReturns a tensor that is a transposed version of input.\n\n\n\n\nunbind\n\n\t\n\nRemoves a tensor dimension.\n\n\n\n\nunsqueeze\n\n\t\n\nReturns a new tensor with a dimension of size one inserted at the specified position.\n\n\n\n\nvsplit\n\n\t\n\nSplits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.\n\n\n\n\nvstack\n\n\t\n\nStack tensors in sequence vertically (row wise).\n\n\n\n\nwhere\n\n\t\n\nReturn a tensor of elements selected from either input or other, depending on condition.\n\nGenerators\n\nGenerator\n\n\t\n\nCreates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.\n\nRandom sampling\n\nseed\n\n\t\n\nSets the seed for generating random numbers to a non-deterministic random number.\n\n\n\n\nmanual_seed\n\n\t\n\nSets the seed for generating random numbers.\n\n\n\n\ninitial_seed\n\n\t\n\nReturns the initial seed for generating random numbers as a Python long.\n\n\n\n\nget_rng_state\n\n\t\n\nReturns the random number generator state as a torch.ByteTensor.\n\n\n\n\nset_rng_state\n\n\t\n\nSets the random number generator state.\n\ntorch.default_generator Returns the default CPU torch.Generator\n\nbernoulli\n\n\t\n\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\n\n\n\nmultinomial\n\n\t\n\nReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.\n\n\n\n\nnormal\n\n\t\n\nReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.\n\n\n\n\npoisson\n\n\t\n\nReturns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,\n\n\n\n\nrand\n\n\t\n\nReturns a tensor filled with random numbers from a uniform distribution on the interval \n[\n0\n,\n1\n)\n[0,1)\n\n\n\n\nrand_like\n\n\t\n\nReturns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval \n[\n0\n,\n1\n)\n[0,1).\n\n\n\n\nrandint\n\n\t\n\nReturns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).\n\n\n\n\nrandint_like\n\n\t\n\nReturns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).\n\n\n\n\nrandn\n\n\t\n\nReturns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n\n\n\n\nrandn_like\n\n\t\n\nReturns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.\n\n\n\n\nrandperm\n\n\t\n\nReturns a random permutation of integers from 0 to n - 1.\n\nIn-place random sampling\n\nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\n\ntorch.Tensor.bernoulli_() - in-place version of torch.bernoulli()\n\ntorch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution\n\ntorch.Tensor.exponential_() - numbers drawn from the exponential distribution\n\ntorch.Tensor.geometric_() - elements drawn from the geometric distribution\n\ntorch.Tensor.log_normal_() - samples from the log-normal distribution\n\ntorch.Tensor.normal_() - in-place version of torch.normal()\n\ntorch.Tensor.random_() - numbers sampled from the discrete uniform distribution\n\ntorch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution\n\nQuasi-random sampling\n\nquasirandom.SobolEngine\n\n\t\n\nThe torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.\n\nSerialization\n\nsave\n\n\t\n\nSaves an object to a disk file.\n\n\n\n\nload\n\n\t\n\nLoads an object saved with torch.save() from a file.\n\nParallelism\n\nget_num_threads\n\n\t\n\nReturns the number of threads used for parallelizing CPU operations\n\n\n\n\nset_num_threads\n\n\t\n\nSets the number of threads used for intraop parallelism on CPU.\n\n\n\n\nget_num_interop_threads\n\n\t\n\nReturns the number of threads used for inter-op parallelism on CPU (e.g.\n\n\n\n\nset_num_interop_threads\n\n\t\n\nSets the number of threads used for interop parallelism (e.g.\n\nLocally disabling gradient computation\n\nThe context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled() are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the threading module, etc.\n\nExamples:\n\n>>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n\n\nno_grad\n\n\t\n\nContext-manager that disables gradient calculation.\n\n\n\n\nenable_grad\n\n\t\n\nContext-manager that enables gradient calculation.\n\n\n\n\nset_grad_enabled\n\n\t\n\nContext-manager that sets gradient calculation on or off.\n\n\n\n\nis_grad_enabled\n\n\t\n\nReturns True if grad mode is currently enabled.\n\n\n\n\ninference_mode\n\n\t\n\nContext-manager that enables or disables inference mode\n\n\n\n\nis_inference_mode_enabled\n\n\t\n\nReturns True if inference mode is currently enabled.\n\nMath operations\nPointwise Ops\n\nabs\n\n\t\n\nComputes the absolute value of each element in input.\n\n\n\n\nabsolute\n\n\t\n\nAlias for torch.abs()\n\n\n\n\nacos\n\n\t\n\nComputes the inverse cosine of each element in input.\n\n\n\n\narccos\n\n\t\n\nAlias for torch.acos().\n\n\n\n\nacosh\n\n\t\n\nReturns a new tensor with the inverse hyperbolic cosine of the elements of input.\n\n\n\n\narccosh\n\n\t\n\nAlias for torch.acosh().\n\n\n\n\nadd\n\n\t\n\nAdds other, scaled by alpha, to input.\n\n\n\n\naddcdiv\n\n\t\n\nPerforms the element-wise division of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.\n\n\n\n\naddcmul\n\n\t\n\nPerforms the element-wise multiplication of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.\n\n\n\n\nangle\n\n\t\n\nComputes the element-wise angle (in radians) of the given input tensor.\n\n\n\n\nasin\n\n\t\n\nReturns a new tensor with the arcsine of the elements of input.\n\n\n\n\narcsin\n\n\t\n\nAlias for torch.asin().\n\n\n\n\nasinh\n\n\t\n\nReturns a new tensor with the inverse hyperbolic sine of the elements of input.\n\n\n\n\narcsinh\n\n\t\n\nAlias for torch.asinh().\n\n\n\n\natan\n\n\t\n\nReturns a new tensor with the arctangent of the elements of input.\n\n\n\n\narctan\n\n\t\n\nAlias for torch.atan().\n\n\n\n\natanh\n\n\t\n\nReturns a new tensor with the inverse hyperbolic tangent of the elements of input.\n\n\n\n\narctanh\n\n\t\n\nAlias for torch.atanh().\n\n\n\n\natan2\n\n\t\n\nElement-wise arctangent of \ninput\n𝑖\n/\nother\n𝑖\ninput\ni\n\t​\n\n/other\ni\n\t​\n\n with consideration of the quadrant.\n\n\n\n\narctan2\n\n\t\n\nAlias for torch.atan2().\n\n\n\n\nbitwise_not\n\n\t\n\nComputes the bitwise NOT of the given input tensor.\n\n\n\n\nbitwise_and\n\n\t\n\nComputes the bitwise AND of input and other.\n\n\n\n\nbitwise_or\n\n\t\n\nComputes the bitwise OR of input and other.\n\n\n\n\nbitwise_xor\n\n\t\n\nComputes the bitwise XOR of input and other.\n\n\n\n\nbitwise_left_shift\n\n\t\n\nComputes the left arithmetic shift of input by other bits.\n\n\n\n\nbitwise_right_shift\n\n\t\n\nComputes the right arithmetic shift of input by other bits.\n\n\n\n\nceil\n\n\t\n\nReturns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.\n\n\n\n\nclamp\n\n\t\n\nClamps all elements in input into the range [ min, max ].\n\n\n\n\nclip\n\n\t\n\nAlias for torch.clamp().\n\n\n\n\nconj_physical\n\n\t\n\nComputes the element-wise conjugate of the given input tensor.\n\n\n\n\ncopysign\n\n\t\n\nCreate a new floating-point tensor with the magnitude of input and the sign of other, elementwise.\n\n\n\n\ncos\n\n\t\n\nReturns a new tensor with the cosine of the elements of input.\n\n\n\n\ncosh\n\n\t\n\nReturns a new tensor with the hyperbolic cosine of the elements of input.\n\n\n\n\ndeg2rad\n\n\t\n\nReturns a new tensor with each of the elements of input converted from angles in degrees to radians.\n\n\n\n\ndiv\n\n\t\n\nDivides each element of the input input by the corresponding element of other.\n\n\n\n\ndivide\n\n\t\n\nAlias for torch.div().\n\n\n\n\ndigamma\n\n\t\n\nAlias for torch.special.digamma().\n\n\n\n\nerf\n\n\t\n\nAlias for torch.special.erf().\n\n\n\n\nerfc\n\n\t\n\nAlias for torch.special.erfc().\n\n\n\n\nerfinv\n\n\t\n\nAlias for torch.special.erfinv().\n\n\n\n\nexp\n\n\t\n\nReturns a new tensor with the exponential of the elements of the input tensor input.\n\n\n\n\nexp2\n\n\t\n\nAlias for torch.special.exp2().\n\n\n\n\nexpm1\n\n\t\n\nAlias for torch.special.expm1().\n\n\n\n\nfake_quantize_per_channel_affine\n\n\t\n\nReturns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.\n\n\n\n\nfake_quantize_per_tensor_affine\n\n\t\n\nReturns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.\n\n\n\n\nfix\n\n\t\n\nAlias for torch.trunc()\n\n\n\n\nfloat_power\n\n\t\n\nRaises input to the power of exponent, elementwise, in double precision.\n\n\n\n\nfloor\n\n\t\n\nReturns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.\n\n\n\n\nfloor_divide\n\n\t\n\n\n\n\nfmod\n\n\t\n\nApplies C++'s std::fmod entrywise.\n\n\n\n\nfrac\n\n\t\n\nComputes the fractional portion of each element in input.\n\n\n\n\nfrexp\n\n\t\n\nDecomposes input into mantissa and exponent tensors such that \ninput\n=\nmantissa\n×\n2\nexponent\ninput=mantissa×2\nexponent\n.\n\n\n\n\ngradient\n\n\t\n\nEstimates the gradient of a function \n𝑔\n:\n𝑅\n𝑛\n→\n𝑅\ng:R\nn\n→R in one or more dimensions using the second-order accurate central differences method and either first or second order estimates at the boundaries.\n\n\n\n\nimag\n\n\t\n\nReturns a new tensor containing imaginary values of the self tensor.\n\n\n\n\nldexp\n\n\t\n\nMultiplies input by 2 ** other.\n\n\n\n\nlerp\n\n\t\n\nDoes a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.\n\n\n\n\nlgamma\n\n\t\n\nComputes the natural logarithm of the absolute value of the gamma function on input.\n\n\n\n\nlog\n\n\t\n\nReturns a new tensor with the natural logarithm of the elements of input.\n\n\n\n\nlog10\n\n\t\n\nReturns a new tensor with the logarithm to the base 10 of the elements of input.\n\n\n\n\nlog1p\n\n\t\n\nReturns a new tensor with the natural logarithm of (1 + input).\n\n\n\n\nlog2\n\n\t\n\nReturns a new tensor with the logarithm to the base 2 of the elements of input.\n\n\n\n\nlogaddexp\n\n\t\n\nLogarithm of the sum of exponentiations of the inputs.\n\n\n\n\nlogaddexp2\n\n\t\n\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\n\n\n\nlogical_and\n\n\t\n\nComputes the element-wise logical AND of the given input tensors.\n\n\n\n\nlogical_not\n\n\t\n\nComputes the element-wise logical NOT of the given input tensor.\n\n\n\n\nlogical_or\n\n\t\n\nComputes the element-wise logical OR of the given input tensors.\n\n\n\n\nlogical_xor\n\n\t\n\nComputes the element-wise logical XOR of the given input tensors.\n\n\n\n\nlogit\n\n\t\n\nAlias for torch.special.logit().\n\n\n\n\nhypot\n\n\t\n\nGiven the legs of a right triangle, return its hypotenuse.\n\n\n\n\ni0\n\n\t\n\nAlias for torch.special.i0().\n\n\n\n\nigamma\n\n\t\n\nAlias for torch.special.gammainc().\n\n\n\n\nigammac\n\n\t\n\nAlias for torch.special.gammaincc().\n\n\n\n\nmul\n\n\t\n\nMultiplies input by other.\n\n\n\n\nmultiply\n\n\t\n\nAlias for torch.mul().\n\n\n\n\nmvlgamma\n\n\t\n\nAlias for torch.special.multigammaln().\n\n\n\n\nnan_to_num\n\n\t\n\nReplaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively.\n\n\n\n\nneg\n\n\t\n\nReturns a new tensor with the negative of the elements of input.\n\n\n\n\nnegative\n\n\t\n\nAlias for torch.neg()\n\n\n\n\nnextafter\n\n\t\n\nReturn the next floating-point value after input towards other, elementwise.\n\n\n\n\npolygamma\n\n\t\n\nAlias for torch.special.polygamma().\n\n\n\n\npositive\n\n\t\n\nReturns input.\n\n\n\n\npow\n\n\t\n\nTakes the power of each element in input with exponent and returns a tensor with the result.\n\n\n\n\nquantized_batch_norm\n\n\t\n\nApplies batch normalization on a 4D (NCHW) quantized tensor.\n\n\n\n\nquantized_max_pool1d\n\n\t\n\nApplies a 1D max pooling over an input quantized tensor composed of several input planes.\n\n\n\n\nquantized_max_pool2d\n\n\t\n\nApplies a 2D max pooling over an input quantized tensor composed of several input planes.\n\n\n\n\nrad2deg\n\n\t\n\nReturns a new tensor with each of the elements of input converted from angles in radians to degrees.\n\n\n\n\nreal\n\n\t\n\nReturns a new tensor containing real values of the self tensor.\n\n\n\n\nreciprocal\n\n\t\n\nReturns a new tensor with the reciprocal of the elements of input\n\n\n\n\nremainder\n\n\t\n\nComputes Python's modulus operation entrywise.\n\n\n\n\nround\n\n\t\n\nRounds elements of input to the nearest integer.\n\n\n\n\nrsqrt\n\n\t\n\nReturns a new tensor with the reciprocal of the square-root of each of the elements of input.\n\n\n\n\nsigmoid\n\n\t\n\nAlias for torch.special.expit().\n\n\n\n\nsign\n\n\t\n\nReturns a new tensor with the signs of the elements of input.\n\n\n\n\nsgn\n\n\t\n\nThis function is an extension of torch.sign() to complex tensors.\n\n\n\n\nsignbit\n\n\t\n\nTests if each element of input has its sign bit set or not.\n\n\n\n\nsin\n\n\t\n\nReturns a new tensor with the sine of the elements of input.\n\n\n\n\nsinc\n\n\t\n\nAlias for torch.special.sinc().\n\n\n\n\nsinh\n\n\t\n\nReturns a new tensor with the hyperbolic sine of the elements of input.\n\n\n\n\nsoftmax\n\n\t\n\nAlias for torch.nn.functional.softmax().\n\n\n\n\nsqrt\n\n\t\n\nReturns a new tensor with the square-root of the elements of input.\n\n\n\n\nsquare\n\n\t\n\nReturns a new tensor with the square of the elements of input.\n\n\n\n\nsub\n\n\t\n\nSubtracts other, scaled by alpha, from input.\n\n\n\n\nsubtract\n\n\t\n\nAlias for torch.sub().\n\n\n\n\ntan\n\n\t\n\nReturns a new tensor with the tangent of the elements of input.\n\n\n\n\ntanh\n\n\t\n\nReturns a new tensor with the hyperbolic tangent of the elements of input.\n\n\n\n\ntrue_divide\n\n\t\n\nAlias for torch.div() with rounding_mode=None.\n\n\n\n\ntrunc\n\n\t\n\nReturns a new tensor with the truncated integer values of the elements of input.\n\n\n\n\nxlogy\n\n\t\n\nAlias for torch.special.xlogy().\n\nReduction Ops\n\nargmax\n\n\t\n\nReturns the indices of the maximum value of all elements in the input tensor.\n\n\n\n\nargmin\n\n\t\n\nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension\n\n\n\n\namax\n\n\t\n\nReturns the maximum value of each slice of the input tensor in the given dimension(s) dim.\n\n\n\n\namin\n\n\t\n\nReturns the minimum value of each slice of the input tensor in the given dimension(s) dim.\n\n\n\n\naminmax\n\n\t\n\nComputes the minimum and maximum values of the input tensor.\n\n\n\n\nall\n\n\t\n\nTests if all elements in input evaluate to True.\n\n\n\n\nany\n\n\t\n\nTests if any element in input evaluates to True.\n\n\n\n\nmax\n\n\t\n\nReturns the maximum value of all elements in the input tensor.\n\n\n\n\nmin\n\n\t\n\nReturns the minimum value of all elements in the input tensor.\n\n\n\n\ndist\n\n\t\n\nReturns the p-norm of (input - other)\n\n\n\n\nlogsumexp\n\n\t\n\nReturns the log of summed exponentials of each row of the input tensor in the given dimension dim.\n\n\n\n\nmean\n\n\t\n\nReturns the mean value of all elements in the input tensor.\n\n\n\n\nnanmean\n\n\t\n\nComputes the mean of all non-NaN elements along the specified dimensions.\n\n\n\n\nmedian\n\n\t\n\nReturns the median of the values in input.\n\n\n\n\nnanmedian\n\n\t\n\nReturns the median of the values in input, ignoring NaN values.\n\n\n\n\nmode\n\n\t\n\nReturns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often in that row, and indices is the index location of each mode value found.\n\n\n\n\nnorm\n\n\t\n\nReturns the matrix norm or vector norm of a given tensor.\n\n\n\n\nnansum\n\n\t\n\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\n\n\n\nprod\n\n\t\n\nReturns the product of all elements in the input tensor.\n\n\n\n\nquantile\n\n\t\n\nComputes the q-th quantiles of each row of the input tensor along the dimension dim.\n\n\n\n\nnanquantile\n\n\t\n\nThis is a variant of torch.quantile() that \"ignores\" NaN values, computing the quantiles q as if NaN values in input did not exist.\n\n\n\n\nstd\n\n\t\n\nCalculates the standard deviation over the dimensions specified by dim.\n\n\n\n\nstd_mean\n\n\t\n\nCalculates the standard deviation and mean over the dimensions specified by dim.\n\n\n\n\nsum\n\n\t\n\nReturns the sum of all elements in the input tensor.\n\n\n\n\nunique\n\n\t\n\nReturns the unique elements of the input tensor.\n\n\n\n\nunique_consecutive\n\n\t\n\nEliminates all but the first element from every consecutive group of equivalent elements.\n\n\n\n\nvar\n\n\t\n\nCalculates the variance over the dimensions specified by dim.\n\n\n\n\nvar_mean\n\n\t\n\nCalculates the variance and mean over the dimensions specified by dim.\n\n\n\n\ncount_nonzero\n\n\t\n\nCounts the number of non-zero values in the tensor input along the given dim.\n\nComparison Ops\n\nallclose\n\n\t\n\nThis function checks if input and other satisfy the condition:\n\n\n\n\nargsort\n\n\t\n\nReturns the indices that sort a tensor along a given dimension in ascending order by value.\n\n\n\n\neq\n\n\t\n\nComputes element-wise equality\n\n\n\n\nequal\n\n\t\n\nTrue if two tensors have the same size and elements, False otherwise.\n\n\n\n\nge\n\n\t\n\nComputes \ninput\n≥\nother\ninput≥other element-wise.\n\n\n\n\ngreater_equal\n\n\t\n\nAlias for torch.ge().\n\n\n\n\ngt\n\n\t\n\nComputes \ninput\n>\nother\ninput>other element-wise.\n\n\n\n\ngreater\n\n\t\n\nAlias for torch.gt().\n\n\n\n\nisclose\n\n\t\n\nReturns a new tensor with boolean elements representing if each element of input is \"close\" to the corresponding element of other.\n\n\n\n\nisfinite\n\n\t\n\nReturns a new tensor with boolean elements representing if each element is finite or not.\n\n\n\n\nisin\n\n\t\n\nTests if each element of elements is in test_elements.\n\n\n\n\nisinf\n\n\t\n\nTests if each element of input is infinite (positive or negative infinity) or not.\n\n\n\n\nisposinf\n\n\t\n\nTests if each element of input is positive infinity or not.\n\n\n\n\nisneginf\n\n\t\n\nTests if each element of input is negative infinity or not.\n\n\n\n\nisnan\n\n\t\n\nReturns a new tensor with boolean elements representing if each element of input is NaN or not.\n\n\n\n\nisreal\n\n\t\n\nReturns a new tensor with boolean elements representing if each element of input is real-valued or not.\n\n\n\n\nkthvalue\n\n\t\n\nReturns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.\n\n\n\n\nle\n\n\t\n\nComputes \ninput\n≤\nother\ninput≤other element-wise.\n\n\n\n\nless_equal\n\n\t\n\nAlias for torch.le().\n\n\n\n\nlt\n\n\t\n\nComputes \ninput\n<\nother\ninput<other element-wise.\n\n\n\n\nless\n\n\t\n\nAlias for torch.lt().\n\n\n\n\nmaximum\n\n\t\n\nComputes the element-wise maximum of input and other.\n\n\n\n\nminimum\n\n\t\n\nComputes the element-wise minimum of input and other.\n\n\n\n\nfmax\n\n\t\n\nComputes the element-wise maximum of input and other.\n\n\n\n\nfmin\n\n\t\n\nComputes the element-wise minimum of input and other.\n\n\n\n\nne\n\n\t\n\nComputes \ninput\n≠\nother\ninput\n\n=other element-wise.\n\n\n\n\nnot_equal\n\n\t\n\nAlias for torch.ne().\n\n\n\n\nsort\n\n\t\n\nSorts the elements of the input tensor along a given dimension in ascending order by value.\n\n\n\n\ntopk\n\n\t\n\nReturns the k largest elements of the given input tensor along a given dimension.\n\n\n\n\nmsort\n\n\t\n\nSorts the elements of the input tensor along its first dimension in ascending order by value.\n\nSpectral Ops\n\nstft\n\n\t\n\nShort-time Fourier transform (STFT).\n\n\n\n\nistft\n\n\t\n\nInverse short time Fourier Transform.\n\n\n\n\nbartlett_window\n\n\t\n\nBartlett window function.\n\n\n\n\nblackman_window\n\n\t\n\nBlackman window function.\n\n\n\n\nhamming_window\n\n\t\n\nHamming window function.\n\n\n\n\nhann_window\n\n\t\n\nHann window function.\n\n\n\n\nkaiser_window\n\n\t\n\nComputes the Kaiser window with window length window_length and shape parameter beta.\n\nOther Operations\n\natleast_1d\n\n\t\n\nReturns a 1-dimensional view of each input tensor with zero dimensions.\n\n\n\n\natleast_2d\n\n\t\n\nReturns a 2-dimensional view of each input tensor with zero dimensions.\n\n\n\n\natleast_3d\n\n\t\n\nReturns a 3-dimensional view of each input tensor with zero dimensions.\n\n\n\n\nbincount\n\n\t\n\nCount the frequency of each value in an array of non-negative ints.\n\n\n\n\nblock_diag\n\n\t\n\nCreate a block diagonal matrix from provided tensors.\n\n\n\n\nbroadcast_tensors\n\n\t\n\nBroadcasts the given tensors according to Broadcasting semantics.\n\n\n\n\nbroadcast_to\n\n\t\n\nBroadcasts input to the shape shape.\n\n\n\n\nbroadcast_shapes\n\n\t\n\nSimilar to broadcast_tensors() but for shapes.\n\n\n\n\nbucketize\n\n\t\n\nReturns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.\n\n\n\n\ncartesian_prod\n\n\t\n\nDo cartesian product of the given sequence of tensors.\n\n\n\n\ncdist\n\n\t\n\nComputes batched the p-norm distance between each pair of the two collections of row vectors.\n\n\n\n\nclone\n\n\t\n\nReturns a copy of input.\n\n\n\n\ncombinations\n\n\t\n\nCompute combinations of length \n𝑟\nr of the given tensor.\n\n\n\n\ncorrcoef\n\n\t\n\nEstimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.\n\n\n\n\ncov\n\n\t\n\nEstimates the covariance matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.\n\n\n\n\ncross\n\n\t\n\nReturns the cross product of vectors in dimension dim of input and other.\n\n\n\n\ncummax\n\n\t\n\nReturns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.\n\n\n\n\ncummin\n\n\t\n\nReturns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.\n\n\n\n\ncumprod\n\n\t\n\nReturns the cumulative product of elements of input in the dimension dim.\n\n\n\n\ncumsum\n\n\t\n\nReturns the cumulative sum of elements of input in the dimension dim.\n\n\n\n\ndiag\n\n\t\n\nIf input is a vector (1-D tensor), then returns a 2-D square tensor\n\n\n\n\ndiag_embed\n\n\t\n\nCreates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.\n\n\n\n\ndiagflat\n\n\t\n\nIf input is a vector (1-D tensor), then returns a 2-D square tensor\n\n\n\n\ndiagonal\n\n\t\n\nReturns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.\n\n\n\n\ndiff\n\n\t\n\nComputes the n-th forward difference along the given dimension.\n\n\n\n\neinsum\n\n\t\n\nSums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.\n\n\n\n\nflatten\n\n\t\n\nFlattens input by reshaping it into a one-dimensional tensor.\n\n\n\n\nflip\n\n\t\n\nReverse the order of an n-D tensor along given axis in dims.\n\n\n\n\nfliplr\n\n\t\n\nFlip tensor in the left/right direction, returning a new tensor.\n\n\n\n\nflipud\n\n\t\n\nFlip tensor in the up/down direction, returning a new tensor.\n\n\n\n\nkron\n\n\t\n\nComputes the Kronecker product, denoted by \n⊗\n⊗, of input and other.\n\n\n\n\nrot90\n\n\t\n\nRotate an n-D tensor by 90 degrees in the plane specified by dims axis.\n\n\n\n\ngcd\n\n\t\n\nComputes the element-wise greatest common divisor (GCD) of input and other.\n\n\n\n\nhistc\n\n\t\n\nComputes the histogram of a tensor.\n\n\n\n\nhistogram\n\n\t\n\nComputes a histogram of the values in a tensor.\n\n\n\n\nhistogramdd\n\n\t\n\nComputes a multi-dimensional histogram of the values in a tensor.\n\n\n\n\nmeshgrid\n\n\t\n\nCreates grids of coordinates specified by the 1D inputs in attr:tensors.\n\n\n\n\nlcm\n\n\t\n\nComputes the element-wise least common multiple (LCM) of input and other.\n\n\n\n\nlogcumsumexp\n\n\t\n\nReturns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.\n\n\n\n\nravel\n\n\t\n\nReturn a contiguous flattened tensor.\n\n\n\n\nrenorm\n\n\t\n\nReturns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm\n\n\n\n\nrepeat_interleave\n\n\t\n\nRepeat elements of a tensor.\n\n\n\n\nroll\n\n\t\n\nRoll the tensor input along the given dimension(s).\n\n\n\n\nsearchsorted\n\n\t\n\nFind the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, when sorted, the order of the corresponding innermost dimension within sorted_sequence would be preserved.\n\n\n\n\ntensordot\n\n\t\n\nReturns a contraction of a and b over multiple dimensions.\n\n\n\n\ntrace\n\n\t\n\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\n\n\n\ntril\n\n\t\n\nReturns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.\n\n\n\n\ntril_indices\n\n\t\n\nReturns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\n\n\n\n\ntriu\n\n\t\n\nReturns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.\n\n\n\n\ntriu_indices\n\n\t\n\nReturns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\n\n\n\n\nunflatten\n\n\t\n\nExpands a dimension of the input tensor over multiple dimensions.\n\n\n\n\nvander\n\n\t\n\nGenerates a Vandermonde matrix.\n\n\n\n\nview_as_real\n\n\t\n\nReturns a view of input as a real tensor.\n\n\n\n\nview_as_complex\n\n\t\n\nReturns a view of input as a complex tensor.\n\n\n\n\nresolve_conj\n\n\t\n\nReturns a new tensor with materialized conjugation if input's conjugate bit is set to True, else returns input.\n\n\n\n\nresolve_neg\n\n\t\n\nReturns a new tensor with materialized negation if input's negative bit is set to True, else returns input.\n\nBLAS and LAPACK Operations\n\naddbmm\n\n\t\n\nPerforms a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).\n\n\n\n\naddmm\n\n\t\n\nPerforms a matrix multiplication of the matrices mat1 and mat2.\n\n\n\n\naddmv\n\n\t\n\nPerforms a matrix-vector product of the matrix mat and the vector vec.\n\n\n\n\naddr\n\n\t\n\nPerforms the outer-product of vectors vec1 and vec2 and adds it to the matrix input.\n\n\n\n\nbaddbmm\n\n\t\n\nPerforms a batch matrix-matrix product of matrices in batch1 and batch2.\n\n\n\n\nbmm\n\n\t\n\nPerforms a batch matrix-matrix product of matrices stored in input and mat2.\n\n\n\n\nchain_matmul\n\n\t\n\nReturns the matrix product of the \n𝑁\nN 2-D tensors.\n\n\n\n\ncholesky\n\n\t\n\nComputes the Cholesky decomposition of a symmetric positive-definite matrix \n𝐴\nA or for batches of symmetric positive-definite matrices.\n\n\n\n\ncholesky_inverse\n\n\t\n\nComputes the inverse of a symmetric positive-definite matrix \n𝐴\nA using its Cholesky factor \n𝑢\nu: returns matrix inv.\n\n\n\n\ncholesky_solve\n\n\t\n\nSolves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix \n𝑢\nu.\n\n\n\n\ndot\n\n\t\n\nComputes the dot product of two 1D tensors.\n\n\n\n\ngeqrf\n\n\t\n\nThis is a low-level function for calling LAPACK's geqrf directly.\n\n\n\n\nger\n\n\t\n\nAlias of torch.outer().\n\n\n\n\ninner\n\n\t\n\nComputes the dot product for 1D tensors.\n\n\n\n\ninverse\n\n\t\n\nAlias for torch.linalg.inv()\n\n\n\n\ndet\n\n\t\n\nAlias for torch.linalg.det()\n\n\n\n\nlogdet\n\n\t\n\nCalculates log determinant of a square matrix or batches of square matrices.\n\n\n\n\nslogdet\n\n\t\n\nAlias for torch.linalg.slogdet()\n\n\n\n\nlu\n\n\t\n\nComputes the LU factorization of a matrix or batches of matrices A.\n\n\n\n\nlu_solve\n\n\t\n\nReturns the LU solve of the linear system \n𝐴\n𝑥\n=\n𝑏\nAx=b using the partially pivoted LU factorization of A from lu_factor().\n\n\n\n\nlu_unpack\n\n\t\n\nUnpacks the LU decomposition returned by lu_factor() into the P, L, U matrices.\n\n\n\n\nmatmul\n\n\t\n\nMatrix product of two tensors.\n\n\n\n\nmatrix_power\n\n\t\n\nAlias for torch.linalg.matrix_power()\n\n\n\n\nmatrix_exp\n\n\t\n\nAlias for torch.linalg.matrix_exp().\n\n\n\n\nmm\n\n\t\n\nPerforms a matrix multiplication of the matrices input and mat2.\n\n\n\n\nmv\n\n\t\n\nPerforms a matrix-vector product of the matrix input and the vector vec.\n\n\n\n\norgqr\n\n\t\n\nAlias for torch.linalg.householder_product().\n\n\n\n\normqr\n\n\t\n\nComputes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.\n\n\n\n\nouter\n\n\t\n\nOuter product of input and vec2.\n\n\n\n\npinverse\n\n\t\n\nAlias for torch.linalg.pinv()\n\n\n\n\nqr\n\n\t\n\nComputes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that \ninput\n=\n𝑄\n𝑅\ninput=QR with \n𝑄\nQ being an orthogonal matrix or batch of orthogonal matrices and \n𝑅\nR being an upper triangular matrix or batch of upper triangular matrices.\n\n\n\n\nsvd\n\n\t\n\nComputes the singular value decomposition of either a matrix or batch of matrices input.\n\n\n\n\nsvd_lowrank\n\n\t\n\nReturn the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix \n𝐴\nA such that \n𝐴\n≈\n𝑈\n𝑑\n𝑖\n𝑎\n𝑔\n(\n𝑆\n)\n𝑉\n𝑇\nA≈Udiag(S)V\nT\n.\n\n\n\n\npca_lowrank\n\n\t\n\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.\n\n\n\n\nlobpcg\n\n\t\n\nFind the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.\n\n\n\n\ntrapz\n\n\t\n\nAlias for torch.trapezoid().\n\n\n\n\ntrapezoid\n\n\t\n\nComputes the trapezoidal rule along dim.\n\n\n\n\ncumulative_trapezoid\n\n\t\n\nCumulatively computes the trapezoidal rule along dim.\n\n\n\n\ntriangular_solve\n\n\t\n\nSolves a system of equations with a square upper or lower triangular invertible matrix \n𝐴\nA and multiple right-hand sides \n𝑏\nb.\n\n\n\n\nvdot\n\n\t\n\nComputes the dot product of two 1D vectors along a dimension.\n\nForeach Operations\n\nWARNING\n\nThis API is in beta and subject to future changes. Forward-mode AD is not supported.\n\n_foreach_abs\n\n\t\n\nApply torch.abs() to each Tensor of the input list.\n\n\n\n\n_foreach_abs_\n\n\t\n\nApply torch.abs() to each Tensor of the input list.\n\n\n\n\n_foreach_acos\n\n\t\n\nApply torch.acos() to each Tensor of the input list.\n\n\n\n\n_foreach_acos_\n\n\t\n\nApply torch.acos() to each Tensor of the input list.\n\n\n\n\n_foreach_asin\n\n\t\n\nApply torch.asin() to each Tensor of the input list.\n\n\n\n\n_foreach_asin_\n\n\t\n\nApply torch.asin() to each Tensor of the input list.\n\n\n\n\n_foreach_atan\n\n\t\n\nApply torch.atan() to each Tensor of the input list.\n\n\n\n\n_foreach_atan_\n\n\t\n\nApply torch.atan() to each Tensor of the input list.\n\n\n\n\n_foreach_ceil\n\n\t\n\nApply torch.ceil() to each Tensor of the input list.\n\n\n\n\n_foreach_ceil_\n\n\t\n\nApply torch.ceil() to each Tensor of the input list.\n\n\n\n\n_foreach_cos\n\n\t\n\nApply torch.cos() to each Tensor of the input list.\n\n\n\n\n_foreach_cos_\n\n\t\n\nApply torch.cos() to each Tensor of the input list.\n\n\n\n\n_foreach_cosh\n\n\t\n\nApply torch.cosh() to each Tensor of the input list.\n\n\n\n\n_foreach_cosh_\n\n\t\n\nApply torch.cosh() to each Tensor of the input list.\n\n\n\n\n_foreach_erf\n\n\t\n\nApply torch.erf() to each Tensor of the input list.\n\n\n\n\n_foreach_erf_\n\n\t\n\nApply torch.erf() to each Tensor of the input list.\n\n\n\n\n_foreach_erfc\n\n\t\n\nApply torch.erfc() to each Tensor of the input list.\n\n\n\n\n_foreach_erfc_\n\n\t\n\nApply torch.erfc() to each Tensor of the input list.\n\n\n\n\n_foreach_exp\n\n\t\n\nApply torch.exp() to each Tensor of the input list.\n\n\n\n\n_foreach_exp_\n\n\t\n\nApply torch.exp() to each Tensor of the input list.\n\n\n\n\n_foreach_expm1\n\n\t\n\nApply torch.expm1() to each Tensor of the input list.\n\n\n\n\n_foreach_expm1_\n\n\t\n\nApply torch.expm1() to each Tensor of the input list.\n\n\n\n\n_foreach_floor\n\n\t\n\nApply torch.floor() to each Tensor of the input list.\n\n\n\n\n_foreach_floor_\n\n\t\n\nApply torch.floor() to each Tensor of the input list.\n\n\n\n\n_foreach_log\n\n\t\n\nApply torch.log() to each Tensor of the input list.\n\n\n\n\n_foreach_log_\n\n\t\n\nApply torch.log() to each Tensor of the input list.\n\n\n\n\n_foreach_log10\n\n\t\n\nApply torch.log10() to each Tensor of the input list.\n\n\n\n\n_foreach_log10_\n\n\t\n\nApply torch.log10() to each Tensor of the input list.\n\n\n\n\n_foreach_log1p\n\n\t\n\nApply torch.log1p() to each Tensor of the input list.\n\n\n\n\n_foreach_log1p_\n\n\t\n\nApply torch.log1p() to each Tensor of the input list.\n\n\n\n\n_foreach_log2\n\n\t\n\nApply torch.log2() to each Tensor of the input list.\n\n\n\n\n_foreach_log2_\n\n\t\n\nApply torch.log2() to each Tensor of the input list.\n\n\n\n\n_foreach_neg\n\n\t\n\nApply torch.neg() to each Tensor of the input list.\n\n\n\n\n_foreach_neg_\n\n\t\n\nApply torch.neg() to each Tensor of the input list.\n\n\n\n\n_foreach_tan\n\n\t\n\nApply torch.tan() to each Tensor of the input list.\n\n\n\n\n_foreach_tan_\n\n\t\n\nApply torch.tan() to each Tensor of the input list.\n\n\n\n\n_foreach_sin\n\n\t\n\nApply torch.sin() to each Tensor of the input list.\n\n\n\n\n_foreach_sin_\n\n\t\n\nApply torch.sin() to each Tensor of the input list.\n\n\n\n\n_foreach_sinh\n\n\t\n\nApply torch.sinh() to each Tensor of the input list.\n\n\n\n\n_foreach_sinh_\n\n\t\n\nApply torch.sinh() to each Tensor of the input list.\n\n\n\n\n_foreach_round\n\n\t\n\nApply torch.round() to each Tensor of the input list.\n\n\n\n\n_foreach_round_\n\n\t\n\nApply torch.round() to each Tensor of the input list.\n\n\n\n\n_foreach_sqrt\n\n\t\n\nApply torch.sqrt() to each Tensor of the input list.\n\n\n\n\n_foreach_sqrt_\n\n\t\n\nApply torch.sqrt() to each Tensor of the input list.\n\n\n\n\n_foreach_lgamma\n\n\t\n\nApply torch.lgamma() to each Tensor of the input list.\n\n\n\n\n_foreach_lgamma_\n\n\t\n\nApply torch.lgamma() to each Tensor of the input list.\n\n\n\n\n_foreach_frac\n\n\t\n\nApply torch.frac() to each Tensor of the input list.\n\n\n\n\n_foreach_frac_\n\n\t\n\nApply torch.frac() to each Tensor of the input list.\n\n\n\n\n_foreach_reciprocal\n\n\t\n\nApply torch.reciprocal() to each Tensor of the input list.\n\n\n\n\n_foreach_reciprocal_\n\n\t\n\nApply torch.reciprocal() to each Tensor of the input list.\n\n\n\n\n_foreach_sigmoid\n\n\t\n\nApply torch.sigmoid() to each Tensor of the input list.\n\n\n\n\n_foreach_sigmoid_\n\n\t\n\nApply torch.sigmoid() to each Tensor of the input list.\n\n\n\n\n_foreach_trunc\n\n\t\n\nApply torch.trunc() to each Tensor of the input list.\n\n\n\n\n_foreach_trunc_\n\n\t\n\nApply torch.trunc() to each Tensor of the input list.\n\n\n\n\n_foreach_zero_\n\n\t\n\nApply torch.zero() to each Tensor of the input list.\n\nUtilities\n\ncompiled_with_cxx11_abi\n\n\t\n\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\n\n\n\nresult_type\n\n\t\n\nReturns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.\n\n\n\n\ncan_cast\n\n\t\n\nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.\n\n\n\n\npromote_types\n\n\t\n\nReturns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.\n\n\n\n\nuse_deterministic_algorithms\n\n\t\n\nSets whether PyTorch operations must use \"deterministic\" algorithms.\n\n\n\n\nare_deterministic_algorithms_enabled\n\n\t\n\nReturns True if the global deterministic flag is turned on.\n\n\n\n\nis_deterministic_algorithms_warn_only_enabled\n\n\t\n\nReturns True if the global deterministic flag is set to warn only.\n\n\n\n\nset_deterministic_debug_mode\n\n\t\n\nSets the debug mode for deterministic operations.\n\n\n\n\nget_deterministic_debug_mode\n\n\t\n\nReturns the current value of the debug mode for deterministic operations.\n\n\n\n\nset_float32_matmul_precision\n\n\t\n\nSets the internal precision of float32 matrix multiplications.\n\n\n\n\nget_float32_matmul_precision\n\n\t\n\nReturns the current value of float32 matrix multiplication precision.\n\n\n\n\nset_warn_always\n\n\t\n\nWhen this flag is False (default) then some PyTorch warnings may only appear once per process.\n\n\n\n\nis_warn_always_enabled\n\n\t\n\nReturns True if the global warn_always flag is turned on.\n\n\n\n\nvmap\n\n\t\n\nvmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs.\n\n\n\n\n_assert\n\n\t\n\nA wrapper around Python's assert which is symbolically traceable.\n\nSymbolic Numbers\nCLASS\ntorch.SymInt(node)\n[SOURCE]\n\nLike an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.\n\nCLASS\ntorch.SymFloat(node)\n[SOURCE]\n\nLike an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.\n\nCLASS\ntorch.SymBool(node)\n[SOURCE]\n\nLike an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.\n\nUnlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this.\n\nsym_float\n\n\t\n\nSymInt-aware utility for float casting.\n\n\n\n\nsym_int\n\n\t\n\nSymInt-aware utility for int casting.\n\n\n\n\nsym_max\n\n\t\n\nSymInt-aware utility for max().\n\n\n\n\nsym_min\n\n\t\n\nSymInt-aware utility for max().\n\n\n\n\nsym_not\n\n\t\n\nSymInt-aware utility for logical negation.\n\nExport Path\n\nWARNING\n\nThis feature is a prototype and may have compatibility breaking changes in the future.\n\nexport generated/exportdb/index\n\nOptimizations\n\ncompile\n\n\t\n\nOptimizes given model/function using TorchDynamo and specified backend.\n\ntorch.compile documentation\n\nOperator Tags\nCLASS\ntorch.Tag\n\nMembers:\n\ncore\n\ndata_dependent_output\n\ndynamic_output_shape\n\ngenerated\n\ninplace_view\n\nnondeterministic_bitwise\n\nnondeterministic_seeded\n\npointwise\n\nview_copy\n\nPROPERTY name\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch\nTensors\nGenerators\nRandom sampling\nSerialization\nParallelism\nLocally disabling gradient computation\nMath operations\nUtilities\nSymbolic Numbers\nExport Path\nOptimizations\nOperator Tags\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.Tensor — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/tensors.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.Tensor\nShortcuts\nTORCH.TENSOR\n\nA torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n\nData types\n\nTorch defines 10 tensor types with CPU and GPU variants which are as follows:\n\nData type\n\n\t\n\ndtype\n\n\t\n\nCPU tensor\n\n\t\n\nGPU tensor\n\n\n\n\n32-bit floating point\n\n\t\n\ntorch.float32 or torch.float\n\n\t\n\ntorch.FloatTensor\n\n\t\n\ntorch.cuda.FloatTensor\n\n\n\n\n64-bit floating point\n\n\t\n\ntorch.float64 or torch.double\n\n\t\n\ntorch.DoubleTensor\n\n\t\n\ntorch.cuda.DoubleTensor\n\n\n\n\n16-bit floating point 1\n\n\t\n\ntorch.float16 or torch.half\n\n\t\n\ntorch.HalfTensor\n\n\t\n\ntorch.cuda.HalfTensor\n\n\n\n\n16-bit floating point 2\n\n\t\n\ntorch.bfloat16\n\n\t\n\ntorch.BFloat16Tensor\n\n\t\n\ntorch.cuda.BFloat16Tensor\n\n\n\n\n32-bit complex\n\n\t\n\ntorch.complex32 or torch.chalf\n\n\t\t\n\n\n64-bit complex\n\n\t\n\ntorch.complex64 or torch.cfloat\n\n\t\t\n\n\n128-bit complex\n\n\t\n\ntorch.complex128 or torch.cdouble\n\n\t\t\n\n\n8-bit integer (unsigned)\n\n\t\n\ntorch.uint8\n\n\t\n\ntorch.ByteTensor\n\n\t\n\ntorch.cuda.ByteTensor\n\n\n\n\n8-bit integer (signed)\n\n\t\n\ntorch.int8\n\n\t\n\ntorch.CharTensor\n\n\t\n\ntorch.cuda.CharTensor\n\n\n\n\n16-bit integer (signed)\n\n\t\n\ntorch.int16 or torch.short\n\n\t\n\ntorch.ShortTensor\n\n\t\n\ntorch.cuda.ShortTensor\n\n\n\n\n32-bit integer (signed)\n\n\t\n\ntorch.int32 or torch.int\n\n\t\n\ntorch.IntTensor\n\n\t\n\ntorch.cuda.IntTensor\n\n\n\n\n64-bit integer (signed)\n\n\t\n\ntorch.int64 or torch.long\n\n\t\n\ntorch.LongTensor\n\n\t\n\ntorch.cuda.LongTensor\n\n\n\n\nBoolean\n\n\t\n\ntorch.bool\n\n\t\n\ntorch.BoolTensor\n\n\t\n\ntorch.cuda.BoolTensor\n\n\n\n\nquantized 8-bit integer (unsigned)\n\n\t\n\ntorch.quint8\n\n\t\n\ntorch.ByteTensor\n\n\t\n\n/\n\n\n\n\nquantized 8-bit integer (signed)\n\n\t\n\ntorch.qint8\n\n\t\n\ntorch.CharTensor\n\n\t\n\n/\n\n\n\n\nquantized 32-bit integer (signed)\n\n\t\n\ntorch.qint32\n\n\t\n\ntorch.IntTensor\n\n\t\n\n/\n\n\n\n\nquantized 4-bit integer (unsigned) 3\n\n\t\n\ntorch.quint4x2\n\n\t\n\ntorch.ByteTensor\n\n\t\n\n/\n\n1\n\nSometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.\n\n2\n\nSometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as float32\n\n3\n\nquantized 4-bit integer is stored as a 8-bit signed integer. Currently it’s only supported in EmbeddingBag operator.\n\ntorch.Tensor is an alias for the default tensor type (torch.FloatTensor).\n\nInitializing and basic operations\n\nA tensor can be constructed from a Python list or sequence using the torch.tensor() constructor:\n\n>>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n\n\nWARNING\n\ntorch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach() to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().\n\nA tensor of specific data type can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor creation op:\n\n>>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n\n\nFor more information about building Tensors, see Creation Ops\n\nThe contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:\n\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n\n\nUse torch.Tensor.item() to get a Python number from a tensor containing a single value:\n\n>>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n\n\nFor more information about indexing, see Indexing, Slicing, Joining, Mutating Ops\n\nA tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.\n\n>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n\n\nEach tensor has an associated torch.Storage, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.\n\nNOTE\n\nFor more information on tensor views, see Tensor Views.\n\nNOTE\n\nFor more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor, see Tensor Attributes.\n\nNOTE\n\nMethods which mutate a tensor are marked with an underscore suffix. For example, torch.FloatTensor.abs_() computes the absolute value in-place and returns the modified tensor, while torch.FloatTensor.abs() computes the result in a new tensor.\n\nNOTE\n\nTo change an existing tensor’s torch.device and/or torch.dtype, consider using to() method on the tensor.\n\nWARNING\n\nCurrent implementation of torch.Tensor introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.\n\nTensor class reference\nCLASS\ntorch.Tensor\n\nThere are a few main ways to create a tensor, depending on your use case.\n\nTo create a tensor with pre-existing data, use torch.tensor().\n\nTo create a tensor with specific size, use torch.* tensor creation ops (see Creation Ops).\n\nTo create a tensor with the same size (and similar types) as another tensor, use torch.*_like tensor creation ops (see Creation Ops).\n\nTo create a tensor with similar type but different size as another tensor, use tensor.new_* creation ops.\n\nTensor.T\n\nReturns a view of this tensor with its dimensions reversed.\n\nIf n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).\n\nWARNING\n\nThe use of Tensor.T() on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider mT to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse the dimensions of a tensor.\n\nTensor.H\n\nReturns a view of a matrix (2-D tensor) conjugated and transposed.\n\nx.H is equivalent to x.transpose(0, 1).conj() for complex matrices and x.transpose(0, 1) for real matrices.\n\nSEE ALSO\n\nmH: An attribute that also works on batches of matrices.\n\nTensor.mT\n\nReturns a view of this tensor with the last two dimensions transposed.\n\nx.mT is equivalent to x.transpose(-2, -1).\n\nTensor.mH\n\nAccessing this property is equivalent to calling adjoint().\n\nTensor.new_tensor\n\n\t\n\nReturns a new Tensor with data as the tensor data.\n\n\n\n\nTensor.new_full\n\n\t\n\nReturns a Tensor of size size filled with fill_value.\n\n\n\n\nTensor.new_empty\n\n\t\n\nReturns a Tensor of size size filled with uninitialized data.\n\n\n\n\nTensor.new_ones\n\n\t\n\nReturns a Tensor of size size filled with 1.\n\n\n\n\nTensor.new_zeros\n\n\t\n\nReturns a Tensor of size size filled with 0.\n\n\n\n\nTensor.is_cuda\n\n\t\n\nIs True if the Tensor is stored on the GPU, False otherwise.\n\n\n\n\nTensor.is_quantized\n\n\t\n\nIs True if the Tensor is quantized, False otherwise.\n\n\n\n\nTensor.is_meta\n\n\t\n\nIs True if the Tensor is a meta tensor, False otherwise.\n\n\n\n\nTensor.device\n\n\t\n\nIs the torch.device where this Tensor is.\n\n\n\n\nTensor.grad\n\n\t\n\nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.\n\n\n\n\nTensor.ndim\n\n\t\n\nAlias for dim()\n\n\n\n\nTensor.real\n\n\t\n\nReturns a new tensor containing real values of the self tensor for a complex-valued input tensor.\n\n\n\n\nTensor.imag\n\n\t\n\nReturns a new tensor containing imaginary values of the self tensor.\n\n\n\n\nTensor.nbytes\n\n\t\n\nReturns the number of bytes consumed by the \"view\" of elements of the Tensor if the Tensor does not use sparse storage layout.\n\n\n\n\nTensor.itemsize\n\n\t\n\nAlias for element_size()\n\n\n\n\nTensor.abs\n\n\t\n\nSee torch.abs()\n\n\n\n\nTensor.abs_\n\n\t\n\nIn-place version of abs()\n\n\n\n\nTensor.absolute\n\n\t\n\nAlias for abs()\n\n\n\n\nTensor.absolute_\n\n\t\n\nIn-place version of absolute() Alias for abs_()\n\n\n\n\nTensor.acos\n\n\t\n\nSee torch.acos()\n\n\n\n\nTensor.acos_\n\n\t\n\nIn-place version of acos()\n\n\n\n\nTensor.arccos\n\n\t\n\nSee torch.arccos()\n\n\n\n\nTensor.arccos_\n\n\t\n\nIn-place version of arccos()\n\n\n\n\nTensor.add\n\n\t\n\nAdd a scalar or tensor to self tensor.\n\n\n\n\nTensor.add_\n\n\t\n\nIn-place version of add()\n\n\n\n\nTensor.addbmm\n\n\t\n\nSee torch.addbmm()\n\n\n\n\nTensor.addbmm_\n\n\t\n\nIn-place version of addbmm()\n\n\n\n\nTensor.addcdiv\n\n\t\n\nSee torch.addcdiv()\n\n\n\n\nTensor.addcdiv_\n\n\t\n\nIn-place version of addcdiv()\n\n\n\n\nTensor.addcmul\n\n\t\n\nSee torch.addcmul()\n\n\n\n\nTensor.addcmul_\n\n\t\n\nIn-place version of addcmul()\n\n\n\n\nTensor.addmm\n\n\t\n\nSee torch.addmm()\n\n\n\n\nTensor.addmm_\n\n\t\n\nIn-place version of addmm()\n\n\n\n\nTensor.sspaddmm\n\n\t\n\nSee torch.sspaddmm()\n\n\n\n\nTensor.addmv\n\n\t\n\nSee torch.addmv()\n\n\n\n\nTensor.addmv_\n\n\t\n\nIn-place version of addmv()\n\n\n\n\nTensor.addr\n\n\t\n\nSee torch.addr()\n\n\n\n\nTensor.addr_\n\n\t\n\nIn-place version of addr()\n\n\n\n\nTensor.adjoint\n\n\t\n\nAlias for adjoint()\n\n\n\n\nTensor.allclose\n\n\t\n\nSee torch.allclose()\n\n\n\n\nTensor.amax\n\n\t\n\nSee torch.amax()\n\n\n\n\nTensor.amin\n\n\t\n\nSee torch.amin()\n\n\n\n\nTensor.aminmax\n\n\t\n\nSee torch.aminmax()\n\n\n\n\nTensor.angle\n\n\t\n\nSee torch.angle()\n\n\n\n\nTensor.apply_\n\n\t\n\nApplies the function callable to each element in the tensor, replacing each element with the value returned by callable.\n\n\n\n\nTensor.argmax\n\n\t\n\nSee torch.argmax()\n\n\n\n\nTensor.argmin\n\n\t\n\nSee torch.argmin()\n\n\n\n\nTensor.argsort\n\n\t\n\nSee torch.argsort()\n\n\n\n\nTensor.argwhere\n\n\t\n\nSee torch.argwhere()\n\n\n\n\nTensor.asin\n\n\t\n\nSee torch.asin()\n\n\n\n\nTensor.asin_\n\n\t\n\nIn-place version of asin()\n\n\n\n\nTensor.arcsin\n\n\t\n\nSee torch.arcsin()\n\n\n\n\nTensor.arcsin_\n\n\t\n\nIn-place version of arcsin()\n\n\n\n\nTensor.as_strided\n\n\t\n\nSee torch.as_strided()\n\n\n\n\nTensor.atan\n\n\t\n\nSee torch.atan()\n\n\n\n\nTensor.atan_\n\n\t\n\nIn-place version of atan()\n\n\n\n\nTensor.arctan\n\n\t\n\nSee torch.arctan()\n\n\n\n\nTensor.arctan_\n\n\t\n\nIn-place version of arctan()\n\n\n\n\nTensor.atan2\n\n\t\n\nSee torch.atan2()\n\n\n\n\nTensor.atan2_\n\n\t\n\nIn-place version of atan2()\n\n\n\n\nTensor.arctan2\n\n\t\n\nSee torch.arctan2()\n\n\n\n\nTensor.arctan2_\n\n\t\n\natan2_(other) -> Tensor\n\n\n\n\nTensor.all\n\n\t\n\nSee torch.all()\n\n\n\n\nTensor.any\n\n\t\n\nSee torch.any()\n\n\n\n\nTensor.backward\n\n\t\n\nComputes the gradient of current tensor wrt graph leaves.\n\n\n\n\nTensor.baddbmm\n\n\t\n\nSee torch.baddbmm()\n\n\n\n\nTensor.baddbmm_\n\n\t\n\nIn-place version of baddbmm()\n\n\n\n\nTensor.bernoulli\n\n\t\n\nReturns a result tensor where each \nresult[i]\nresult[i] is independently sampled from \nBernoulli\n(\nself[i]\n)\nBernoulli(self[i]).\n\n\n\n\nTensor.bernoulli_\n\n\t\n\nFills each location of self with an independent sample from \nBernoulli\n(\np\n)\nBernoulli(p).\n\n\n\n\nTensor.bfloat16\n\n\t\n\nself.bfloat16() is equivalent to self.to(torch.bfloat16).\n\n\n\n\nTensor.bincount\n\n\t\n\nSee torch.bincount()\n\n\n\n\nTensor.bitwise_not\n\n\t\n\nSee torch.bitwise_not()\n\n\n\n\nTensor.bitwise_not_\n\n\t\n\nIn-place version of bitwise_not()\n\n\n\n\nTensor.bitwise_and\n\n\t\n\nSee torch.bitwise_and()\n\n\n\n\nTensor.bitwise_and_\n\n\t\n\nIn-place version of bitwise_and()\n\n\n\n\nTensor.bitwise_or\n\n\t\n\nSee torch.bitwise_or()\n\n\n\n\nTensor.bitwise_or_\n\n\t\n\nIn-place version of bitwise_or()\n\n\n\n\nTensor.bitwise_xor\n\n\t\n\nSee torch.bitwise_xor()\n\n\n\n\nTensor.bitwise_xor_\n\n\t\n\nIn-place version of bitwise_xor()\n\n\n\n\nTensor.bitwise_left_shift\n\n\t\n\nSee torch.bitwise_left_shift()\n\n\n\n\nTensor.bitwise_left_shift_\n\n\t\n\nIn-place version of bitwise_left_shift()\n\n\n\n\nTensor.bitwise_right_shift\n\n\t\n\nSee torch.bitwise_right_shift()\n\n\n\n\nTensor.bitwise_right_shift_\n\n\t\n\nIn-place version of bitwise_right_shift()\n\n\n\n\nTensor.bmm\n\n\t\n\nSee torch.bmm()\n\n\n\n\nTensor.bool\n\n\t\n\nself.bool() is equivalent to self.to(torch.bool).\n\n\n\n\nTensor.byte\n\n\t\n\nself.byte() is equivalent to self.to(torch.uint8).\n\n\n\n\nTensor.broadcast_to\n\n\t\n\nSee torch.broadcast_to().\n\n\n\n\nTensor.cauchy_\n\n\t\n\nFills the tensor with numbers drawn from the Cauchy distribution:\n\n\n\n\nTensor.ceil\n\n\t\n\nSee torch.ceil()\n\n\n\n\nTensor.ceil_\n\n\t\n\nIn-place version of ceil()\n\n\n\n\nTensor.char\n\n\t\n\nself.char() is equivalent to self.to(torch.int8).\n\n\n\n\nTensor.cholesky\n\n\t\n\nSee torch.cholesky()\n\n\n\n\nTensor.cholesky_inverse\n\n\t\n\nSee torch.cholesky_inverse()\n\n\n\n\nTensor.cholesky_solve\n\n\t\n\nSee torch.cholesky_solve()\n\n\n\n\nTensor.chunk\n\n\t\n\nSee torch.chunk()\n\n\n\n\nTensor.clamp\n\n\t\n\nSee torch.clamp()\n\n\n\n\nTensor.clamp_\n\n\t\n\nIn-place version of clamp()\n\n\n\n\nTensor.clip\n\n\t\n\nAlias for clamp().\n\n\n\n\nTensor.clip_\n\n\t\n\nAlias for clamp_().\n\n\n\n\nTensor.clone\n\n\t\n\nSee torch.clone()\n\n\n\n\nTensor.contiguous\n\n\t\n\nReturns a contiguous in memory tensor containing the same data as self tensor.\n\n\n\n\nTensor.copy_\n\n\t\n\nCopies the elements from src into self tensor and returns self.\n\n\n\n\nTensor.conj\n\n\t\n\nSee torch.conj()\n\n\n\n\nTensor.conj_physical\n\n\t\n\nSee torch.conj_physical()\n\n\n\n\nTensor.conj_physical_\n\n\t\n\nIn-place version of conj_physical()\n\n\n\n\nTensor.resolve_conj\n\n\t\n\nSee torch.resolve_conj()\n\n\n\n\nTensor.resolve_neg\n\n\t\n\nSee torch.resolve_neg()\n\n\n\n\nTensor.copysign\n\n\t\n\nSee torch.copysign()\n\n\n\n\nTensor.copysign_\n\n\t\n\nIn-place version of copysign()\n\n\n\n\nTensor.cos\n\n\t\n\nSee torch.cos()\n\n\n\n\nTensor.cos_\n\n\t\n\nIn-place version of cos()\n\n\n\n\nTensor.cosh\n\n\t\n\nSee torch.cosh()\n\n\n\n\nTensor.cosh_\n\n\t\n\nIn-place version of cosh()\n\n\n\n\nTensor.corrcoef\n\n\t\n\nSee torch.corrcoef()\n\n\n\n\nTensor.count_nonzero\n\n\t\n\nSee torch.count_nonzero()\n\n\n\n\nTensor.cov\n\n\t\n\nSee torch.cov()\n\n\n\n\nTensor.acosh\n\n\t\n\nSee torch.acosh()\n\n\n\n\nTensor.acosh_\n\n\t\n\nIn-place version of acosh()\n\n\n\n\nTensor.arccosh\n\n\t\n\nacosh() -> Tensor\n\n\n\n\nTensor.arccosh_\n\n\t\n\nacosh_() -> Tensor\n\n\n\n\nTensor.cpu\n\n\t\n\nReturns a copy of this object in CPU memory.\n\n\n\n\nTensor.cross\n\n\t\n\nSee torch.cross()\n\n\n\n\nTensor.cuda\n\n\t\n\nReturns a copy of this object in CUDA memory.\n\n\n\n\nTensor.logcumsumexp\n\n\t\n\nSee torch.logcumsumexp()\n\n\n\n\nTensor.cummax\n\n\t\n\nSee torch.cummax()\n\n\n\n\nTensor.cummin\n\n\t\n\nSee torch.cummin()\n\n\n\n\nTensor.cumprod\n\n\t\n\nSee torch.cumprod()\n\n\n\n\nTensor.cumprod_\n\n\t\n\nIn-place version of cumprod()\n\n\n\n\nTensor.cumsum\n\n\t\n\nSee torch.cumsum()\n\n\n\n\nTensor.cumsum_\n\n\t\n\nIn-place version of cumsum()\n\n\n\n\nTensor.chalf\n\n\t\n\nself.chalf() is equivalent to self.to(torch.complex32).\n\n\n\n\nTensor.cfloat\n\n\t\n\nself.cfloat() is equivalent to self.to(torch.complex64).\n\n\n\n\nTensor.cdouble\n\n\t\n\nself.cdouble() is equivalent to self.to(torch.complex128).\n\n\n\n\nTensor.data_ptr\n\n\t\n\nReturns the address of the first element of self tensor.\n\n\n\n\nTensor.deg2rad\n\n\t\n\nSee torch.deg2rad()\n\n\n\n\nTensor.dequantize\n\n\t\n\nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.\n\n\n\n\nTensor.det\n\n\t\n\nSee torch.det()\n\n\n\n\nTensor.dense_dim\n\n\t\n\nReturn the number of dense dimensions in a sparse tensor self.\n\n\n\n\nTensor.detach\n\n\t\n\nReturns a new Tensor, detached from the current graph.\n\n\n\n\nTensor.detach_\n\n\t\n\nDetaches the Tensor from the graph that created it, making it a leaf.\n\n\n\n\nTensor.diag\n\n\t\n\nSee torch.diag()\n\n\n\n\nTensor.diag_embed\n\n\t\n\nSee torch.diag_embed()\n\n\n\n\nTensor.diagflat\n\n\t\n\nSee torch.diagflat()\n\n\n\n\nTensor.diagonal\n\n\t\n\nSee torch.diagonal()\n\n\n\n\nTensor.diagonal_scatter\n\n\t\n\nSee torch.diagonal_scatter()\n\n\n\n\nTensor.fill_diagonal_\n\n\t\n\nFill the main diagonal of a tensor that has at least 2-dimensions.\n\n\n\n\nTensor.fmax\n\n\t\n\nSee torch.fmax()\n\n\n\n\nTensor.fmin\n\n\t\n\nSee torch.fmin()\n\n\n\n\nTensor.diff\n\n\t\n\nSee torch.diff()\n\n\n\n\nTensor.digamma\n\n\t\n\nSee torch.digamma()\n\n\n\n\nTensor.digamma_\n\n\t\n\nIn-place version of digamma()\n\n\n\n\nTensor.dim\n\n\t\n\nReturns the number of dimensions of self tensor.\n\n\n\n\nTensor.dim_order\n\n\t\n\nReturns a tuple of int describing the dim order or physical layout of self.\n\n\n\n\nTensor.dist\n\n\t\n\nSee torch.dist()\n\n\n\n\nTensor.div\n\n\t\n\nSee torch.div()\n\n\n\n\nTensor.div_\n\n\t\n\nIn-place version of div()\n\n\n\n\nTensor.divide\n\n\t\n\nSee torch.divide()\n\n\n\n\nTensor.divide_\n\n\t\n\nIn-place version of divide()\n\n\n\n\nTensor.dot\n\n\t\n\nSee torch.dot()\n\n\n\n\nTensor.double\n\n\t\n\nself.double() is equivalent to self.to(torch.float64).\n\n\n\n\nTensor.dsplit\n\n\t\n\nSee torch.dsplit()\n\n\n\n\nTensor.element_size\n\n\t\n\nReturns the size in bytes of an individual element.\n\n\n\n\nTensor.eq\n\n\t\n\nSee torch.eq()\n\n\n\n\nTensor.eq_\n\n\t\n\nIn-place version of eq()\n\n\n\n\nTensor.equal\n\n\t\n\nSee torch.equal()\n\n\n\n\nTensor.erf\n\n\t\n\nSee torch.erf()\n\n\n\n\nTensor.erf_\n\n\t\n\nIn-place version of erf()\n\n\n\n\nTensor.erfc\n\n\t\n\nSee torch.erfc()\n\n\n\n\nTensor.erfc_\n\n\t\n\nIn-place version of erfc()\n\n\n\n\nTensor.erfinv\n\n\t\n\nSee torch.erfinv()\n\n\n\n\nTensor.erfinv_\n\n\t\n\nIn-place version of erfinv()\n\n\n\n\nTensor.exp\n\n\t\n\nSee torch.exp()\n\n\n\n\nTensor.exp_\n\n\t\n\nIn-place version of exp()\n\n\n\n\nTensor.expm1\n\n\t\n\nSee torch.expm1()\n\n\n\n\nTensor.expm1_\n\n\t\n\nIn-place version of expm1()\n\n\n\n\nTensor.expand\n\n\t\n\nReturns a new view of the self tensor with singleton dimensions expanded to a larger size.\n\n\n\n\nTensor.expand_as\n\n\t\n\nExpand this tensor to the same size as other.\n\n\n\n\nTensor.exponential_\n\n\t\n\nFills self tensor with elements drawn from the exponential distribution:\n\n\n\n\nTensor.fix\n\n\t\n\nSee torch.fix().\n\n\n\n\nTensor.fix_\n\n\t\n\nIn-place version of fix()\n\n\n\n\nTensor.fill_\n\n\t\n\nFills self tensor with the specified value.\n\n\n\n\nTensor.flatten\n\n\t\n\nSee torch.flatten()\n\n\n\n\nTensor.flip\n\n\t\n\nSee torch.flip()\n\n\n\n\nTensor.fliplr\n\n\t\n\nSee torch.fliplr()\n\n\n\n\nTensor.flipud\n\n\t\n\nSee torch.flipud()\n\n\n\n\nTensor.float\n\n\t\n\nself.float() is equivalent to self.to(torch.float32).\n\n\n\n\nTensor.float_power\n\n\t\n\nSee torch.float_power()\n\n\n\n\nTensor.float_power_\n\n\t\n\nIn-place version of float_power()\n\n\n\n\nTensor.floor\n\n\t\n\nSee torch.floor()\n\n\n\n\nTensor.floor_\n\n\t\n\nIn-place version of floor()\n\n\n\n\nTensor.floor_divide\n\n\t\n\nSee torch.floor_divide()\n\n\n\n\nTensor.floor_divide_\n\n\t\n\nIn-place version of floor_divide()\n\n\n\n\nTensor.fmod\n\n\t\n\nSee torch.fmod()\n\n\n\n\nTensor.fmod_\n\n\t\n\nIn-place version of fmod()\n\n\n\n\nTensor.frac\n\n\t\n\nSee torch.frac()\n\n\n\n\nTensor.frac_\n\n\t\n\nIn-place version of frac()\n\n\n\n\nTensor.frexp\n\n\t\n\nSee torch.frexp()\n\n\n\n\nTensor.gather\n\n\t\n\nSee torch.gather()\n\n\n\n\nTensor.gcd\n\n\t\n\nSee torch.gcd()\n\n\n\n\nTensor.gcd_\n\n\t\n\nIn-place version of gcd()\n\n\n\n\nTensor.ge\n\n\t\n\nSee torch.ge().\n\n\n\n\nTensor.ge_\n\n\t\n\nIn-place version of ge().\n\n\n\n\nTensor.greater_equal\n\n\t\n\nSee torch.greater_equal().\n\n\n\n\nTensor.greater_equal_\n\n\t\n\nIn-place version of greater_equal().\n\n\n\n\nTensor.geometric_\n\n\t\n\nFills self tensor with elements drawn from the geometric distribution:\n\n\n\n\nTensor.geqrf\n\n\t\n\nSee torch.geqrf()\n\n\n\n\nTensor.ger\n\n\t\n\nSee torch.ger()\n\n\n\n\nTensor.get_device\n\n\t\n\nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n\n\n\n\nTensor.gt\n\n\t\n\nSee torch.gt().\n\n\n\n\nTensor.gt_\n\n\t\n\nIn-place version of gt().\n\n\n\n\nTensor.greater\n\n\t\n\nSee torch.greater().\n\n\n\n\nTensor.greater_\n\n\t\n\nIn-place version of greater().\n\n\n\n\nTensor.half\n\n\t\n\nself.half() is equivalent to self.to(torch.float16).\n\n\n\n\nTensor.hardshrink\n\n\t\n\nSee torch.nn.functional.hardshrink()\n\n\n\n\nTensor.heaviside\n\n\t\n\nSee torch.heaviside()\n\n\n\n\nTensor.histc\n\n\t\n\nSee torch.histc()\n\n\n\n\nTensor.histogram\n\n\t\n\nSee torch.histogram()\n\n\n\n\nTensor.hsplit\n\n\t\n\nSee torch.hsplit()\n\n\n\n\nTensor.hypot\n\n\t\n\nSee torch.hypot()\n\n\n\n\nTensor.hypot_\n\n\t\n\nIn-place version of hypot()\n\n\n\n\nTensor.i0\n\n\t\n\nSee torch.i0()\n\n\n\n\nTensor.i0_\n\n\t\n\nIn-place version of i0()\n\n\n\n\nTensor.igamma\n\n\t\n\nSee torch.igamma()\n\n\n\n\nTensor.igamma_\n\n\t\n\nIn-place version of igamma()\n\n\n\n\nTensor.igammac\n\n\t\n\nSee torch.igammac()\n\n\n\n\nTensor.igammac_\n\n\t\n\nIn-place version of igammac()\n\n\n\n\nTensor.index_add_\n\n\t\n\nAccumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index.\n\n\n\n\nTensor.index_add\n\n\t\n\nOut-of-place version of torch.Tensor.index_add_().\n\n\n\n\nTensor.index_copy_\n\n\t\n\nCopies the elements of tensor into the self tensor by selecting the indices in the order given in index.\n\n\n\n\nTensor.index_copy\n\n\t\n\nOut-of-place version of torch.Tensor.index_copy_().\n\n\n\n\nTensor.index_fill_\n\n\t\n\nFills the elements of the self tensor with value value by selecting the indices in the order given in index.\n\n\n\n\nTensor.index_fill\n\n\t\n\nOut-of-place version of torch.Tensor.index_fill_().\n\n\n\n\nTensor.index_put_\n\n\t\n\nPuts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors).\n\n\n\n\nTensor.index_put\n\n\t\n\nOut-place version of index_put_().\n\n\n\n\nTensor.index_reduce_\n\n\t\n\nAccumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument.\n\n\n\n\nTensor.index_reduce\n\n\t\n\n\n\n\nTensor.index_select\n\n\t\n\nSee torch.index_select()\n\n\n\n\nTensor.indices\n\n\t\n\nReturn the indices tensor of a sparse COO tensor.\n\n\n\n\nTensor.inner\n\n\t\n\nSee torch.inner().\n\n\n\n\nTensor.int\n\n\t\n\nself.int() is equivalent to self.to(torch.int32).\n\n\n\n\nTensor.int_repr\n\n\t\n\nGiven a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.\n\n\n\n\nTensor.inverse\n\n\t\n\nSee torch.inverse()\n\n\n\n\nTensor.isclose\n\n\t\n\nSee torch.isclose()\n\n\n\n\nTensor.isfinite\n\n\t\n\nSee torch.isfinite()\n\n\n\n\nTensor.isinf\n\n\t\n\nSee torch.isinf()\n\n\n\n\nTensor.isposinf\n\n\t\n\nSee torch.isposinf()\n\n\n\n\nTensor.isneginf\n\n\t\n\nSee torch.isneginf()\n\n\n\n\nTensor.isnan\n\n\t\n\nSee torch.isnan()\n\n\n\n\nTensor.is_contiguous\n\n\t\n\nReturns True if self tensor is contiguous in memory in the order specified by memory format.\n\n\n\n\nTensor.is_complex\n\n\t\n\nReturns True if the data type of self is a complex data type.\n\n\n\n\nTensor.is_conj\n\n\t\n\nReturns True if the conjugate bit of self is set to true.\n\n\n\n\nTensor.is_floating_point\n\n\t\n\nReturns True if the data type of self is a floating point data type.\n\n\n\n\nTensor.is_inference\n\n\t\n\nSee torch.is_inference()\n\n\n\n\nTensor.is_leaf\n\n\t\n\nAll Tensors that have requires_grad which is False will be leaf Tensors by convention.\n\n\n\n\nTensor.is_pinned\n\n\t\n\nReturns true if this tensor resides in pinned memory.\n\n\n\n\nTensor.is_set_to\n\n\t\n\nReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).\n\n\n\n\nTensor.is_shared\n\n\t\n\nChecks if tensor is in shared memory.\n\n\n\n\nTensor.is_signed\n\n\t\n\nReturns True if the data type of self is a signed data type.\n\n\n\n\nTensor.is_sparse\n\n\t\n\nIs True if the Tensor uses sparse COO storage layout, False otherwise.\n\n\n\n\nTensor.istft\n\n\t\n\nSee torch.istft()\n\n\n\n\nTensor.isreal\n\n\t\n\nSee torch.isreal()\n\n\n\n\nTensor.item\n\n\t\n\nReturns the value of this tensor as a standard Python number.\n\n\n\n\nTensor.kthvalue\n\n\t\n\nSee torch.kthvalue()\n\n\n\n\nTensor.lcm\n\n\t\n\nSee torch.lcm()\n\n\n\n\nTensor.lcm_\n\n\t\n\nIn-place version of lcm()\n\n\n\n\nTensor.ldexp\n\n\t\n\nSee torch.ldexp()\n\n\n\n\nTensor.ldexp_\n\n\t\n\nIn-place version of ldexp()\n\n\n\n\nTensor.le\n\n\t\n\nSee torch.le().\n\n\n\n\nTensor.le_\n\n\t\n\nIn-place version of le().\n\n\n\n\nTensor.less_equal\n\n\t\n\nSee torch.less_equal().\n\n\n\n\nTensor.less_equal_\n\n\t\n\nIn-place version of less_equal().\n\n\n\n\nTensor.lerp\n\n\t\n\nSee torch.lerp()\n\n\n\n\nTensor.lerp_\n\n\t\n\nIn-place version of lerp()\n\n\n\n\nTensor.lgamma\n\n\t\n\nSee torch.lgamma()\n\n\n\n\nTensor.lgamma_\n\n\t\n\nIn-place version of lgamma()\n\n\n\n\nTensor.log\n\n\t\n\nSee torch.log()\n\n\n\n\nTensor.log_\n\n\t\n\nIn-place version of log()\n\n\n\n\nTensor.logdet\n\n\t\n\nSee torch.logdet()\n\n\n\n\nTensor.log10\n\n\t\n\nSee torch.log10()\n\n\n\n\nTensor.log10_\n\n\t\n\nIn-place version of log10()\n\n\n\n\nTensor.log1p\n\n\t\n\nSee torch.log1p()\n\n\n\n\nTensor.log1p_\n\n\t\n\nIn-place version of log1p()\n\n\n\n\nTensor.log2\n\n\t\n\nSee torch.log2()\n\n\n\n\nTensor.log2_\n\n\t\n\nIn-place version of log2()\n\n\n\n\nTensor.log_normal_\n\n\t\n\nFills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \n𝜇\nμ and standard deviation \n𝜎\nσ.\n\n\n\n\nTensor.logaddexp\n\n\t\n\nSee torch.logaddexp()\n\n\n\n\nTensor.logaddexp2\n\n\t\n\nSee torch.logaddexp2()\n\n\n\n\nTensor.logsumexp\n\n\t\n\nSee torch.logsumexp()\n\n\n\n\nTensor.logical_and\n\n\t\n\nSee torch.logical_and()\n\n\n\n\nTensor.logical_and_\n\n\t\n\nIn-place version of logical_and()\n\n\n\n\nTensor.logical_not\n\n\t\n\nSee torch.logical_not()\n\n\n\n\nTensor.logical_not_\n\n\t\n\nIn-place version of logical_not()\n\n\n\n\nTensor.logical_or\n\n\t\n\nSee torch.logical_or()\n\n\n\n\nTensor.logical_or_\n\n\t\n\nIn-place version of logical_or()\n\n\n\n\nTensor.logical_xor\n\n\t\n\nSee torch.logical_xor()\n\n\n\n\nTensor.logical_xor_\n\n\t\n\nIn-place version of logical_xor()\n\n\n\n\nTensor.logit\n\n\t\n\nSee torch.logit()\n\n\n\n\nTensor.logit_\n\n\t\n\nIn-place version of logit()\n\n\n\n\nTensor.long\n\n\t\n\nself.long() is equivalent to self.to(torch.int64).\n\n\n\n\nTensor.lt\n\n\t\n\nSee torch.lt().\n\n\n\n\nTensor.lt_\n\n\t\n\nIn-place version of lt().\n\n\n\n\nTensor.less\n\n\t\n\nlt(other) -> Tensor\n\n\n\n\nTensor.less_\n\n\t\n\nIn-place version of less().\n\n\n\n\nTensor.lu\n\n\t\n\nSee torch.lu()\n\n\n\n\nTensor.lu_solve\n\n\t\n\nSee torch.lu_solve()\n\n\n\n\nTensor.as_subclass\n\n\t\n\nMakes a cls instance with the same data pointer as self.\n\n\n\n\nTensor.map_\n\n\t\n\nApplies callable for each element in self tensor and the given tensor and stores the results in self tensor.\n\n\n\n\nTensor.masked_scatter_\n\n\t\n\nCopies elements from source into self tensor at positions where the mask is True.\n\n\n\n\nTensor.masked_scatter\n\n\t\n\nOut-of-place version of torch.Tensor.masked_scatter_()\n\n\n\n\nTensor.masked_fill_\n\n\t\n\nFills elements of self tensor with value where mask is True.\n\n\n\n\nTensor.masked_fill\n\n\t\n\nOut-of-place version of torch.Tensor.masked_fill_()\n\n\n\n\nTensor.masked_select\n\n\t\n\nSee torch.masked_select()\n\n\n\n\nTensor.matmul\n\n\t\n\nSee torch.matmul()\n\n\n\n\nTensor.matrix_power\n\n\t\n\nNOTE\n\nmatrix_power() is deprecated, use torch.linalg.matrix_power() instead.\n\n\n\n\nTensor.matrix_exp\n\n\t\n\nSee torch.matrix_exp()\n\n\n\n\nTensor.max\n\n\t\n\nSee torch.max()\n\n\n\n\nTensor.maximum\n\n\t\n\nSee torch.maximum()\n\n\n\n\nTensor.mean\n\n\t\n\nSee torch.mean()\n\n\n\n\nTensor.nanmean\n\n\t\n\nSee torch.nanmean()\n\n\n\n\nTensor.median\n\n\t\n\nSee torch.median()\n\n\n\n\nTensor.nanmedian\n\n\t\n\nSee torch.nanmedian()\n\n\n\n\nTensor.min\n\n\t\n\nSee torch.min()\n\n\n\n\nTensor.minimum\n\n\t\n\nSee torch.minimum()\n\n\n\n\nTensor.mm\n\n\t\n\nSee torch.mm()\n\n\n\n\nTensor.smm\n\n\t\n\nSee torch.smm()\n\n\n\n\nTensor.mode\n\n\t\n\nSee torch.mode()\n\n\n\n\nTensor.movedim\n\n\t\n\nSee torch.movedim()\n\n\n\n\nTensor.moveaxis\n\n\t\n\nSee torch.moveaxis()\n\n\n\n\nTensor.msort\n\n\t\n\nSee torch.msort()\n\n\n\n\nTensor.mul\n\n\t\n\nSee torch.mul().\n\n\n\n\nTensor.mul_\n\n\t\n\nIn-place version of mul().\n\n\n\n\nTensor.multiply\n\n\t\n\nSee torch.multiply().\n\n\n\n\nTensor.multiply_\n\n\t\n\nIn-place version of multiply().\n\n\n\n\nTensor.multinomial\n\n\t\n\nSee torch.multinomial()\n\n\n\n\nTensor.mv\n\n\t\n\nSee torch.mv()\n\n\n\n\nTensor.mvlgamma\n\n\t\n\nSee torch.mvlgamma()\n\n\n\n\nTensor.mvlgamma_\n\n\t\n\nIn-place version of mvlgamma()\n\n\n\n\nTensor.nansum\n\n\t\n\nSee torch.nansum()\n\n\n\n\nTensor.narrow\n\n\t\n\nSee torch.narrow().\n\n\n\n\nTensor.narrow_copy\n\n\t\n\nSee torch.narrow_copy().\n\n\n\n\nTensor.ndimension\n\n\t\n\nAlias for dim()\n\n\n\n\nTensor.nan_to_num\n\n\t\n\nSee torch.nan_to_num().\n\n\n\n\nTensor.nan_to_num_\n\n\t\n\nIn-place version of nan_to_num().\n\n\n\n\nTensor.ne\n\n\t\n\nSee torch.ne().\n\n\n\n\nTensor.ne_\n\n\t\n\nIn-place version of ne().\n\n\n\n\nTensor.not_equal\n\n\t\n\nSee torch.not_equal().\n\n\n\n\nTensor.not_equal_\n\n\t\n\nIn-place version of not_equal().\n\n\n\n\nTensor.neg\n\n\t\n\nSee torch.neg()\n\n\n\n\nTensor.neg_\n\n\t\n\nIn-place version of neg()\n\n\n\n\nTensor.negative\n\n\t\n\nSee torch.negative()\n\n\n\n\nTensor.negative_\n\n\t\n\nIn-place version of negative()\n\n\n\n\nTensor.nelement\n\n\t\n\nAlias for numel()\n\n\n\n\nTensor.nextafter\n\n\t\n\nSee torch.nextafter()\n\n\n\n\nTensor.nextafter_\n\n\t\n\nIn-place version of nextafter()\n\n\n\n\nTensor.nonzero\n\n\t\n\nSee torch.nonzero()\n\n\n\n\nTensor.norm\n\n\t\n\nSee torch.norm()\n\n\n\n\nTensor.normal_\n\n\t\n\nFills self tensor with elements samples from the normal distribution parameterized by mean and std.\n\n\n\n\nTensor.numel\n\n\t\n\nSee torch.numel()\n\n\n\n\nTensor.numpy\n\n\t\n\nReturns the tensor as a NumPy ndarray.\n\n\n\n\nTensor.orgqr\n\n\t\n\nSee torch.orgqr()\n\n\n\n\nTensor.ormqr\n\n\t\n\nSee torch.ormqr()\n\n\n\n\nTensor.outer\n\n\t\n\nSee torch.outer().\n\n\n\n\nTensor.permute\n\n\t\n\nSee torch.permute()\n\n\n\n\nTensor.pin_memory\n\n\t\n\nCopies the tensor to pinned memory, if it's not already pinned.\n\n\n\n\nTensor.pinverse\n\n\t\n\nSee torch.pinverse()\n\n\n\n\nTensor.polygamma\n\n\t\n\nSee torch.polygamma()\n\n\n\n\nTensor.polygamma_\n\n\t\n\nIn-place version of polygamma()\n\n\n\n\nTensor.positive\n\n\t\n\nSee torch.positive()\n\n\n\n\nTensor.pow\n\n\t\n\nSee torch.pow()\n\n\n\n\nTensor.pow_\n\n\t\n\nIn-place version of pow()\n\n\n\n\nTensor.prod\n\n\t\n\nSee torch.prod()\n\n\n\n\nTensor.put_\n\n\t\n\nCopies the elements from source into the positions specified by index.\n\n\n\n\nTensor.qr\n\n\t\n\nSee torch.qr()\n\n\n\n\nTensor.qscheme\n\n\t\n\nReturns the quantization scheme of a given QTensor.\n\n\n\n\nTensor.quantile\n\n\t\n\nSee torch.quantile()\n\n\n\n\nTensor.nanquantile\n\n\t\n\nSee torch.nanquantile()\n\n\n\n\nTensor.q_scale\n\n\t\n\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().\n\n\n\n\nTensor.q_zero_point\n\n\t\n\nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().\n\n\n\n\nTensor.q_per_channel_scales\n\n\t\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.\n\n\n\n\nTensor.q_per_channel_zero_points\n\n\t\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.\n\n\n\n\nTensor.q_per_channel_axis\n\n\t\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.\n\n\n\n\nTensor.rad2deg\n\n\t\n\nSee torch.rad2deg()\n\n\n\n\nTensor.random_\n\n\t\n\nFills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1].\n\n\n\n\nTensor.ravel\n\n\t\n\nsee torch.ravel()\n\n\n\n\nTensor.reciprocal\n\n\t\n\nSee torch.reciprocal()\n\n\n\n\nTensor.reciprocal_\n\n\t\n\nIn-place version of reciprocal()\n\n\n\n\nTensor.record_stream\n\n\t\n\nEnsures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.\n\n\n\n\nTensor.register_hook\n\n\t\n\nRegisters a backward hook.\n\n\n\n\nTensor.register_post_accumulate_grad_hook\n\n\t\n\nRegisters a backward hook that runs after grad accumulation.\n\n\n\n\nTensor.remainder\n\n\t\n\nSee torch.remainder()\n\n\n\n\nTensor.remainder_\n\n\t\n\nIn-place version of remainder()\n\n\n\n\nTensor.renorm\n\n\t\n\nSee torch.renorm()\n\n\n\n\nTensor.renorm_\n\n\t\n\nIn-place version of renorm()\n\n\n\n\nTensor.repeat\n\n\t\n\nRepeats this tensor along the specified dimensions.\n\n\n\n\nTensor.repeat_interleave\n\n\t\n\nSee torch.repeat_interleave().\n\n\n\n\nTensor.requires_grad\n\n\t\n\nIs True if gradients need to be computed for this Tensor, False otherwise.\n\n\n\n\nTensor.requires_grad_\n\n\t\n\nChange if autograd should record operations on this tensor: sets this tensor's requires_grad attribute in-place.\n\n\n\n\nTensor.reshape\n\n\t\n\nReturns a tensor with the same data and number of elements as self but with the specified shape.\n\n\n\n\nTensor.reshape_as\n\n\t\n\nReturns this tensor as the same shape as other.\n\n\n\n\nTensor.resize_\n\n\t\n\nResizes self tensor to the specified size.\n\n\n\n\nTensor.resize_as_\n\n\t\n\nResizes the self tensor to be the same size as the specified tensor.\n\n\n\n\nTensor.retain_grad\n\n\t\n\nEnables this Tensor to have their grad populated during backward().\n\n\n\n\nTensor.retains_grad\n\n\t\n\nIs True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise.\n\n\n\n\nTensor.roll\n\n\t\n\nSee torch.roll()\n\n\n\n\nTensor.rot90\n\n\t\n\nSee torch.rot90()\n\n\n\n\nTensor.round\n\n\t\n\nSee torch.round()\n\n\n\n\nTensor.round_\n\n\t\n\nIn-place version of round()\n\n\n\n\nTensor.rsqrt\n\n\t\n\nSee torch.rsqrt()\n\n\n\n\nTensor.rsqrt_\n\n\t\n\nIn-place version of rsqrt()\n\n\n\n\nTensor.scatter\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_()\n\n\n\n\nTensor.scatter_\n\n\t\n\nWrites all values from the tensor src into self at the indices specified in the index tensor.\n\n\n\n\nTensor.scatter_add_\n\n\t\n\nAdds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_().\n\n\n\n\nTensor.scatter_add\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_add_()\n\n\n\n\nTensor.scatter_reduce_\n\n\t\n\nReduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\").\n\n\n\n\nTensor.scatter_reduce\n\n\t\n\nOut-of-place version of torch.Tensor.scatter_reduce_()\n\n\n\n\nTensor.select\n\n\t\n\nSee torch.select()\n\n\n\n\nTensor.select_scatter\n\n\t\n\nSee torch.select_scatter()\n\n\n\n\nTensor.set_\n\n\t\n\nSets the underlying storage, size, and strides.\n\n\n\n\nTensor.share_memory_\n\n\t\n\nMoves the underlying storage to shared memory.\n\n\n\n\nTensor.short\n\n\t\n\nself.short() is equivalent to self.to(torch.int16).\n\n\n\n\nTensor.sigmoid\n\n\t\n\nSee torch.sigmoid()\n\n\n\n\nTensor.sigmoid_\n\n\t\n\nIn-place version of sigmoid()\n\n\n\n\nTensor.sign\n\n\t\n\nSee torch.sign()\n\n\n\n\nTensor.sign_\n\n\t\n\nIn-place version of sign()\n\n\n\n\nTensor.signbit\n\n\t\n\nSee torch.signbit()\n\n\n\n\nTensor.sgn\n\n\t\n\nSee torch.sgn()\n\n\n\n\nTensor.sgn_\n\n\t\n\nIn-place version of sgn()\n\n\n\n\nTensor.sin\n\n\t\n\nSee torch.sin()\n\n\n\n\nTensor.sin_\n\n\t\n\nIn-place version of sin()\n\n\n\n\nTensor.sinc\n\n\t\n\nSee torch.sinc()\n\n\n\n\nTensor.sinc_\n\n\t\n\nIn-place version of sinc()\n\n\n\n\nTensor.sinh\n\n\t\n\nSee torch.sinh()\n\n\n\n\nTensor.sinh_\n\n\t\n\nIn-place version of sinh()\n\n\n\n\nTensor.asinh\n\n\t\n\nSee torch.asinh()\n\n\n\n\nTensor.asinh_\n\n\t\n\nIn-place version of asinh()\n\n\n\n\nTensor.arcsinh\n\n\t\n\nSee torch.arcsinh()\n\n\n\n\nTensor.arcsinh_\n\n\t\n\nIn-place version of arcsinh()\n\n\n\n\nTensor.shape\n\n\t\n\nReturns the size of the self tensor.\n\n\n\n\nTensor.size\n\n\t\n\nReturns the size of the self tensor.\n\n\n\n\nTensor.slogdet\n\n\t\n\nSee torch.slogdet()\n\n\n\n\nTensor.slice_scatter\n\n\t\n\nSee torch.slice_scatter()\n\n\n\n\nTensor.softmax\n\n\t\n\nAlias for torch.nn.functional.softmax().\n\n\n\n\nTensor.sort\n\n\t\n\nSee torch.sort()\n\n\n\n\nTensor.split\n\n\t\n\nSee torch.split()\n\n\n\n\nTensor.sparse_mask\n\n\t\n\nReturns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask.\n\n\n\n\nTensor.sparse_dim\n\n\t\n\nReturn the number of sparse dimensions in a sparse tensor self.\n\n\n\n\nTensor.sqrt\n\n\t\n\nSee torch.sqrt()\n\n\n\n\nTensor.sqrt_\n\n\t\n\nIn-place version of sqrt()\n\n\n\n\nTensor.square\n\n\t\n\nSee torch.square()\n\n\n\n\nTensor.square_\n\n\t\n\nIn-place version of square()\n\n\n\n\nTensor.squeeze\n\n\t\n\nSee torch.squeeze()\n\n\n\n\nTensor.squeeze_\n\n\t\n\nIn-place version of squeeze()\n\n\n\n\nTensor.std\n\n\t\n\nSee torch.std()\n\n\n\n\nTensor.stft\n\n\t\n\nSee torch.stft()\n\n\n\n\nTensor.storage\n\n\t\n\nReturns the underlying TypedStorage.\n\n\n\n\nTensor.untyped_storage\n\n\t\n\nReturns the underlying UntypedStorage.\n\n\n\n\nTensor.storage_offset\n\n\t\n\nReturns self tensor's offset in the underlying storage in terms of number of storage elements (not bytes).\n\n\n\n\nTensor.storage_type\n\n\t\n\nReturns the type of the underlying storage.\n\n\n\n\nTensor.stride\n\n\t\n\nReturns the stride of self tensor.\n\n\n\n\nTensor.sub\n\n\t\n\nSee torch.sub().\n\n\n\n\nTensor.sub_\n\n\t\n\nIn-place version of sub()\n\n\n\n\nTensor.subtract\n\n\t\n\nSee torch.subtract().\n\n\n\n\nTensor.subtract_\n\n\t\n\nIn-place version of subtract().\n\n\n\n\nTensor.sum\n\n\t\n\nSee torch.sum()\n\n\n\n\nTensor.sum_to_size\n\n\t\n\nSum this tensor to size.\n\n\n\n\nTensor.svd\n\n\t\n\nSee torch.svd()\n\n\n\n\nTensor.swapaxes\n\n\t\n\nSee torch.swapaxes()\n\n\n\n\nTensor.swapdims\n\n\t\n\nSee torch.swapdims()\n\n\n\n\nTensor.t\n\n\t\n\nSee torch.t()\n\n\n\n\nTensor.t_\n\n\t\n\nIn-place version of t()\n\n\n\n\nTensor.tensor_split\n\n\t\n\nSee torch.tensor_split()\n\n\n\n\nTensor.tile\n\n\t\n\nSee torch.tile()\n\n\n\n\nTensor.to\n\n\t\n\nPerforms Tensor dtype and/or device conversion.\n\n\n\n\nTensor.to_mkldnn\n\n\t\n\nReturns a copy of the tensor in torch.mkldnn layout.\n\n\n\n\nTensor.take\n\n\t\n\nSee torch.take()\n\n\n\n\nTensor.take_along_dim\n\n\t\n\nSee torch.take_along_dim()\n\n\n\n\nTensor.tan\n\n\t\n\nSee torch.tan()\n\n\n\n\nTensor.tan_\n\n\t\n\nIn-place version of tan()\n\n\n\n\nTensor.tanh\n\n\t\n\nSee torch.tanh()\n\n\n\n\nTensor.tanh_\n\n\t\n\nIn-place version of tanh()\n\n\n\n\nTensor.atanh\n\n\t\n\nSee torch.atanh()\n\n\n\n\nTensor.atanh_\n\n\t\n\nIn-place version of atanh()\n\n\n\n\nTensor.arctanh\n\n\t\n\nSee torch.arctanh()\n\n\n\n\nTensor.arctanh_\n\n\t\n\nIn-place version of arctanh()\n\n\n\n\nTensor.tolist\n\n\t\n\nReturns the tensor as a (nested) list.\n\n\n\n\nTensor.topk\n\n\t\n\nSee torch.topk()\n\n\n\n\nTensor.to_dense\n\n\t\n\nCreates a strided copy of self if self is not a strided tensor, otherwise returns self.\n\n\n\n\nTensor.to_sparse\n\n\t\n\nReturns a sparse copy of the tensor.\n\n\n\n\nTensor.to_sparse_csr\n\n\t\n\nConvert a tensor to compressed row storage format (CSR).\n\n\n\n\nTensor.to_sparse_csc\n\n\t\n\nConvert a tensor to compressed column storage (CSC) format.\n\n\n\n\nTensor.to_sparse_bsr\n\n\t\n\nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.\n\n\n\n\nTensor.to_sparse_bsc\n\n\t\n\nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.\n\n\n\n\nTensor.trace\n\n\t\n\nSee torch.trace()\n\n\n\n\nTensor.transpose\n\n\t\n\nSee torch.transpose()\n\n\n\n\nTensor.transpose_\n\n\t\n\nIn-place version of transpose()\n\n\n\n\nTensor.triangular_solve\n\n\t\n\nSee torch.triangular_solve()\n\n\n\n\nTensor.tril\n\n\t\n\nSee torch.tril()\n\n\n\n\nTensor.tril_\n\n\t\n\nIn-place version of tril()\n\n\n\n\nTensor.triu\n\n\t\n\nSee torch.triu()\n\n\n\n\nTensor.triu_\n\n\t\n\nIn-place version of triu()\n\n\n\n\nTensor.true_divide\n\n\t\n\nSee torch.true_divide()\n\n\n\n\nTensor.true_divide_\n\n\t\n\nIn-place version of true_divide_()\n\n\n\n\nTensor.trunc\n\n\t\n\nSee torch.trunc()\n\n\n\n\nTensor.trunc_\n\n\t\n\nIn-place version of trunc()\n\n\n\n\nTensor.type\n\n\t\n\nReturns the type if dtype is not provided, else casts this object to the specified type.\n\n\n\n\nTensor.type_as\n\n\t\n\nReturns this tensor cast to the type of the given tensor.\n\n\n\n\nTensor.unbind\n\n\t\n\nSee torch.unbind()\n\n\n\n\nTensor.unflatten\n\n\t\n\nSee torch.unflatten().\n\n\n\n\nTensor.unfold\n\n\t\n\nReturns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.\n\n\n\n\nTensor.uniform_\n\n\t\n\nFills self tensor with numbers sampled from the continuous uniform distribution:\n\n\n\n\nTensor.unique\n\n\t\n\nReturns the unique elements of the input tensor.\n\n\n\n\nTensor.unique_consecutive\n\n\t\n\nEliminates all but the first element from every consecutive group of equivalent elements.\n\n\n\n\nTensor.unsqueeze\n\n\t\n\nSee torch.unsqueeze()\n\n\n\n\nTensor.unsqueeze_\n\n\t\n\nIn-place version of unsqueeze()\n\n\n\n\nTensor.values\n\n\t\n\nReturn the values tensor of a sparse COO tensor.\n\n\n\n\nTensor.var\n\n\t\n\nSee torch.var()\n\n\n\n\nTensor.vdot\n\n\t\n\nSee torch.vdot()\n\n\n\n\nTensor.view\n\n\t\n\nReturns a new tensor with the same data as the self tensor but of a different shape.\n\n\n\n\nTensor.view_as\n\n\t\n\nView this tensor as the same size as other.\n\n\n\n\nTensor.vsplit\n\n\t\n\nSee torch.vsplit()\n\n\n\n\nTensor.where\n\n\t\n\nself.where(condition, y) is equivalent to torch.where(condition, self, y).\n\n\n\n\nTensor.xlogy\n\n\t\n\nSee torch.xlogy()\n\n\n\n\nTensor.xlogy_\n\n\t\n\nIn-place version of xlogy()\n\n\n\n\nTensor.zero_\n\n\t\n\nFills self tensor with zeros.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.Tensor\nData types\nInitializing and basic operations\nTensor class reference\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.nn — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/nn.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.nn\nShortcuts\nTORCH.NN\n\nThese are the basic building blocks for graphs:\n\ntorch.nn\n\nContainers\n\nConvolution Layers\n\nPooling layers\n\nPadding Layers\n\nNon-linear Activations (weighted sum, nonlinearity)\n\nNon-linear Activations (other)\n\nNormalization Layers\n\nRecurrent Layers\n\nTransformer Layers\n\nLinear Layers\n\nDropout Layers\n\nSparse Layers\n\nDistance Functions\n\nLoss Functions\n\nVision Layers\n\nShuffle Layers\n\nDataParallel Layers (multi-GPU, distributed)\n\nUtilities\n\nQuantized Functions\n\nLazy Modules Initialization\n\nParameter\n\n\t\n\nA kind of Tensor that is to be considered a module parameter.\n\n\n\n\nUninitializedParameter\n\n\t\n\nA parameter that is not initialized.\n\n\n\n\nUninitializedBuffer\n\n\t\n\nA buffer that is not initialized.\n\nContainers\n\nModule\n\n\t\n\nBase class for all neural network modules.\n\n\n\n\nSequential\n\n\t\n\nA sequential container.\n\n\n\n\nModuleList\n\n\t\n\nHolds submodules in a list.\n\n\n\n\nModuleDict\n\n\t\n\nHolds submodules in a dictionary.\n\n\n\n\nParameterList\n\n\t\n\nHolds parameters in a list.\n\n\n\n\nParameterDict\n\n\t\n\nHolds parameters in a dictionary.\n\nGlobal Hooks For Module\n\nregister_module_forward_pre_hook\n\n\t\n\nRegisters a forward pre-hook common to all modules.\n\n\n\n\nregister_module_forward_hook\n\n\t\n\nRegisters a global forward hook for all the modules\n\n\n\n\nregister_module_backward_hook\n\n\t\n\nRegisters a backward hook common to all the modules.\n\n\n\n\nregister_module_full_backward_pre_hook\n\n\t\n\nRegisters a backward pre-hook common to all the modules.\n\n\n\n\nregister_module_full_backward_hook\n\n\t\n\nRegisters a backward hook common to all the modules.\n\n\n\n\nregister_module_buffer_registration_hook\n\n\t\n\nRegisters a buffer registration hook common to all modules.\n\n\n\n\nregister_module_module_registration_hook\n\n\t\n\nRegisters a module registration hook common to all modules.\n\n\n\n\nregister_module_parameter_registration_hook\n\n\t\n\nRegisters a parameter registration hook common to all modules.\n\nConvolution Layers\n\nnn.Conv1d\n\n\t\n\nApplies a 1D convolution over an input signal composed of several input planes.\n\n\n\n\nnn.Conv2d\n\n\t\n\nApplies a 2D convolution over an input signal composed of several input planes.\n\n\n\n\nnn.Conv3d\n\n\t\n\nApplies a 3D convolution over an input signal composed of several input planes.\n\n\n\n\nnn.ConvTranspose1d\n\n\t\n\nApplies a 1D transposed convolution operator over an input image composed of several input planes.\n\n\n\n\nnn.ConvTranspose2d\n\n\t\n\nApplies a 2D transposed convolution operator over an input image composed of several input planes.\n\n\n\n\nnn.ConvTranspose3d\n\n\t\n\nApplies a 3D transposed convolution operator over an input image composed of several input planes.\n\n\n\n\nnn.LazyConv1d\n\n\t\n\nA torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyConv2d\n\n\t\n\nA torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyConv3d\n\n\t\n\nA torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyConvTranspose1d\n\n\t\n\nA torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyConvTranspose2d\n\n\t\n\nA torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyConvTranspose3d\n\n\t\n\nA torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).\n\n\n\n\nnn.Unfold\n\n\t\n\nExtracts sliding local blocks from a batched input tensor.\n\n\n\n\nnn.Fold\n\n\t\n\nCombines an array of sliding local blocks into a large containing tensor.\n\nPooling layers\n\nnn.MaxPool1d\n\n\t\n\nApplies a 1D max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.MaxPool2d\n\n\t\n\nApplies a 2D max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.MaxPool3d\n\n\t\n\nApplies a 3D max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.MaxUnpool1d\n\n\t\n\nComputes a partial inverse of MaxPool1d.\n\n\n\n\nnn.MaxUnpool2d\n\n\t\n\nComputes a partial inverse of MaxPool2d.\n\n\n\n\nnn.MaxUnpool3d\n\n\t\n\nComputes a partial inverse of MaxPool3d.\n\n\n\n\nnn.AvgPool1d\n\n\t\n\nApplies a 1D average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AvgPool2d\n\n\t\n\nApplies a 2D average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AvgPool3d\n\n\t\n\nApplies a 3D average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.FractionalMaxPool2d\n\n\t\n\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.FractionalMaxPool3d\n\n\t\n\nApplies a 3D fractional max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.LPPool1d\n\n\t\n\nApplies a 1D power-average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.LPPool2d\n\n\t\n\nApplies a 2D power-average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveMaxPool1d\n\n\t\n\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveMaxPool2d\n\n\t\n\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveMaxPool3d\n\n\t\n\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveAvgPool1d\n\n\t\n\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveAvgPool2d\n\n\t\n\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n\n\n\nnn.AdaptiveAvgPool3d\n\n\t\n\nApplies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPadding Layers\n\nnn.ReflectionPad1d\n\n\t\n\nPads the input tensor using the reflection of the input boundary.\n\n\n\n\nnn.ReflectionPad2d\n\n\t\n\nPads the input tensor using the reflection of the input boundary.\n\n\n\n\nnn.ReflectionPad3d\n\n\t\n\nPads the input tensor using the reflection of the input boundary.\n\n\n\n\nnn.ReplicationPad1d\n\n\t\n\nPads the input tensor using replication of the input boundary.\n\n\n\n\nnn.ReplicationPad2d\n\n\t\n\nPads the input tensor using replication of the input boundary.\n\n\n\n\nnn.ReplicationPad3d\n\n\t\n\nPads the input tensor using replication of the input boundary.\n\n\n\n\nnn.ZeroPad1d\n\n\t\n\nPads the input tensor boundaries with zero.\n\n\n\n\nnn.ZeroPad2d\n\n\t\n\nPads the input tensor boundaries with zero.\n\n\n\n\nnn.ZeroPad3d\n\n\t\n\nPads the input tensor boundaries with zero.\n\n\n\n\nnn.ConstantPad1d\n\n\t\n\nPads the input tensor boundaries with a constant value.\n\n\n\n\nnn.ConstantPad2d\n\n\t\n\nPads the input tensor boundaries with a constant value.\n\n\n\n\nnn.ConstantPad3d\n\n\t\n\nPads the input tensor boundaries with a constant value.\n\nNon-linear Activations (weighted sum, nonlinearity)\n\nnn.ELU\n\n\t\n\nApplies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).\n\n\n\n\nnn.Hardshrink\n\n\t\n\nApplies the Hard Shrinkage (Hardshrink) function element-wise.\n\n\n\n\nnn.Hardsigmoid\n\n\t\n\nApplies the Hardsigmoid function element-wise.\n\n\n\n\nnn.Hardtanh\n\n\t\n\nApplies the HardTanh function element-wise.\n\n\n\n\nnn.Hardswish\n\n\t\n\nApplies the Hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.\n\n\n\n\nnn.LeakyReLU\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.LogSigmoid\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.MultiheadAttention\n\n\t\n\nAllows the model to jointly attend to information from different representation subspaces as described in the paper: Attention Is All You Need.\n\n\n\n\nnn.PReLU\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.ReLU\n\n\t\n\nApplies the rectified linear unit function element-wise:\n\n\n\n\nnn.ReLU6\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.RReLU\n\n\t\n\nApplies the randomized leaky rectified liner unit function, element-wise, as described in the paper:\n\n\n\n\nnn.SELU\n\n\t\n\nApplied element-wise, as:\n\n\n\n\nnn.CELU\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.GELU\n\n\t\n\nApplies the Gaussian Error Linear Units function:\n\n\n\n\nnn.Sigmoid\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.SiLU\n\n\t\n\nApplies the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n\n\n\nnn.Mish\n\n\t\n\nApplies the Mish function, element-wise.\n\n\n\n\nnn.Softplus\n\n\t\n\nApplies the Softplus function \nSoftplus\n(\n𝑥\n)\n=\n1\n𝛽\n∗\nlog\n⁡\n(\n1\n+\nexp\n⁡\n(\n𝛽\n∗\n𝑥\n)\n)\nSoftplus(x)=\nβ\n1\n\t​\n\n∗log(1+exp(β∗x)) element-wise.\n\n\n\n\nnn.Softshrink\n\n\t\n\nApplies the soft shrinkage function elementwise:\n\n\n\n\nnn.Softsign\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.Tanh\n\n\t\n\nApplies the Hyperbolic Tangent (Tanh) function element-wise.\n\n\n\n\nnn.Tanhshrink\n\n\t\n\nApplies the element-wise function:\n\n\n\n\nnn.Threshold\n\n\t\n\nThresholds each element of the input Tensor.\n\n\n\n\nnn.GLU\n\n\t\n\nApplies the gated linear unit function \n𝐺\n𝐿\n𝑈\n(\n𝑎\n,\n𝑏\n)\n=\n𝑎\n⊗\n𝜎\n(\n𝑏\n)\nGLU(a,b)=a⊗σ(b) where \n𝑎\na is the first half of the input matrices and \n𝑏\nb is the second half.\n\nNon-linear Activations (other)\n\nnn.Softmin\n\n\t\n\nApplies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.\n\n\n\n\nnn.Softmax\n\n\t\n\nApplies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n\n\n\n\nnn.Softmax2d\n\n\t\n\nApplies SoftMax over features to each spatial location.\n\n\n\n\nnn.LogSoftmax\n\n\t\n\nApplies the \nlog\n⁡\n(\nSoftmax\n(\n𝑥\n)\n)\nlog(Softmax(x)) function to an n-dimensional input Tensor.\n\n\n\n\nnn.AdaptiveLogSoftmaxWithLoss\n\n\t\n\nEfficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou.\n\nNormalization Layers\n\nnn.BatchNorm1d\n\n\t\n\nApplies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n\n\n\nnn.BatchNorm2d\n\n\t\n\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n\n\n\nnn.BatchNorm3d\n\n\t\n\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n\n\n\nnn.LazyBatchNorm1d\n\n\t\n\nA torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyBatchNorm2d\n\n\t\n\nA torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyBatchNorm3d\n\n\t\n\nA torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1).\n\n\n\n\nnn.GroupNorm\n\n\t\n\nApplies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization\n\n\n\n\nnn.SyncBatchNorm\n\n\t\n\nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n\n\n\nnn.InstanceNorm1d\n\n\t\n\nApplies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n\n\n\n\nnn.InstanceNorm2d\n\n\t\n\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n\n\n\n\nnn.InstanceNorm3d\n\n\t\n\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n\n\n\n\nnn.LazyInstanceNorm1d\n\n\t\n\nA torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyInstanceNorm2d\n\n\t\n\nA torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1).\n\n\n\n\nnn.LazyInstanceNorm3d\n\n\t\n\nA torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1).\n\n\n\n\nnn.LayerNorm\n\n\t\n\nApplies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization\n\n\n\n\nnn.LocalResponseNorm\n\n\t\n\nApplies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.\n\nRecurrent Layers\n\nnn.RNNBase\n\n\t\n\nBase class for RNN modules (RNN, LSTM, GRU).\n\n\n\n\nnn.RNN\n\n\t\n\nApplies a multi-layer Elman RNN with \ntanh\n⁡\ntanh or \nReLU\nReLU non-linearity to an input sequence.\n\n\n\n\nnn.LSTM\n\n\t\n\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\n\n\n\nnn.GRU\n\n\t\n\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n\n\n\nnn.RNNCell\n\n\t\n\nAn Elman RNN cell with tanh or ReLU non-linearity.\n\n\n\n\nnn.LSTMCell\n\n\t\n\nA long short-term memory (LSTM) cell.\n\n\n\n\nnn.GRUCell\n\n\t\n\nA gated recurrent unit (GRU) cell\n\nTransformer Layers\n\nnn.Transformer\n\n\t\n\nA transformer model.\n\n\n\n\nnn.TransformerEncoder\n\n\t\n\nTransformerEncoder is a stack of N encoder layers.\n\n\n\n\nnn.TransformerDecoder\n\n\t\n\nTransformerDecoder is a stack of N decoder layers\n\n\n\n\nnn.TransformerEncoderLayer\n\n\t\n\nTransformerEncoderLayer is made up of self-attn and feedforward network.\n\n\n\n\nnn.TransformerDecoderLayer\n\n\t\n\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n\nLinear Layers\n\nnn.Identity\n\n\t\n\nA placeholder identity operator that is argument-insensitive.\n\n\n\n\nnn.Linear\n\n\t\n\nApplies a linear transformation to the incoming data: \n𝑦\n=\n𝑥\n𝐴\n𝑇\n+\n𝑏\ny=xA\nT\n+b\n\n\n\n\nnn.Bilinear\n\n\t\n\nApplies a bilinear transformation to the incoming data: \n𝑦\n=\n𝑥\n1\n𝑇\n𝐴\n𝑥\n2\n+\n𝑏\ny=x\n1\nT\n\t​\n\nAx\n2\n\t​\n\n+b\n\n\n\n\nnn.LazyLinear\n\n\t\n\nA torch.nn.Linear module where in_features is inferred.\n\nDropout Layers\n\nnn.Dropout\n\n\t\n\nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n\n\n\n\nnn.Dropout1d\n\n\t\n\nRandomly zero out entire channels (a channel is a 1D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 1D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]).\n\n\n\n\nnn.Dropout2d\n\n\t\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 2D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]).\n\n\n\n\nnn.Dropout3d\n\n\t\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 3D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]).\n\n\n\n\nnn.AlphaDropout\n\n\t\n\nApplies Alpha Dropout over the input.\n\n\n\n\nnn.FeatureAlphaDropout\n\n\t\n\nRandomly masks out entire channels (a channel is a feature map, e.g.\n\nSparse Layers\n\nnn.Embedding\n\n\t\n\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\n\n\n\nnn.EmbeddingBag\n\n\t\n\nComputes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\n\nDistance Functions\n\nnn.CosineSimilarity\n\n\t\n\nReturns cosine similarity between \n𝑥\n1\nx\n1\n\t​\n\n and \n𝑥\n2\nx\n2\n\t​\n\n, computed along dim.\n\n\n\n\nnn.PairwiseDistance\n\n\t\n\nComputes the pairwise distance between input vectors, or between columns of input matrices.\n\nLoss Functions\n\nnn.L1Loss\n\n\t\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input \n𝑥\nx and target \n𝑦\ny.\n\n\n\n\nnn.MSELoss\n\n\t\n\nCreates a criterion that measures the mean squared error (squared L2 norm) between each element in the input \n𝑥\nx and target \n𝑦\ny.\n\n\n\n\nnn.CrossEntropyLoss\n\n\t\n\nThis criterion computes the cross entropy loss between input logits and target.\n\n\n\n\nnn.CTCLoss\n\n\t\n\nThe Connectionist Temporal Classification loss.\n\n\n\n\nnn.NLLLoss\n\n\t\n\nThe negative log likelihood loss.\n\n\n\n\nnn.PoissonNLLLoss\n\n\t\n\nNegative log likelihood loss with Poisson distribution of target.\n\n\n\n\nnn.GaussianNLLLoss\n\n\t\n\nGaussian negative log likelihood loss.\n\n\n\n\nnn.KLDivLoss\n\n\t\n\nThe Kullback-Leibler divergence loss.\n\n\n\n\nnn.BCELoss\n\n\t\n\nCreates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n\n\n\n\nnn.BCEWithLogitsLoss\n\n\t\n\nThis loss combines a Sigmoid layer and the BCELoss in one single class.\n\n\n\n\nnn.MarginRankingLoss\n\n\t\n\nCreates a criterion that measures the loss given inputs \n𝑥\n1\nx1, \n𝑥\n2\nx2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor \n𝑦\ny (containing 1 or -1).\n\n\n\n\nnn.HingeEmbeddingLoss\n\n\t\n\nMeasures the loss given an input tensor \n𝑥\nx and a labels tensor \n𝑦\ny (containing 1 or -1).\n\n\n\n\nnn.MultiLabelMarginLoss\n\n\t\n\nCreates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input \n𝑥\nx (a 2D mini-batch Tensor) and output \n𝑦\ny (which is a 2D Tensor of target class indices).\n\n\n\n\nnn.HuberLoss\n\n\t\n\nCreates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.\n\n\n\n\nnn.SmoothL1Loss\n\n\t\n\nCreates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.\n\n\n\n\nnn.SoftMarginLoss\n\n\t\n\nCreates a criterion that optimizes a two-class classification logistic loss between input tensor \n𝑥\nx and target tensor \n𝑦\ny (containing 1 or -1).\n\n\n\n\nnn.MultiLabelSoftMarginLoss\n\n\t\n\nCreates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input \n𝑥\nx and target \n𝑦\ny of size \n(\n𝑁\n,\n𝐶\n)\n(N,C).\n\n\n\n\nnn.CosineEmbeddingLoss\n\n\t\n\nCreates a criterion that measures the loss given input tensors \n𝑥\n1\nx\n1\n\t​\n\n, \n𝑥\n2\nx\n2\n\t​\n\n and a Tensor label \n𝑦\ny with values 1 or -1.\n\n\n\n\nnn.MultiMarginLoss\n\n\t\n\nCreates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input \n𝑥\nx (a 2D mini-batch Tensor) and output \n𝑦\ny (which is a 1D tensor of target class indices, \n0\n≤\n𝑦\n≤\nx.size\n(\n1\n)\n−\n1\n0≤y≤x.size(1)−1):\n\n\n\n\nnn.TripletMarginLoss\n\n\t\n\nCreates a criterion that measures the triplet loss given an input tensors \n𝑥\n1\nx1, \n𝑥\n2\nx2, \n𝑥\n3\nx3 and a margin with a value greater than \n0\n0.\n\n\n\n\nnn.TripletMarginWithDistanceLoss\n\n\t\n\nCreates a criterion that measures the triplet loss given input tensors \n𝑎\na, \n𝑝\np, and \n𝑛\nn (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nVision Layers\n\nnn.PixelShuffle\n\n\t\n\nRearranges elements in a tensor of shape \n(\n∗\n,\n𝐶\n×\n𝑟\n2\n,\n𝐻\n,\n𝑊\n)\n(∗,C×r\n2\n,H,W) to a tensor of shape \n(\n∗\n,\n𝐶\n,\n𝐻\n×\n𝑟\n,\n𝑊\n×\n𝑟\n)\n(∗,C,H×r,W×r), where r is an upscale factor.\n\n\n\n\nnn.PixelUnshuffle\n\n\t\n\nReverses the PixelShuffle operation by rearranging elements in a tensor of shape \n(\n∗\n,\n𝐶\n,\n𝐻\n×\n𝑟\n,\n𝑊\n×\n𝑟\n)\n(∗,C,H×r,W×r) to a tensor of shape \n(\n∗\n,\n𝐶\n×\n𝑟\n2\n,\n𝐻\n,\n𝑊\n)\n(∗,C×r\n2\n,H,W), where r is a downscale factor.\n\n\n\n\nnn.Upsample\n\n\t\n\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\n\n\n\n\nnn.UpsamplingNearest2d\n\n\t\n\nApplies a 2D nearest neighbor upsampling to an input signal composed of several input channels.\n\n\n\n\nnn.UpsamplingBilinear2d\n\n\t\n\nApplies a 2D bilinear upsampling to an input signal composed of several input channels.\n\nShuffle Layers\n\nnn.ChannelShuffle\n\n\t\n\nDivide the channels in a tensor of shape \n(\n∗\n,\n𝐶\n,\n𝐻\n,\n𝑊\n)\n(∗,C,H,W) into g groups and rearrange them as \n(\n∗\n,\n𝐶\n𝑔\n,\n𝑔\n,\n𝐻\n,\n𝑊\n)\n(∗,C\n,\ng\n\t​\n\ng,H,W), while keeping the original tensor shape.\n\nDataParallel Layers (multi-GPU, distributed)\n\nnn.DataParallel\n\n\t\n\nImplements data parallelism at the module level.\n\n\n\n\nnn.parallel.DistributedDataParallel\n\n\t\n\nImplements distributed data parallelism that is based on torch.distributed package at the module level.\n\nUtilities\n\nFrom the torch.nn.utils module\n\nclip_grad_norm_\n\n\t\n\nClips gradient norm of an iterable of parameters.\n\n\n\n\nclip_grad_value_\n\n\t\n\nClips gradient of an iterable of parameters at specified value.\n\n\n\n\nparameters_to_vector\n\n\t\n\nConvert parameters to one vector\n\n\n\n\nvector_to_parameters\n\n\t\n\nConvert one vector to the parameters\n\n\n\n\nprune.BasePruningMethod\n\n\t\n\nAbstract base class for creation of new pruning techniques.\n\nprune.PruningContainer\n\n\t\n\nContainer holding a sequence of pruning methods for iterative pruning.\n\n\n\n\nprune.Identity\n\n\t\n\nUtility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.\n\n\n\n\nprune.RandomUnstructured\n\n\t\n\nPrune (currently unpruned) units in a tensor at random.\n\n\n\n\nprune.L1Unstructured\n\n\t\n\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.\n\n\n\n\nprune.RandomStructured\n\n\t\n\nPrune entire (currently unpruned) channels in a tensor at random.\n\n\n\n\nprune.LnStructured\n\n\t\n\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n\n\n\n\nprune.CustomFromMask\n\n\t\n\n\n\n\nprune.identity\n\n\t\n\nApplies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units.\n\n\n\n\nprune.random_unstructured\n\n\t\n\nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random.\n\n\n\n\nprune.l1_unstructured\n\n\t\n\nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm.\n\n\n\n\nprune.random_structured\n\n\t\n\nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random.\n\n\n\n\nprune.ln_structured\n\n\t\n\nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest Ln-norm.\n\n\n\n\nprune.global_unstructured\n\n\t\n\nGlobally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.\n\n\n\n\nprune.custom_from_mask\n\n\t\n\nPrunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.\n\n\n\n\nprune.remove\n\n\t\n\nRemoves the pruning reparameterization from a module and the pruning method from the forward hook.\n\n\n\n\nprune.is_pruned\n\n\t\n\nCheck whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.\n\n\n\n\nweight_norm\n\n\t\n\nApplies weight normalization to a parameter in the given module.\n\n\n\n\nremove_weight_norm\n\n\t\n\nRemoves the weight normalization reparameterization from a module.\n\n\n\n\nspectral_norm\n\n\t\n\nApplies spectral normalization to a parameter in the given module.\n\n\n\n\nremove_spectral_norm\n\n\t\n\nRemoves the spectral normalization reparameterization from a module.\n\n\n\n\nskip_init\n\n\t\n\nGiven a module class object and args / kwargs, instantiates the module without initializing parameters / buffers.\n\nParametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization().\n\nparametrizations.orthogonal\n\n\t\n\nApplies an orthogonal or unitary parametrization to a matrix or a batch of matrices.\n\n\n\n\nparametrizations.spectral_norm\n\n\t\n\nApplies spectral normalization to a parameter in the given module.\n\nUtility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations.\n\nparametrize.register_parametrization\n\n\t\n\nAdds a parametrization to a tensor in a module.\n\n\n\n\nparametrize.remove_parametrizations\n\n\t\n\nRemoves the parametrizations on a tensor in a module.\n\n\n\n\nparametrize.cached\n\n\t\n\nContext manager that enables the caching system within parametrizations registered with register_parametrization().\n\n\n\n\nparametrize.is_parametrized\n\n\t\n\nReturns True if module has an active parametrization.\n\nparametrize.ParametrizationList\n\n\t\n\nA sequential container that holds and manages the original or original0, original1, .\n\nUtility functions to calls a given Module in a stateless manner.\n\nstateless.functional_call\n\n\t\n\nPerforms a functional call on the module by replacing the module parameters and buffers with the provided ones.\n\nUtility functions in other modules\n\nnn.utils.rnn.PackedSequence\n\n\t\n\nHolds the data and list of batch_sizes of a packed sequence.\n\n\n\n\nnn.utils.rnn.pack_padded_sequence\n\n\t\n\nPacks a Tensor containing padded sequences of variable length.\n\n\n\n\nnn.utils.rnn.pad_packed_sequence\n\n\t\n\nPads a packed batch of variable length sequences.\n\n\n\n\nnn.utils.rnn.pad_sequence\n\n\t\n\nPad a list of variable length Tensors with padding_value\n\n\n\n\nnn.utils.rnn.pack_sequence\n\n\t\n\nPacks a list of variable length Tensors\n\n\n\n\nnn.utils.rnn.unpack_sequence\n\n\t\n\nUnpacks PackedSequence into a list of variable length Tensors\n\n\n\n\nnn.utils.rnn.unpad_sequence\n\n\t\n\nUnpad padded Tensor into a list of variable length Tensors\n\nnn.Flatten\n\n\t\n\nFlattens a contiguous range of dims into a tensor.\n\n\n\n\nnn.Unflatten\n\n\t\n\nUnflattens a tensor dim expanding it to a desired shape.\n\nQuantized Functions\n\nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.\n\nLazy Modules Initialization\n\nnn.modules.lazy.LazyModuleMixin\n\n\t\n\nA mixin for modules that lazily initialize parameters, also known as \"lazy modules.\"\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.nn\nContainers\nConvolution Layers\nPooling layers\nPadding Layers\nNon-linear Activations (weighted sum, nonlinearity)\nNon-linear Activations (other)\nNormalization Layers\nRecurrent Layers\nTransformer Layers\nLinear Layers\nDropout Layers\nSparse Layers\nDistance Functions\nLoss Functions\nVision Layers\nShuffle Layers\nDataParallel Layers (multi-GPU, distributed)\nUtilities\nQuantized Functions\nLazy Modules Initialization\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch.nn.functional — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/nn.functional.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch.nn.functional\nShortcuts\nTORCH.NN.FUNCTIONAL\nConvolution functions\n\nconv1d\n\n\t\n\nApplies a 1D convolution over an input signal composed of several input planes.\n\n\n\n\nconv2d\n\n\t\n\nApplies a 2D convolution over an input image composed of several input planes.\n\n\n\n\nconv3d\n\n\t\n\nApplies a 3D convolution over an input image composed of several input planes.\n\n\n\n\nconv_transpose1d\n\n\t\n\nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".\n\n\n\n\nconv_transpose2d\n\n\t\n\nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".\n\n\n\n\nconv_transpose3d\n\n\t\n\nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"\n\n\n\n\nunfold\n\n\t\n\nExtracts sliding local blocks from a batched input tensor.\n\n\n\n\nfold\n\n\t\n\nCombines an array of sliding local blocks into a large containing tensor.\n\nPooling functions\n\navg_pool1d\n\n\t\n\nApplies a 1D average pooling over an input signal composed of several input planes.\n\n\n\n\navg_pool2d\n\n\t\n\nApplies 2D average-pooling operation in \n𝑘\n𝐻\n×\n𝑘\n𝑊\nkH×kW regions by step size \n𝑠\n𝐻\n×\n𝑠\n𝑊\nsH×sW steps.\n\n\n\n\navg_pool3d\n\n\t\n\nApplies 3D average-pooling operation in \n𝑘\n𝑇\n×\n𝑘\n𝐻\n×\n𝑘\n𝑊\nkT×kH×kW regions by step size \n𝑠\n𝑇\n×\n𝑠\n𝐻\n×\n𝑠\n𝑊\nsT×sH×sW steps.\n\n\n\n\nmax_pool1d\n\n\t\n\nApplies a 1D max pooling over an input signal composed of several input planes.\n\n\n\n\nmax_pool2d\n\n\t\n\nApplies a 2D max pooling over an input signal composed of several input planes.\n\n\n\n\nmax_pool3d\n\n\t\n\nApplies a 3D max pooling over an input signal composed of several input planes.\n\n\n\n\nmax_unpool1d\n\n\t\n\nComputes a partial inverse of MaxPool1d.\n\n\n\n\nmax_unpool2d\n\n\t\n\nComputes a partial inverse of MaxPool2d.\n\n\n\n\nmax_unpool3d\n\n\t\n\nComputes a partial inverse of MaxPool3d.\n\n\n\n\nlp_pool1d\n\n\t\n\nApplies a 1D power-average pooling over an input signal composed of several input planes.\n\n\n\n\nlp_pool2d\n\n\t\n\nApplies a 2D power-average pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_max_pool1d\n\n\t\n\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_max_pool2d\n\n\t\n\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_max_pool3d\n\n\t\n\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_avg_pool1d\n\n\t\n\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_avg_pool2d\n\n\t\n\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n\n\n\nadaptive_avg_pool3d\n\n\t\n\nApplies a 3D adaptive average pooling over an input signal composed of several input planes.\n\n\n\n\nfractional_max_pool2d\n\n\t\n\nApplies 2D fractional max pooling over an input signal composed of several input planes.\n\n\n\n\nfractional_max_pool3d\n\n\t\n\nApplies 3D fractional max pooling over an input signal composed of several input planes.\n\nAttention Mechanisms\n\nscaled_dot_product_attention\n\n\t\n\nComputes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.\n\nNon-linear activation functions\n\nthreshold\n\n\t\n\nThresholds each element of the input Tensor.\n\n\n\n\nthreshold_\n\n\t\n\nIn-place version of threshold().\n\n\n\n\nrelu\n\n\t\n\nApplies the rectified linear unit function element-wise.\n\n\n\n\nrelu_\n\n\t\n\nIn-place version of relu().\n\n\n\n\nhardtanh\n\n\t\n\nApplies the HardTanh function element-wise.\n\n\n\n\nhardtanh_\n\n\t\n\nIn-place version of hardtanh().\n\n\n\n\nhardswish\n\n\t\n\nApplies the hardswish function, element-wise, as described in the paper:\n\n\n\n\nrelu6\n\n\t\n\nApplies the element-wise function \nReLU6\n(\n𝑥\n)\n=\nmin\n⁡\n(\nmax\n⁡\n(\n0\n,\n𝑥\n)\n,\n6\n)\nReLU6(x)=min(max(0,x),6).\n\n\n\n\nelu\n\n\t\n\nApplies the Exponential Linear Unit (ELU) function element-wise.\n\n\n\n\nelu_\n\n\t\n\nIn-place version of elu().\n\n\n\n\nselu\n\n\t\n\nApplies element-wise, \nSELU\n(\n𝑥\n)\n=\n𝑠\n𝑐\n𝑎\n𝑙\n𝑒\n∗\n(\nmax\n⁡\n(\n0\n,\n𝑥\n)\n+\nmin\n⁡\n(\n0\n,\n𝛼\n∗\n(\nexp\n⁡\n(\n𝑥\n)\n−\n1\n)\n)\n)\nSELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1))), with \n𝛼\n=\n1.6732632423543772848170429916717\nα=1.6732632423543772848170429916717 and \n𝑠\n𝑐\n𝑎\n𝑙\n𝑒\n=\n1.0507009873554804934193349852946\nscale=1.0507009873554804934193349852946.\n\n\n\n\ncelu\n\n\t\n\nApplies element-wise, \nCELU\n(\n𝑥\n)\n=\nmax\n⁡\n(\n0\n,\n𝑥\n)\n+\nmin\n⁡\n(\n0\n,\n𝛼\n∗\n(\nexp\n⁡\n(\n𝑥\n/\n𝛼\n)\n−\n1\n)\n)\nCELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1)).\n\n\n\n\nleaky_relu\n\n\t\n\nApplies element-wise, \nLeakyReLU\n(\n𝑥\n)\n=\nmax\n⁡\n(\n0\n,\n𝑥\n)\n+\nnegative_slope\n∗\nmin\n⁡\n(\n0\n,\n𝑥\n)\nLeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)\n\n\n\n\nleaky_relu_\n\n\t\n\nIn-place version of leaky_relu().\n\n\n\n\nprelu\n\n\t\n\nApplies element-wise the function \nPReLU\n(\n𝑥\n)\n=\nmax\n⁡\n(\n0\n,\n𝑥\n)\n+\nweight\n∗\nmin\n⁡\n(\n0\n,\n𝑥\n)\nPReLU(x)=max(0,x)+weight∗min(0,x) where weight is a learnable parameter.\n\n\n\n\nrrelu\n\n\t\n\nRandomized leaky ReLU.\n\n\n\n\nrrelu_\n\n\t\n\nIn-place version of rrelu().\n\n\n\n\nglu\n\n\t\n\nThe gated linear unit.\n\n\n\n\ngelu\n\n\t\n\nWhen the approximate argument is 'none', it applies element-wise the function \nGELU\n(\n𝑥\n)\n=\n𝑥\n∗\nΦ\n(\n𝑥\n)\nGELU(x)=x∗Φ(x)\n\n\n\n\nlogsigmoid\n\n\t\n\nApplies element-wise \nLogSigmoid\n(\n𝑥\n𝑖\n)\n=\nlog\n⁡\n(\n1\n1\n+\nexp\n⁡\n(\n−\n𝑥\n𝑖\n)\n)\nLogSigmoid(x\ni\n\t​\n\n)=log(\n1+exp(−x\ni\n\t​\n\n)\n1\n\t​\n\n)\n\n\n\n\nhardshrink\n\n\t\n\nApplies the hard shrinkage function element-wise\n\n\n\n\ntanhshrink\n\n\t\n\nApplies element-wise, \nTanhshrink\n(\n𝑥\n)\n=\n𝑥\n−\nTanh\n(\n𝑥\n)\nTanhshrink(x)=x−Tanh(x)\n\n\n\n\nsoftsign\n\n\t\n\nApplies element-wise, the function \nSoftSign\n(\n𝑥\n)\n=\n𝑥\n1\n+\n∣\n𝑥\n∣\nSoftSign(x)=\n1+∣x∣\nx\n\t​\n\n\n\n\n\nsoftplus\n\n\t\n\nApplies element-wise, the function \nSoftplus\n(\n𝑥\n)\n=\n1\n𝛽\n∗\nlog\n⁡\n(\n1\n+\nexp\n⁡\n(\n𝛽\n∗\n𝑥\n)\n)\nSoftplus(x)=\nβ\n1\n\t​\n\n∗log(1+exp(β∗x)).\n\n\n\n\nsoftmin\n\n\t\n\nApplies a softmin function.\n\n\n\n\nsoftmax\n\n\t\n\nApplies a softmax function.\n\n\n\n\nsoftshrink\n\n\t\n\nApplies the soft shrinkage function elementwise\n\n\n\n\ngumbel_softmax\n\n\t\n\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.\n\n\n\n\nlog_softmax\n\n\t\n\nApplies a softmax followed by a logarithm.\n\n\n\n\ntanh\n\n\t\n\nApplies element-wise, \nTanh\n(\n𝑥\n)\n=\ntanh\n⁡\n(\n𝑥\n)\n=\nexp\n⁡\n(\n𝑥\n)\n−\nexp\n⁡\n(\n−\n𝑥\n)\nexp\n⁡\n(\n𝑥\n)\n+\nexp\n⁡\n(\n−\n𝑥\n)\nTanh(x)=tanh(x)=\nexp(x)+exp(−x)\nexp(x)−exp(−x)\n\t​\n\n\n\n\n\nsigmoid\n\n\t\n\nApplies the element-wise function \nSigmoid\n(\n𝑥\n)\n=\n1\n1\n+\nexp\n⁡\n(\n−\n𝑥\n)\nSigmoid(x)=\n1+exp(−x)\n1\n\t​\n\n\n\n\n\nhardsigmoid\n\n\t\n\nApplies the element-wise function\n\n\n\n\nsilu\n\n\t\n\nApplies the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n\n\n\nmish\n\n\t\n\nApplies the Mish function, element-wise.\n\n\n\n\nbatch_norm\n\n\t\n\nApplies Batch Normalization for each channel across a batch of data.\n\n\n\n\ngroup_norm\n\n\t\n\nApplies Group Normalization for last certain number of dimensions.\n\n\n\n\ninstance_norm\n\n\t\n\nApplies Instance Normalization for each channel in each data sample in a batch.\n\n\n\n\nlayer_norm\n\n\t\n\nApplies Layer Normalization for last certain number of dimensions.\n\n\n\n\nlocal_response_norm\n\n\t\n\nApplies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.\n\n\n\n\nnormalize\n\n\t\n\nPerforms \n𝐿\n𝑝\nL\np\n\t​\n\n normalization of inputs over specified dimension.\n\nLinear functions\n\nlinear\n\n\t\n\nApplies a linear transformation to the incoming data: \n𝑦\n=\n𝑥\n𝐴\n𝑇\n+\n𝑏\ny=xA\nT\n+b.\n\n\n\n\nbilinear\n\n\t\n\nApplies a bilinear transformation to the incoming data: \n𝑦\n=\n𝑥\n1\n𝑇\n𝐴\n𝑥\n2\n+\n𝑏\ny=x\n1\nT\n\t​\n\nAx\n2\n\t​\n\n+b\n\nDropout functions\n\ndropout\n\n\t\n\nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n\n\n\n\nalpha_dropout\n\n\t\n\nApplies alpha dropout to the input.\n\n\n\n\nfeature_alpha_dropout\n\n\t\n\nRandomly masks out entire channels (a channel is a feature map, e.g.\n\n\n\n\ndropout1d\n\n\t\n\nRandomly zero out entire channels (a channel is a 1D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 1D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]) of the input tensor).\n\n\n\n\ndropout2d\n\n\t\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 2D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]) of the input tensor).\n\n\n\n\ndropout3d\n\n\t\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the \n𝑗\nj-th channel of the \n𝑖\ni-th sample in the batched input is a 3D tensor \ninput\n[\n𝑖\n,\n𝑗\n]\ninput[i,j]) of the input tensor).\n\nSparse functions\n\nembedding\n\n\t\n\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n\n\n\nembedding_bag\n\n\t\n\nComputes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.\n\n\n\n\none_hot\n\n\t\n\nTakes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.\n\nDistance functions\n\npairwise_distance\n\n\t\n\nSee torch.nn.PairwiseDistance for details\n\n\n\n\ncosine_similarity\n\n\t\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\n\n\n\npdist\n\n\t\n\nComputes the p-norm distance between every pair of row vectors in the input.\n\nLoss functions\n\nbinary_cross_entropy\n\n\t\n\nFunction that measures the Binary Cross Entropy between the target and input probabilities.\n\n\n\n\nbinary_cross_entropy_with_logits\n\n\t\n\nFunction that measures Binary Cross Entropy between target and input logits.\n\n\n\n\npoisson_nll_loss\n\n\t\n\nPoisson negative log likelihood loss.\n\n\n\n\ncosine_embedding_loss\n\n\t\n\nSee CosineEmbeddingLoss for details.\n\n\n\n\ncross_entropy\n\n\t\n\nThis criterion computes the cross entropy loss between input logits and target.\n\n\n\n\nctc_loss\n\n\t\n\nThe Connectionist Temporal Classification loss.\n\n\n\n\ngaussian_nll_loss\n\n\t\n\nGaussian negative log likelihood loss.\n\n\n\n\nhinge_embedding_loss\n\n\t\n\nSee HingeEmbeddingLoss for details.\n\n\n\n\nkl_div\n\n\t\n\nThe Kullback-Leibler divergence Loss\n\n\n\n\nl1_loss\n\n\t\n\nFunction that takes the mean element-wise absolute value difference.\n\n\n\n\nmse_loss\n\n\t\n\nMeasures the element-wise mean squared error.\n\n\n\n\nmargin_ranking_loss\n\n\t\n\nSee MarginRankingLoss for details.\n\n\n\n\nmultilabel_margin_loss\n\n\t\n\nSee MultiLabelMarginLoss for details.\n\n\n\n\nmultilabel_soft_margin_loss\n\n\t\n\nSee MultiLabelSoftMarginLoss for details.\n\n\n\n\nmulti_margin_loss\n\n\t\n\nSee MultiMarginLoss for details.\n\n\n\n\nnll_loss\n\n\t\n\nThe negative log likelihood loss.\n\n\n\n\nhuber_loss\n\n\t\n\nFunction that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.\n\n\n\n\nsmooth_l1_loss\n\n\t\n\nFunction that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.\n\n\n\n\nsoft_margin_loss\n\n\t\n\nSee SoftMarginLoss for details.\n\n\n\n\ntriplet_margin_loss\n\n\t\n\nSee TripletMarginLoss for details\n\n\n\n\ntriplet_margin_with_distance_loss\n\n\t\n\nSee TripletMarginWithDistanceLoss for details.\n\nVision functions\n\npixel_shuffle\n\n\t\n\nRearranges elements in a tensor of shape \n(\n∗\n,\n𝐶\n×\n𝑟\n2\n,\n𝐻\n,\n𝑊\n)\n(∗,C×r\n2\n,H,W) to a tensor of shape \n(\n∗\n,\n𝐶\n,\n𝐻\n×\n𝑟\n,\n𝑊\n×\n𝑟\n)\n(∗,C,H×r,W×r), where r is the upscale_factor.\n\n\n\n\npixel_unshuffle\n\n\t\n\nReverses the PixelShuffle operation by rearranging elements in a tensor of shape \n(\n∗\n,\n𝐶\n,\n𝐻\n×\n𝑟\n,\n𝑊\n×\n𝑟\n)\n(∗,C,H×r,W×r) to a tensor of shape \n(\n∗\n,\n𝐶\n×\n𝑟\n2\n,\n𝐻\n,\n𝑊\n)\n(∗,C×r\n2\n,H,W), where r is the downscale_factor.\n\n\n\n\npad\n\n\t\n\nPads tensor.\n\n\n\n\ninterpolate\n\n\t\n\nDown/up samples the input to either the given size or the given scale_factor\n\n\n\n\nupsample\n\n\t\n\nUpsamples the input to either the given size or the given scale_factor\n\n\n\n\nupsample_nearest\n\n\t\n\nUpsamples the input, using nearest neighbours' pixel values.\n\n\n\n\nupsample_bilinear\n\n\t\n\nUpsamples the input, using bilinear upsampling.\n\n\n\n\ngrid_sample\n\n\t\n\nGiven an input and a flow-field grid, computes the output using input values and pixel locations from grid.\n\n\n\n\naffine_grid\n\n\t\n\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.\n\nDataParallel functions (multi-GPU, distributed)\ndata_parallel\n\ntorch.nn.parallel.data_parallel\n\n\t\n\nEvaluates module(input) in parallel across the GPUs given in device_ids.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\ntorch.nn.functional\nConvolution functions\nPooling functions\nAttention Mechanisms\nNon-linear activation functions\nLinear functions\nDropout functions\nSparse functions\nDistance functions\nLoss functions\nVision functions\nDataParallel functions (multi-GPU, distributed)\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Automatic Mixed Precision package - torch.amp — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/amp.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Automatic Mixed Precision package - torch.amp\nShortcuts\nAUTOMATIC MIXED PRECISION PACKAGE - TORCH.AMP\n\ntorch.amp provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use lower precision floating point datatype (lower_precision_fp): torch.float16 (half) or torch.bfloat16. Some ops, like linear layers and convolutions, are much faster in lower_precision_fp. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype.\n\nOrdinarily, “automatic mixed precision training” with datatype of torch.float16 uses torch.autocast and torch.cuda.amp.GradScaler together, as shown in the CUDA Automatic Mixed Precision examples and CUDA Automatic Mixed Precision recipe. However, torch.autocast and torch.cuda.amp.GradScaler are modular, and may be used separately if desired. As shown in the CPU example section of torch.autocast, “automatic mixed precision training/inference” on CPU with datatype of torch.bfloat16 only uses torch.autocast.\n\nFor CUDA and CPU, APIs are also provided separately:\n\ntorch.autocast(\"cuda\", args...) is equivalent to torch.cuda.amp.autocast(args...).\n\ntorch.autocast(\"cpu\", args...) is equivalent to torch.cpu.amp.autocast(args...). For CPU, only lower precision floating point datatype of torch.bfloat16 is supported for now.\n\ntorch.autocast and torch.cpu.amp.autocast are new in version 1.10.\n\nAutocasting\n\nGradient Scaling\n\nAutocast Op Reference\n\nOp Eligibility\n\nCUDA Op-Specific Behavior\n\nCUDA Ops that can autocast to float16\n\nCUDA Ops that can autocast to float32\n\nCUDA Ops that promote to the widest input type\n\nPrefer binary_cross_entropy_with_logits over binary_cross_entropy\n\nCPU Op-Specific Behavior\n\nCPU Ops that can autocast to bfloat16\n\nCPU Ops that can autocast to float32\n\nCPU Ops that promote to the widest input type\n\nAutocasting\nCLASS\ntorch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)\n[SOURCE]\n\nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.\n\nIn these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.\n\nWhen entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting.\n\nautocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.\n\nExample for CUDA Devices:\n\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n\n\nSee the CUDA Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).\n\nautocast can also be used as a decorator, e.g., on the forward method of your model:\n\nclass AutocastModel(nn.Module):\n    ...\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input):\n        ...\n\n\nFloating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. CUDA Example:\n\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n\n\nCPU Training Example:\n\n# Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step()\n\n\nCPU Inference Example:\n\n# Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input)\n\n\nCPU Inference Example with Jit Trace:\n\nclass TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n    def forward(self, x):\n        return self.fc1(x)\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size))\n\n\nType mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue.\n\nautocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use:\n\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n\n\nThe autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).\n\nParameters\n\ndevice_type (str, required) – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘xpu’ and ‘hpu’. The type is the same as the type attribute of a torch.device. Thus, you may obtain the device type of a tensor using Tensor.device.type.\n\nenabled (bool, optional) – Whether autocasting should be enabled in the region. Default: True\n\ndtype (torch_dtype, optional) – Whether to use torch.float16 or torch.bfloat16.\n\ncache_enabled (bool, optional) – Whether the weight cache inside autocast should be enabled. Default: True\n\nCLASS\ntorch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True)\n[SOURCE]\n\nSee torch.autocast. torch.cuda.amp.autocast(args...) is equivalent to torch.autocast(\"cuda\", args...)\n\ntorch.cuda.amp.custom_fwd(fwd=None, *, cast_inputs=None)\n[SOURCE]\n\nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.\n\nParameters\n\ncast_inputs (torch.dtype or None, optional, default=None) – If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward’s internal ops execute with the current autocast state.\n\nNOTE\n\nIf the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.\n\ntorch.cuda.amp.custom_bwd(bwd)\n[SOURCE]\n\nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail.\n\nCLASS\ntorch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=True)\n[SOURCE]\n\nSee torch.autocast. torch.cpu.amp.autocast(args...) is equivalent to torch.autocast(\"cpu\", args...)\n\nGradient Scaling\n\nIf the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\n\nTo prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\n\nEach parameter’s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.\n\nNOTE\n\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss or gradients when running with AMP/fp16, verify your model is compatible.\n\nCLASS\ntorch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)\n[SOURCE]\nget_backoff_factor()\n[SOURCE]\n\nReturns a Python float containing the scale backoff factor.\n\nget_growth_factor()\n[SOURCE]\n\nReturns a Python float containing the scale growth factor.\n\nget_growth_interval()\n[SOURCE]\n\nReturns a Python int containing the growth interval.\n\nget_scale()\n[SOURCE]\n\nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.\n\nWARNING\n\nget_scale() incurs a CPU-GPU sync.\n\nis_enabled()\n[SOURCE]\n\nReturns a bool indicating whether this instance is enabled.\n\nload_state_dict(state_dict)\n[SOURCE]\n\nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.\n\nParameters\n\nstate_dict (dict) – scaler state. Should be an object returned from a call to state_dict().\n\nscale(outputs)\n[SOURCE]\n\nMultiplies (‘scales’) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n\nParameters\n\noutputs (Tensor or iterable of Tensors) – Outputs to scale.\n\nset_backoff_factor(new_factor)\n[SOURCE]\nParameters\n\nnew_scale (float) – Value to use as the new scale backoff factor.\n\nset_growth_factor(new_factor)\n[SOURCE]\nParameters\n\nnew_scale (float) – Value to use as the new scale growth factor.\n\nset_growth_interval(new_interval)\n[SOURCE]\nParameters\n\nnew_interval (int) – Value to use as the new growth interval.\n\nstate_dict()\n[SOURCE]\n\nReturns the state of the scaler as a dict. It contains five entries:\n\n\"scale\" - a Python float containing the current scale\n\n\"growth_factor\" - a Python float containing the current growth factor\n\n\"backoff_factor\" - a Python float containing the current backoff factor\n\n\"growth_interval\" - a Python int containing the current growth interval\n\n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.\n\nIf this instance is not enabled, returns an empty dict.\n\nNOTE\n\nIf you wish to checkpoint the scaler’s state after a particular iteration, state_dict() should be called after update().\n\nstep(optimizer, *args, **kwargs)\n[SOURCE]\n\nstep() carries out the following two operations:\n\nInternally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs.\n\nIf no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.\n\n*args and **kwargs are forwarded to optimizer.step().\n\nReturns the return value of optimizer.step(*args, **kwargs).\n\nParameters\n\noptimizer (torch.optim.Optimizer) – Optimizer that applies the gradients.\n\nargs – Any arguments.\n\nkwargs – Any keyword arguments.\n\nWARNING\n\nClosure use is not currently supported.\n\nunscale_(optimizer)\n[SOURCE]\n\nDivides (“unscales”) the optimizer’s gradient tensors by the scale factor.\n\nunscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().\n\nSimple example, using unscale_() to enable clipping of unscaled gradients:\n\n...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n\nParameters\n\noptimizer (torch.optim.Optimizer) – Optimizer that owns the gradients to be unscaled.\n\nNOTE\n\nunscale_() does not incur a CPU-GPU sync.\n\nWARNING\n\nunscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer’s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.\n\nWARNING\n\nunscale_() may unscale sparse gradients out of place, replacing the .grad attribute.\n\nupdate(new_scale=None)\n[SOURCE]\n\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.\n\nPassing new_scale sets the new scale value manually. (new_scale is not used directly, it’s used to fill GradScaler’s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)\n\nParameters\n\nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) – New scale factor.\n\nWARNING\n\nupdate() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.\n\nWARNING\n\nFor performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges.\n\nAutocast Op Reference\nOp Eligibility\n\nOps that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.\n\nOnly out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.\n\nOps called with an explicit dtype=... argument are not eligible, and will produce output that respects the dtype argument.\n\nCUDA Op-Specific Behavior\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\n\nIf an op is unlisted, we assume it’s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue.\n\nCUDA Ops that can autocast to float16\n\n__matmul__, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, multi_dot, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell\n\nCUDA Ops that can autocast to float32\n\n__pow__, __rdiv__, __rpow__, __rtruediv__, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss\n\nCUDA Ops that promote to the widest input type\n\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32.\n\naddcdiv, addcmul, atan2, bilinear, cross, dot, grid_sample, index_put, scatter_add, tensordot\n\nSome ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.\n\nPrefer binary_cross_entropy_with_logits over binary_cross_entropy\n\nThe backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it) can produce gradients that aren’t representable in float16. In autocast-enabled regions, the forward input may be float16, which means the backward gradient must be representable in float16 (autocasting float16 forward inputs to float32 doesn’t help, because that cast must be reversed in backward). Therefore, binary_cross_entropy and BCELoss raise an error in autocast-enabled regions.\n\nMany models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits() or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits are safe to autocast.\n\nCPU Op-Specific Behavior\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\n\nIf an op is unlisted, we assume it’s numerically stable in bfloat16. If you believe an unlisted op is numerically unstable in bfloat16, please file an issue.\n\nCPU Ops that can autocast to bfloat16\n\nconv1d, conv2d, conv3d, bmm, mm, baddbmm, addmm, addbmm, linear, matmul, _convolution\n\nCPU Ops that can autocast to float32\n\nconv_transpose1d, conv_transpose2d, conv_transpose3d, avg_pool3d, binary_cross_entropy, grid_sampler, grid_sampler_2d, _grid_sampler_2d_cpu_fallback, grid_sampler_3d, polar, prod, quantile, nanquantile, stft, cdist, trace, view_as_complex, cholesky, cholesky_inverse, cholesky_solve, inverse, lu_solve, orgqr, inverse, ormqr, pinverse, max_pool3d, max_unpool2d, max_unpool3d, adaptive_avg_pool3d, reflection_pad1d, reflection_pad2d, replication_pad1d, replication_pad2d, replication_pad3d, mse_loss, ctc_loss, kl_div, multilabel_margin_loss, fft_fft, fft_ifft, fft_fft2, fft_ifft2, fft_fftn, fft_ifftn, fft_rfft, fft_irfft, fft_rfft2, fft_irfft2, fft_rfftn, fft_irfftn, fft_hfft, fft_ihfft, linalg_matrix_norm, linalg_cond, linalg_matrix_rank, linalg_solve, linalg_cholesky, linalg_svdvals, linalg_eigvals, linalg_eigvalsh, linalg_inv, linalg_householder_product, linalg_tensorinv, linalg_tensorsolve, fake_quantize_per_tensor_affine, eig, geqrf, lstsq, _lu_with_info, qr, solve, svd, symeig, triangular_solve, fractional_max_pool2d, fractional_max_pool3d, adaptive_max_pool3d, multilabel_margin_loss_forward, linalg_qr, linalg_cholesky_ex, linalg_svd, linalg_eig, linalg_eigh, linalg_lstsq, linalg_inv_ex\n\nCPU Ops that promote to the widest input type\n\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are bfloat16, the op runs in bfloat16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32.\n\ncat, stack, index_copy\n\nSome ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of bfloat16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nAutomatic Mixed Precision package - torch.amp\nAutocasting\nGradient Scaling\nAutocast Op Reference\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Tensor Attributes — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/tensor_attributes.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Tensor Attributes\nShortcuts\nTENSOR ATTRIBUTES\n\nEach torch.Tensor has a torch.dtype, torch.device, and torch.layout.\n\ntorch.dtype\nCLASS\ntorch.dtype\n\nA torch.dtype is an object that represents the data type of a torch.Tensor. PyTorch has twelve different data types:\n\nData type\n\n\t\n\ndtype\n\n\t\n\nLegacy Constructors\n\n\n\n\n32-bit floating point\n\n\t\n\ntorch.float32 or torch.float\n\n\t\n\ntorch.*.FloatTensor\n\n\n\n\n64-bit floating point\n\n\t\n\ntorch.float64 or torch.double\n\n\t\n\ntorch.*.DoubleTensor\n\n\n\n\n64-bit complex\n\n\t\n\ntorch.complex64 or torch.cfloat\n\n\t\n\n\n128-bit complex\n\n\t\n\ntorch.complex128 or torch.cdouble\n\n\t\n\n\n16-bit floating point 1\n\n\t\n\ntorch.float16 or torch.half\n\n\t\n\ntorch.*.HalfTensor\n\n\n\n\n16-bit floating point 2\n\n\t\n\ntorch.bfloat16\n\n\t\n\ntorch.*.BFloat16Tensor\n\n\n\n\n8-bit integer (unsigned)\n\n\t\n\ntorch.uint8\n\n\t\n\ntorch.*.ByteTensor\n\n\n\n\n8-bit integer (signed)\n\n\t\n\ntorch.int8\n\n\t\n\ntorch.*.CharTensor\n\n\n\n\n16-bit integer (signed)\n\n\t\n\ntorch.int16 or torch.short\n\n\t\n\ntorch.*.ShortTensor\n\n\n\n\n32-bit integer (signed)\n\n\t\n\ntorch.int32 or torch.int\n\n\t\n\ntorch.*.IntTensor\n\n\n\n\n64-bit integer (signed)\n\n\t\n\ntorch.int64 or torch.long\n\n\t\n\ntorch.*.LongTensor\n\n\n\n\nBoolean\n\n\t\n\ntorch.bool\n\n\t\n\ntorch.*.BoolTensor\n\n1\n\nSometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.\n\n2\n\nSometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as float32\n\nTo find out if a torch.dtype is a floating point data type, the property is_floating_point can be used, which returns True if the data type is a floating point data type.\n\nTo find out if a torch.dtype is a complex data type, the property is_complex can be used, which returns True if the data type is a complex data type.\n\nWhen the dtypes of inputs to an arithmetic operation (add, sub, div, mul) differ, we promote by finding the minimum dtype that satisfies the following rules:\n\nIf the type of a scalar operand is of a higher category than tensor operands (where complex > floating > integral > boolean), we promote to a type with sufficient size to hold all scalar operands of that category.\n\nIf a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.\n\nIf there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.\n\nA floating point scalar operand has dtype torch.get_default_dtype() and an integral non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect values when determining the minimum dtypes of an operand. Quantized and complex types are not yet supported.\n\nPromotion Examples:\n\n>>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n\nWhen the output tensor of an arithmetic operation is specified, we allow casting to its dtype except that:\n\nAn integral output tensor cannot accept a floating point tensor.\n\nA boolean output tensor cannot accept a non-boolean tensor.\n\nA non-complex output tensor cannot accept a complex tensor\n\nCasting Examples:\n\n# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor\n\ntorch.device\nCLASS\ntorch.device\n\nA torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n\nThe torch.device contains a device type ('cpu', 'cuda' or 'mps') and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after torch.cuda.set_device() is called; e.g., a torch.Tensor constructed with device 'cuda' is equivalent to 'cuda:X' where X is the result of torch.cuda.current_device().\n\nA torch.Tensor’s device can be accessed via the Tensor.device property.\n\nA torch.device can be constructed via a string or via a string and device ordinal\n\nVia a string:\n\n>>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('mps')\ndevice(type='mps')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n\n\nVia a string and device ordinal:\n\n>>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('mps', 0)\ndevice(type='mps', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n\n\nThe device object can also be used as a context manager to change the default device tensors are allocated on:\n\n>>> with torch.device('cuda:1'):\n...     r = torch.randn(2, 3)\n>>> r.device\ndevice(type='cuda', index=1)\n\n\nThis context manager has no effect if a factory function is passed an explicit, non-None device argument. To globally change the default device, see also torch.set_default_device().\n\nWARNING\n\nThis function imposes a slight performance cost on every Python call to the torch API (not just factory functions). If this is causing problems for you, please comment on https://github.com/pytorch/pytorch/issues/92701\n\nNOTE\n\nThe torch.device argument in functions can generally be substituted with a string. This allows for fast prototyping of code.\n\n>>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)\n\n>>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')\n\n\nNOTE\n\nFor legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches Tensor.get_device(), which returns an ordinal for cuda tensors and is not supported for cpu tensors.\n\n>>> torch.device(1)\ndevice(type='cuda', index=1)\n\n\nNOTE\n\nMethods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:\n\n>>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n\ntorch.layout\nCLASS\ntorch.layout\n\nWARNING\n\nThe torch.layout class is in beta and subject to change.\n\nA torch.layout is an object that represents the memory layout of a torch.Tensor. Currently, we support torch.strided (dense Tensors) and have beta support for torch.sparse_coo (sparse COO Tensors).\n\ntorch.strided represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated torch.Storage, which holds its data. These tensors provide multi-dimensional, strided view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.\n\nExample:\n\n>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n\n\nFor more information on torch.sparse_coo tensors, see torch.sparse.\n\ntorch.memory_format\nCLASS\ntorch.memory_format\n\nA torch.memory_format is an object representing the memory format on which a torch.Tensor is or will be allocated.\n\nPossible values are:\n\ntorch.contiguous_format: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.\n\ntorch.channels_last: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in strides[0] > strides[2] > strides[3] > strides[1] == 1 aka NHWC order.\n\ntorch.channels_last_3d: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in strides[0] > strides[2] > strides[3] > strides[4] > strides[1] == 1 aka NDHWC order.\n\ntorch.preserve_format: Used in functions like clone to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow torch.contiguous_format\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nTensor Attributes\ntorch.dtype\ntorch.device\ntorch.layout\ntorch.memory_format\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Tensor Views — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/tensor_view.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Tensor Views\nTENSOR VIEWS\n\nPyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.\n\nFor example, to get a view of an existing tensor t, you can call t.view(...).\n\n>>> t = torch.rand(4, 4)\n>>> b = t.view(2, 8)\n>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\nTrue\n# Modifying view tensor changes base tensor as well.\n>>> b[0][0] = 3.14\n>>> t[0][0]\ntensor(3.14)\n\n\nSince views share underlying data with its base tensor, if you edit the data in the view, it will be reflected in the base tensor as well.\n\nTypically a PyTorch op returns a new tensor as output, e.g. add(). But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy. No data movement occurs when creating a view, view tensor just changes the way it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor. Users should pay additional attention as contiguity might have implicit performance impact. transpose() is a common example.\n\n>>> base = torch.tensor([[0, 1],[2, 3]])\n>>> base.is_contiguous()\nTrue\n>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\n# View tensors might be non-contiguous.\n>>> t.is_contiguous()\nFalse\n# To get a contiguous tensor, call `.contiguous()` to enforce\n# copying data when `t` is not contiguous.\n>>> c = t.contiguous()\n\n\nFor reference, here’s a full list of view ops in PyTorch:\n\nBasic slicing and indexing op, e.g. tensor[0, 2:, 1:7:2] returns a view of base tensor, see note below.\n\nadjoint()\n\nas_strided()\n\ndetach()\n\ndiagonal()\n\nexpand()\n\nexpand_as()\n\nmovedim()\n\nnarrow()\n\npermute()\n\nselect()\n\nsqueeze()\n\ntranspose()\n\nt()\n\nT\n\nH\n\nmT\n\nmH\n\nreal\n\nimag\n\nview_as_real()\n\nunflatten()\n\nunfold()\n\nunsqueeze()\n\nview()\n\nview_as()\n\nunbind()\n\nsplit()\n\nhsplit()\n\nvsplit()\n\ntensor_split()\n\nsplit_with_sizes()\n\nswapaxes()\n\nswapdims()\n\nchunk()\n\nindices() (sparse tensor only)\n\nvalues() (sparse tensor only)\n\nNOTE\n\nWhen accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors that basic indexing returns views, while advanced indexing returns a copy. Assignment via either basic or advanced indexing is in-place. See more examples in Numpy indexing documentation.\n\nIt’s also worth mentioning a few ops with special behaviors:\n\nreshape(), reshape_as() and flatten() can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.\n\ncontiguous() returns itself if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.\n\nFor a more detailed walk-through of PyTorch internal implementation, please refer to ezyang’s blogpost about PyTorch Internals.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "torch::deploy has been moved to pytorch/multipy — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/deploy.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > torch::deploy has been moved to pytorch/multipy\nTORCH::DEPLOY HAS BEEN MOVED TO PYTORCH/MULTIPY\n\ntorch::deploy has been moved to its new home at https://github.com/pytorch/multipy.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy."
  },
  {
    "title": "C++ — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/cpp_index.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > C++\nShortcuts\nC++\n\nNOTE\n\nIf you are looking for the PyTorch C++ API docs, directly go here.\n\nPyTorch provides several features for working with C++, and it’s best to choose from them based on your needs. At a high level, the following support is available:\n\nTorchScript C++ API\n\nTorchScript allows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in the Loading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including:\n\nLoading serialized TorchScript models saved from Python\n\nDoing simple model modifications if needed (e.g. pulling out submodules)\n\nConstructing the input and doing preprocessing using C++ Tensor API\n\nExtending PyTorch and TorchScript with C++ Extensions\n\nTorchScript can be augmented with user-supplied code through custom operators and custom classes. Once registered with TorchScript, these operators and classes can be invoked in TorchScript code run from Python or from C++ as part of a serialized TorchScript model. The Extending TorchScript with Custom C++ Operators tutorial walks through interfacing TorchScript with OpenCV. In addition to wrapping a function call with a custom operator, C++ classes and structs can be bound into TorchScript through a pybind11-like interface which is explained in the Extending TorchScript with Custom C++ Classes tutorial.\n\nTensor and Autograd in C++\n\nMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include:\n\ntorch::Tensor methods such as add / reshape / clone. For the full list of methods available, please see: https://pytorch.org/cppdocs/api/classat_1_1_tensor.html\n\nC++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see: https://pytorch.org/cppdocs/notes/tensor_indexing.html\n\nThe tensor autograd APIs and the torch::autograd package that are crucial for building dynamic neural networks in C++ frontend. For more details, please see: https://pytorch.org/tutorials/advanced/cpp_autograd.html\n\nAuthoring Models in C++\n\nThe “author in TorchScript, infer in C++” workflow requires model authoring to be done in TorchScript. However, there might be cases where the model has to be authored in C++ (e.g. in workflows where a Python component is undesirable). To serve such use cases, we provide the full capability of authoring and training a neural net model purely in C++, with familiar components such as torch::nn / torch::nn::functional / torch::optim that closely resemble the Python API.\n\nFor an overview of the PyTorch C++ model authoring and training API, please see: https://pytorch.org/cppdocs/frontend.html\n\nFor a detailed tutorial on how to use the API, please see: https://pytorch.org/tutorials/advanced/cpp_frontend.html\n\nDocs for components such as torch::nn / torch::nn::functional / torch::optim can be found at: https://pytorch.org/cppdocs/api/library_root.html\n\nPackaging for C++\n\nFor guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see: https://pytorch.org/cppdocs/installing.html. Note that on Linux there are two types of libtorch binaries provided: one compiled with GCC pre-cxx11 ABI and the other with GCC cxx11 ABI, and you should make the selection based on the GCC ABI your system is using.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nC++\nTorchScript C++ API\nExtending PyTorch and TorchScript with C++ Extensions\nTensor and Autograd in C++\nAuthoring Models in C++\nPackaging for C++\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Windows FAQ — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/windows.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Windows FAQ\nShortcuts\nWINDOWS FAQ\nBuilding from source\nInclude optional components\n\nThere are two supported components for Windows PyTorch: MKL and MAGMA. Here are the steps to build with them.\n\nREM Make sure you have 7z and curl installed.\n\nREM Download MKL files\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\nREM Download MAGMA files\nREM version available:\nREM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\nREM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)\nREM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nREM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nset CUDA_PREFIX=cuda102\nset CONFIG=release\ncurl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z -o magma.7z\n7z x -aoa magma.7z -omagma\n\nREM Setting essential environment variables\nset \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\"\nset \"LIB=%cd%\\mkl\\lib;%LIB%\"\nset \"MAGMA_HOME=%cd%\\magma\"\n\nSpeeding CUDA build for Windows\n\nVisual Studio doesn’t support parallel custom task currently. As an alternative, we can use Ninja to parallelize CUDA build tasks. It can be used by typing only a few lines of code.\n\nREM Let's install ninja first.\npip install ninja\n\nREM Set it as the cmake generator\nset CMAKE_GENERATOR=Ninja\n\nOne key install script\n\nYou can take a look at this set of scripts. It will lead the way for you.\n\nExtension\nCFFI Extension\n\nThe support for CFFI Extension is very experimental. You must specify additional libraries in Extension object to make it build on Windows.\n\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_compile_args=[\"-std=c99\"],\n    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart\n)\n\nCpp Extension\n\nThis type of extension has better support compared with the previous one. However, it still needs some manual configuration. First, you should open the x86_x64 Cross Tools Command Prompt for VS 2017. And then, you can start your compiling process.\n\nInstallation\nPackage not found in win-32 channel.\nSolving environment: failed\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n- pytorch\n\nCurrent channels:\n- https://conda.anaconda.org/pytorch/win-32\n- https://conda.anaconda.org/pytorch/noarch\n- https://repo.continuum.io/pkgs/main/win-32\n- https://repo.continuum.io/pkgs/main/noarch\n- https://repo.continuum.io/pkgs/free/win-32\n- https://repo.continuum.io/pkgs/free/noarch\n- https://repo.continuum.io/pkgs/r/win-32\n- https://repo.continuum.io/pkgs/r/noarch\n- https://repo.continuum.io/pkgs/pro/win-32\n- https://repo.continuum.io/pkgs/pro/noarch\n- https://repo.continuum.io/pkgs/msys2/win-32\n- https://repo.continuum.io/pkgs/msys2/noarch\n\n\nPyTorch doesn’t work on 32-bit system. Please use Windows and Python 64-bit version.\n\nImport error\nfrom torch._C import *\n\nImportError: DLL load failed: The specified module could not be found.\n\n\nThe problem is caused by the missing of the essential files. Actually, we include almost all the essential files that PyTorch need for the conda package except VC2017 redistributable and some mkl libraries. You can resolve this by typing the following command.\n\nconda install -c peterjc123 vc vs2017_runtime\nconda install mkl_fft intel_openmp numpy mkl\n\n\nAs for the wheels package, since we didn’t pack some libraries and VS2017 redistributable files in, please make sure you install them manually. The VS 2017 redistributable installer can be downloaded. And you should also pay attention to your installation of Numpy. Make sure it uses MKL instead of OpenBLAS. You may type in the following command.\n\npip install numpy mkl intel-openmp mkl_fft\n\n\nAnother possible cause may be you are using GPU version without NVIDIA graphics cards. Please replace your GPU package with the CPU one.\n\nfrom torch._C import *\n\nImportError: DLL load failed: The operating system cannot run %1.\n\n\nThis is actually an upstream issue of Anaconda. When you initialize your environment with conda-forge channel, this issue will emerge. You may fix the intel-openmp libraries through this command.\n\nconda install -c defaults intel-openmp -f\n\nUsage (multiprocessing)\nMultiprocessing error without if-clause protection\nRuntimeError:\n       An attempt has been made to start a new process before the\n       current process has finished its bootstrapping phase.\n\n   This probably means that you are not using fork to start your\n   child processes and you have forgotten to use the proper idiom\n   in the main module:\n\n       if __name__ == '__main__':\n           freeze_support()\n           ...\n\n   The \"freeze_support()\" line can be omitted if the program\n   is not going to be frozen to produce an executable.\n\n\nThe implementation of multiprocessing is different on Windows, which uses spawn instead of fork. So we have to wrap the code with an if-clause to protect the code from executing multiple times. Refactor your code into the following structure.\n\nimport torch\n\ndef main()\n    for i, data in enumerate(dataloader):\n        # do something here\n\nif __name__ == '__main__':\n    main()\n\nMultiprocessing error “Broken pipe”\nForkingPickler(file, protocol).dump(obj)\n\nBrokenPipeError: [Errno 32] Broken pipe\n\n\nThis issue happens when the child process ends before the parent process finishes sending data. There may be something wrong with your code. You can debug your code by reducing the num_worker of DataLoader to zero and see if the issue persists.\n\nMultiprocessing error “driver shut down”\nCouldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n\n[windows] driver shut down\n\n\nPlease update your graphics driver. If this persists, this may be that your graphics card is too old or the calculation is too heavy for your card. Please update the TDR settings according to this post.\n\nCUDA IPC operations\nTHCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n\n\nThey are not supported on Windows. Something like doing multiprocessing on CUDA tensors cannot succeed, there are two alternatives for this.\n\n1. Don’t use multiprocessing. Set the num_worker of DataLoader to zero.\n\n2. Share CPU tensors instead. Make sure your custom DataSet returns CPU tensors.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nWindows FAQ\nBuilding from source\nExtension\nInstallation\nUsage (multiprocessing)\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Serialization semantics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/serialization.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Serialization semantics\nShortcuts\nSERIALIZATION SEMANTICS\n\nThis note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++.\n\nSaving and loading tensors\n\ntorch.save() and torch.load() let you easily save and load tensors:\n\n>>> t = torch.tensor([1., 2.])\n>>> torch.save(t, 'tensor.pt')\n>>> torch.load('tensor.pt')\ntensor([1., 2.])\n\n\nBy convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.\n\ntorch.save() and torch.load() use Python’s pickle by default, so you can also save multiple tensors as part of Python objects like tuples, lists, and dicts:\n\n>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n>>> torch.save(d, 'tensor_dict.pt')\n>>> torch.load('tensor_dict.pt')\n{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}\n\n\nCustom data structures that include PyTorch tensors can also be saved if the data structure is pickle-able.\n\nSaving and loading tensors preserves views\n\nSaving tensors preserves their view relationships:\n\n>>> numbers = torch.arange(1, 10)\n>>> evens = numbers[1::2]\n>>> torch.save([numbers, evens], 'tensors.pt')\n>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n>>> loaded_evens *= 2\n>>> loaded_numbers\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n\n\nBehind the scenes, these tensors share the same “storage.” See Tensor Views for more on views and storage.\n\nWhen PyTorch saves tensors it saves their storage objects and tensor metadata separately. This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors. In the above snippet, for example, only a single storage is written to ‘tensors.pt’.\n\nIn some cases, however, saving the current storage objects may be unnecessary and create prohibitively large files. In the following snippet a storage much larger than the saved tensor is written to a file:\n\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small, 'small.pt')\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n999\n\n\nInstead of saving only the five values in the small tensor to ‘small.pt,’ the 999 values in the storage it shares with large were saved and loaded.\n\nWhen saving tensors with fewer elements than their storage objects, the size of the saved file can be reduced by first cloning the tensors. Cloning a tensor produces a new tensor with a new storage object containing only the values in the tensor:\n\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n5\n\n\nSince the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.\n\nSaving and loading torch.nn.Modules\n\nSee also: Tutorial: Saving and loading modules\n\nIn PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:\n\n>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> list(bn.named_parameters())\n[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n>>> list(bn.named_buffers())\n[('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]\n\n>>> bn.state_dict()\nOrderedDict([('weight', tensor([1., 1., 1.])),\n             ('bias', tensor([0., 0., 0.])),\n             ('running_mean', tensor([0., 0., 0.])),\n             ('running_var', tensor([1., 1., 1.])),\n             ('num_batches_tracked', tensor(0))])\n\n\nInstead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, load_state_dict(), to restore their states from a state dict:\n\n>>> torch.save(bn.state_dict(), 'bn.pt')\n>>> bn_state_dict = torch.load('bn.pt')\n>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> new_bn.load_state_dict(bn_state_dict)\n<All keys matched successfully>\n\n\nNote that the state dict is first loaded from its file with torch.load() and the state then restored with load_state_dict().\n\nEven custom modules and modules containing other modules have state dicts and can use this pattern:\n\n# A module with two linear layers\n>>> class MyModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> m = MyModule()\n>>> m.state_dict()\nOrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),\n             ('l0.bias', tensor([ 0.0300, -0.1316])),\n             ('l1.weight', tensor([[0.6533, 0.3413]])),\n             ('l1.bias', tensor([-0.1112]))])\n\n>>> torch.save(m.state_dict(), 'mymodule.pt')\n>>> m_state_dict = torch.load('mymodule.pt')\n>>> new_m = MyModule()\n>>> new_m.load_state_dict(m_state_dict)\n<All keys matched successfully>\n\nSerializing torch.nn.Modules and loading them in C++\n\nSee also: Tutorial: Loading a TorchScript Model in C++\n\nScriptModules can be serialized as a TorchScript program and loaded using torch.jit.load(). This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python).\n\nThe distinction between torch.jit.save() and torch.save() may not be immediately clear. torch.save() saves Python objects with pickle. This is especially useful for prototyping, researching, and training. torch.jit.save(), on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models.\n\nTo script, serialize and load a module in Python:\n\n>>> scripted_module = torch.jit.script(MyModule())\n>>> torch.jit.save(scripted_module, 'mymodule.pt')\n>>> torch.jit.load('mymodule.pt')\nRecursiveScriptModule( original_name=MyModule\n                      (l0): RecursiveScriptModule(original_name=Linear)\n                      (l1): RecursiveScriptModule(original_name=Linear) )\n\n\nTraced modules can also be saved with torch.jit.save(), with the caveat that only the traced code path is serialized. The following example demonstrates this:\n\n# A module with control flow\n>>> class ControlFlowModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        if input.dim() > 1:\n            return torch.tensor(0)\n\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')\n>>> loaded = torch.jit.load('controlflowmodule_traced.pt')\n>>> loaded(torch.randn(2, 4)))\ntensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)\n\n>>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')\n>>> loaded = torch.jit.load('controlflowmodule_scripted.pt')\n>> loaded(torch.randn(2, 4))\ntensor(0)\n\n\nThe above module has an if statement that is not triggered by the traced inputs, and so is not part of the traced module and not serialized with it. The scripted module, however, contains the if statement and is serialized with it. See the TorchScript documentation for more on scripting and tracing.\n\nFinally, to load the module in C++:\n\n>>> torch::jit::script::Module module;\n>>> module = torch::jit::load('controlflowmodule_scripted.pt');\n\n\nSee the PyTorch C++ API documentation for details about how to use PyTorch modules in C++.\n\nSaving and loading ScriptModules across PyTorch versions\n\nThe PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly. In limited cases, detailed below, PyTorch will preserve the historic behavior of serialized ScriptModules so they do not require an update.\n\ntorch.div performing integer division\n\nIn PyTorch 1.5 and earlier torch.div() would perform floor division when given two integer inputs:\n\n# PyTorch 1.5 (and earlier)\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1)\n\n\nIn PyTorch 1.7, however, torch.div() will always perform a true division of its inputs, just like division in Python 3:\n\n# PyTorch 1.7\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1.6667)\n\n\nThe behavior of torch.div() is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.div() perform floor division when given two integer inputs even when loaded with newer versions of PyTorch. ScriptModules using torch.div() and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.\n\ntorch.full always inferring a float dtype\n\nIn PyTorch 1.5 and earlier torch.full() always returned a float tensor, regardless of the fill value it’s given:\n\n# PyTorch 1.5 and earlier\n>>> torch.full((3,), 1)  # Note the integer fill value...\ntensor([1., 1., 1.])     # ...but float tensor!\n\n\nIn PyTorch 1.7, however, torch.full() will infer the returned tensor’s dtype from the fill value:\n\n# PyTorch 1.7\n>>> torch.full((3,), 1)\ntensor([1, 1, 1])\n\n>>> torch.full((3,), True)\ntensor([True, True, True])\n\n>>> torch.full((3,), 1.)\ntensor([1., 1., 1.])\n\n>>> torch.full((3,), 1 + 1j)\ntensor([1.+1.j, 1.+1.j, 1.+1.j])\n\n\nThe behavior of torch.full() is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.full return float tensors by default, even when given bool or integer fill values. ScriptModules using torch.full() and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.\n\nUtility functions\n\nThe following utility functions are related to serialization:\n\ntorch.serialization.register_package(priority, tagger, deserializer)\n[SOURCE]\n\nRegisters callables for tagging and deserializing storage objects with an associated priority. Tagging associates a device with a storage object at save time while deserializing moves a storage object to an appropriate device at load time. tagger and deserializer are run in the order given by their priority until a tagger/deserializer returns a value that is not None.\n\nTo override the deserialization behavior for a device in the global registry, one can register a tagger with a higher priority than the existing tagger.\n\nThis function can also be used to register a tagger and deserializer for new devices.\n\nParameters\n\npriority (int) – Indicates the priority associated with the tagger and deserializer, where a lower value indicates higher priority.\n\ntagger (Callable[[Union[Storage, TypedStorage, UntypedStorage]], Optional[str]]) – Callable that takes in a storage object and returns its tagged device as a string or None.\n\ndeserializer (Callable[[Union[Storage, TypedStorage, UntypedStorage], str], Optional[Union[Storage, TypedStorage, UntypedStorage]]]) – Callable that takes in storage object and a device string and returns a storage object on the appropriate device or None.\n\nReturns\n\nNone\n\nExample\n\n>>> def ipu_tag(obj):\n>>>     if obj.device.type == 'ipu':\n>>>         return 'ipu'\n>>> def ipu_deserialize(obj, location):\n>>>     if location.startswith('ipu'):\n>>>         ipu = getattr(torch, \"ipu\", None)\n>>>         assert ipu is not None, \"IPU device module is not loaded\"\n>>>         assert torch.ipu.is_available(), \"ipu is not available\"\n>>>         return obj.ipu(location)\n>>> torch.serialization.register_package(11, ipu_tag, ipu_deserialize)\n\ntorch.serialization.get_default_load_endianness()\n[SOURCE]\n\nGet fallback byte order for loading files\n\nIf byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order.\n\nReturns\n\nOptional[LoadEndianness]\n\nReturn type\n\ndefault_load_endian\n\ntorch.serialization.set_default_load_endianness(endianness)\n[SOURCE]\n\nSet fallback byte order for loading files\n\nIf byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order.\n\nParameters\n\nendianness – the new fallback byte order\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nSerialization semantics\nSaving and loading tensors\nSaving and loading tensors preserves views\nSaving and loading torch.nn.Modules\nSerializing torch.nn.Modules and loading them in C++\nSaving and loading ScriptModules across PyTorch versions\nUtility functions\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Reproducibility — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/randomness.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Reproducibility\nShortcuts\nREPRODUCIBILITY\n\nCompletely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.\n\nHowever, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.\n\nWARNING\n\nDeterministic operations are often slower than nondeterministic operations, so single-run performance may decrease for your model. However, determinism may save time in development by facilitating experimentation, debugging, and regression testing.\n\nControlling sources of randomness\nPyTorch random number generator\n\nYou can use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA):\n\nimport torch\ntorch.manual_seed(0)\n\n\nSome PyTorch operations may use random numbers internally. torch.svd_lowrank() does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as torch.manual_seed() is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.\n\nIt is also possible to obtain identical results from an operation that uses random numbers by setting torch.manual_seed() to the same value between subsequent calls.\n\nPython\n\nFor custom operators, you might need to set python seed as well:\n\nimport random\nrandom.seed(0)\n\nRandom number generators in other libraries\n\nIf you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with:\n\nimport numpy as np\nnp.random.seed(0)\n\n\nHowever, some applications and libraries may use NumPy Random Generator objects, not the global RNG (https://numpy.org/doc/stable/reference/random/generator.html), and those will need to be seeded consistently as well.\n\nIf you are using any other libraries that use random number generators, refer to the documentation for those libraries to see how to set consistent seeds for them.\n\nCUDA convolution benchmarking\n\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine.\n\nDisabling the benchmarking feature with torch.backends.cudnn.benchmark = False causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n\nHowever, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with torch.backends.cudnn.benchmark = True.\n\nNote that this setting is different from the torch.backends.cudnn.deterministic setting discussed below.\n\nAvoiding nondeterministic algorithms\n\ntorch.use_deterministic_algorithms() lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).\n\nPlease check the documentation for torch.use_deterministic_algorithms() for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22\n\nFor example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_() will throw an error:\n\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n\n\nWhen torch.bmm() is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used:\n\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n\n\nFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you should set the environment variable CUBLAS_WORKSPACE_CONFIG according to CUDA documentation: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n\nCUDA convolution determinism\n\nWhile disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either torch.use_deterministic_algorithms(True) or torch.backends.cudnn.deterministic = True is set. The latter setting controls only this behavior, unlike torch.use_deterministic_algorithms() which will make other PyTorch operations behave deterministically, too.\n\nCUDA RNN and LSTM\n\nIn some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior. See torch.nn.RNN() and torch.nn.LSTM() for details and workarounds.\n\nDataLoader\n\nDataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use worker_init_fn() and generator to preserve reproducibility:\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nReproducibility\nControlling sources of randomness\nAvoiding nondeterministic algorithms\nDataLoader\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Numerical accuracy — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/numerical_accuracy.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Numerical accuracy\nShortcuts\nNUMERICAL ACCURACY\n\nIn modern computers, floating point numbers are represented using IEEE 754 standard. For more details on floating point arithmetics and IEEE 754 standard, please see Floating point arithmetic In particular, note that floating point provides limited accuracy (about 7 decimal digits for single precision floating point numbers, about 16 decimal digits for double precision floating point numbers) and that floating point addition and multiplication are not associative, so the order of the operations affects the results. Because of this, PyTorch is not guaranteed to produce bitwise identical results for floating point computations that are mathematically identical. Similarly, bitwise identical results are not guaranteed across PyTorch releases, individual commits, or different platforms. In particular, CPU and GPU results can be different even for bitwise-identical inputs and even after controlling for the sources of randomness.\n\nBatched computations or slice computations\n\nMany operations in PyTorch support batched computation, where the same operation is performed for the elements of the batches of inputs. An example of this is torch.mm() and torch.bmm(). It is possible to implement batched computation as a loop over batch elements, and apply the necessary math operations to the individual batch elements, for efficiency reasons we are not doing that, and typically perform computation for the whole batch. The mathematical libraries that we are calling, and PyTorch internal implementations of operations can produces slightly different results in this case, compared to non-batched computations. In particular, let A and B be 3D tensors with the dimensions suitable for batched matrix multiplication. Then (A@B)[0] (the first element of the batched result) is not guaranteed to be bitwise identical to A[0]@B[0] (the matrix product of the first elements of the input batches) even though mathematically it’s an identical computation.\n\nSimilarly, an operation applied to a tensor slice is not guaranteed to produce results that are identical to the slice of the result of the same operation applied to the full tensor. E.g. let A be a 2-dimensional tensor. A.sum(-1)[0] is not guaranteed to be bitwise equal to A[:,0].sum().\n\nExtremal values\n\nWhen inputs contain large values such that intermediate results may overflow the range of the used datatype, the end result may overflow too, even though it is representable in the original datatype. E.g.:\n\nimport torch\na=torch.tensor([1e20, 1e20]) # fp32 type by default\na.norm() # produces tensor(inf)\na.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32\n\nLinear algebra (torch.linalg)\nNon-finite values\n\nThe external libraries (backends) that torch.linalg uses provide no guarantees on their behaviour when the inputs have non-finite values like inf or NaN. As such, neither does PyTorch. The operations may return a tensor with non-finite values, or raise an exception, or even segfault.\n\nConsider using torch.isfinite() before calling these functions to detect this situation.\n\nExtremal values in linalg\n\nFunctions within torch.linalg have more Extremal Values than other PyTorch functions.\n\nSolvers and Inverses assume that the input matrix A is invertible. If it is close to being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return incorrect results. These matrices are said to be ill-conditioned. If provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different devices or when using different backends via the keyword driver.\n\nSpectral operations like svd, eig, and eigh may also return incorrect results (and their gradients may be infinite) when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions struggle to converge for these inputs.\n\nRunning the computation in float64 (as NumPy does by default) often helps, but it does not solve these issues in all cases. Analyzing the spectrum of the inputs via torch.linalg.svdvals() or their condition number via torch.linalg.cond() may help to detect these issues.\n\nTensorFloat-32(TF32) on Nvidia Ampere devices\n\nOn Ampere Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions. When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read. This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input). By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32. We recommend enabling TF32 tensor cores for matrix multiplications with torch.backends.cuda.matmul.allow_tf32 = True if your network does not need full float32 precision. If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with torch.backends.cudnn.allow_tf32 = False.\n\nFor more information see TensorFloat32.\n\nReduced Precision Reduction for FP16 and BF16 GEMMs\n\nHalf-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., inf values when the final result should be be representable in half-precision). If reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\nA similar flag exists for BF16 GEMM operations and is turned on by default. If BF16 reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\nFor more information see allow_fp16_reduced_precision_reduction and allow_bf16_reduced_precision_reduction\n\nReduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices\n\nOn AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.\n\nrocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.\n\nWhen training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:\n\n\t\n\nforward\n\n\t\n\nbackward\n\n\n\n\nEnv unset\n\n\t\n\noriginal\n\n\t\n\nalternate\n\n\n\n\nEnv set to 1\n\n\t\n\nalternate\n\n\t\n\nalternate\n\n\n\n\nEnv set to 0\n\n\t\n\noriginal\n\n\t\n\noriginal\n\nThe following is the list of operations where rocBLAS may be used:\n\ntorch.addbmm\n\ntorch.addmm\n\ntorch.baddbmm\n\ntorch.bmm\n\ntorch.mm\n\ntorch.nn.GRUCell\n\ntorch.nn.LSTMCell\n\ntorch.nn.Linear\n\ntorch.sparse.addmm\n\nthe following torch._C._ConvBackend implementations:\n\nslowNd\n\nslowNd_transposed\n\nslowNd_dilated\n\nslowNd_dilated_transposed\n\nThe following is the list of operations where MIOpen may be used:\n\ntorch.nn.Conv[Transpose]Nd\n\nthe following torch._C._ConvBackend implementations:\n\nConvBackend::Miopen\n\nConvBackend::MiopenDepthwise\n\nConvBackend::MiopenTranspose\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nNumerical accuracy\nBatched computations or slice computations\nExtremal values\nLinear algebra (torch.linalg)\nTensorFloat-32(TF32) on Nvidia Ampere devices\nReduced Precision Reduction for FP16 and BF16 GEMMs\nReduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Gradcheck mechanics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/gradcheck.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Gradcheck mechanics\nShortcuts\nGRADCHECK MECHANICS\n\nThis note presents an overview of how the gradcheck() and gradgradcheck() functions work.\n\nIt will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where fast_mode=True argument is passed (referred to as fast gradcheck below).\n\nNotations and background information\n\nDefault backward mode gradcheck behavior\n\nReal-to-real functions\n\nComplex-to-real functions\n\nFunctions with complex outputs\n\nFast backward mode gradcheck\n\nFast gradcheck for real-to-real functions\n\nFast gradcheck for complex-to-real functions\n\nFast gradcheck for functions with complex outputs\n\nGradgradcheck implementation\n\nNotations and background information\n\nThroughout this note, we will use the following convention:\n\n𝑥\nx, \n𝑦\ny, \n𝑎\na, \n𝑏\nb, \n𝑣\nv, \n𝑢\nu, \n𝑢\n𝑟\nur and \n𝑢\n𝑖\nui are real-valued vectors and \n𝑧\nz is a complex-valued vector that can be rewritten in terms of two real-valued vectors as \n𝑧\n=\n𝑎\n+\n𝑖\n𝑏\nz=a+ib.\n\n𝑁\nN and \n𝑀\nM are two integers that we will use for the dimension of the input and output space respectively.\n\n𝑓\n:\n𝑅\n𝑁\n→\n𝑅\n𝑀\nf:R\nN\n→R\nM\n is our basic real-to-real function such that \n𝑦\n=\n𝑓\n(\n𝑥\n)\ny=f(x).\n\n𝑔\n:\n𝐶\n𝑁\n→\n𝑅\n𝑀\ng:C\nN\n→R\nM\n is our basic complex-to-real function such that \n𝑦\n=\n𝑔\n(\n𝑧\n)\ny=g(z).\n\nFor the simple real-to-real case, we write as \n𝐽\n𝑓\nJ\nf\n\t​\n\n the Jacobian matrix associated with \n𝑓\nf of size \n𝑀\n×\n𝑁\nM×N. This matrix contains all the partial derivatives such that the entry at position \n(\n𝑖\n,\n𝑗\n)\n(i,j) contains \n∂\n𝑦\n𝑖\n∂\n𝑥\n𝑗\n∂x\nj\n\t​\n\n∂y\ni\n\t​\n\n\t​\n\n. Backward mode AD is then computing, for a given vector \n𝑣\nv of size \n𝑀\nM, the quantity \n𝑣\n𝑇\n𝐽\n𝑓\nv\nT\nJ\nf\n\t​\n\n. Forward mode AD on the other hand is computing, for a given vector \n𝑢\nu of size \n𝑁\nN, the quantity \n𝐽\n𝑓\n𝑢\nJ\nf\n\t​\n\nu.\n\nFor functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at Autograd for Complex Numbers.\n\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called \n𝑊\nW below) and the Conjugate Wirtinger derivative (called \n𝐶\n𝑊\nCW below). Both \n𝑊\nW and \n𝐶\n𝑊\nCW need to be propagated because in general, despite their name, one is not the complex conjugate of the other.\n\nTo avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\n\nUnder this assumption, using \n𝑊\nW and \n𝐶\n𝑊\nCW definitions, we can show that \n𝑊\n=\n𝐶\n𝑊\n∗\nW=CW\n∗\n (we use \n∗\n∗ to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered. To simplify internal computations, PyTorch uses \n2\n∗\n𝐶\n𝑊\n2∗CW as the value it backwards and returns when the user asks for gradients. Similarly to the real case, when the output is actually in \n𝑅\n𝑀\nR\nM\n, backward mode AD does not compute \n2\n∗\n𝐶\n𝑊\n2∗CW but only \n𝑣\n𝑇\n(\n2\n∗\n𝐶\n𝑊\n)\nv\nT\n(2∗CW) for a given vector \n𝑣\n∈\n𝑅\n𝑀\nv∈R\nM\n.\n\nFor forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in \n𝑅\nR. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in \n𝑅\nR and in this case, using \n𝑊\nW and \n𝐶\n𝑊\nCW definitions, we can show that \n𝑊\n=\n𝐶\n𝑊\nW=CW for the intermediary functions. To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes \n2\n∗\n𝐶\n𝑊\n2∗CW. Similarly to the real case, when the input is actually in \n𝑅\n𝑁\nR\nN\n, forward mode AD does not compute \n2\n∗\n𝐶\n𝑊\n2∗CW but only \n(\n2\n∗\n𝐶\n𝑊\n)\n𝑢\n(2∗CW)u for a given vector \n𝑢\n∈\n𝑅\n𝑁\nu∈R\nN\n.\n\nDefault backward mode gradcheck behavior\nReal-to-real functions\n\nTo test a function \n𝑓\n:\n𝑅\n𝑁\n→\n𝑅\n𝑀\n,\n𝑥\n→\n𝑦\nf:R\nN\n→R\nM\n,x→y, we reconstruct the full Jacobian matrix \n𝐽\n𝑓\nJ\nf\n\t​\n\n of size \n𝑀\n×\n𝑁\nM×N in two ways: analytically and numerically. The analytical version uses our backward mode AD while the numerical version uses finite difference. The two reconstructed Jacobian matrices are then compared elementwise for equality.\n\nDefault real input numerical evaluation\n\nIf we consider the elementary case of a one-dimensional function (\n𝑁\n=\n𝑀\n=\n1\nN=M=1), then we can use the basic finite difference formula from the wikipedia article. We use the “central difference” for better numerical properties:\n\n∂\n𝑦\n∂\n𝑥\n≈\n𝑓\n(\n𝑥\n+\n𝑒\n𝑝\n𝑠\n)\n−\n𝑓\n(\n𝑥\n−\n𝑒\n𝑝\n𝑠\n)\n2\n∗\n𝑒\n𝑝\n𝑠\n∂x\n∂y\n\t​\n\n≈\n2∗eps\nf(x+eps)−f(x−eps)\n\t​\n\n\nThis formula easily generalizes for multiple outputs (\n𝑀\n>\n1\nM>1) by having \n∂\n𝑦\n∂\n𝑥\n∂x\n∂y\n\t​\n\n be a column vector of size \n𝑀\n×\n1\nM×1 like \n𝑓\n(\n𝑥\n+\n𝑒\n𝑝\n𝑠\n)\nf(x+eps). In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely \n𝑓\n(\n𝑥\n+\n𝑒\n𝑝\n𝑠\n)\nf(x+eps) and \n𝑓\n(\n𝑥\n−\n𝑒\n𝑝\n𝑠\n)\nf(x−eps)).\n\nIt is more computationally expensive to handle the case with multiple inputs (\n𝑁\n>\n1\nN>1). In this scenario, we loop over all the inputs one after the other and apply the \n𝑒\n𝑝\n𝑠\neps perturbation for each element of \n𝑥\nx one after the other. This allows us to reconstruct the \n𝐽\n𝑓\nJ\nf\n\t​\n\n matrix column by column.\n\nDefault real input analytical evaluation\n\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computes \n𝑣\n𝑇\n𝐽\n𝑓\nv\nT\nJ\nf\n\t​\n\n. For functions with a single output, we simply use \n𝑣\n=\n1\nv=1 to recover the full Jacobian matrix with a single backward pass.\n\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where each \n𝑣\nv is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the \n𝐽\n𝑓\nJ\nf\n\t​\n\n matrix row by row.\n\nComplex-to-real functions\n\nTo test a function \n𝑔\n:\n𝐶\n𝑁\n→\n𝑅\n𝑀\n,\n𝑧\n→\n𝑦\ng:C\nN\n→R\nM\n,z→y with \n𝑧\n=\n𝑎\n+\n𝑖\n𝑏\nz=a+ib, we reconstruct the (complex-valued) matrix that contains \n2\n∗\n𝐶\n𝑊\n2∗CW.\n\nDefault complex input numerical evaluation\n\nConsider the elementary case where \n𝑁\n=\n𝑀\n=\n1\nN=M=1 first. We know from (chapter 3 of) this research paper that:\n\n𝐶\n𝑊\n:\n=\n∂\n𝑦\n∂\n𝑧\n∗\n=\n1\n2\n∗\n(\n∂\n𝑦\n∂\n𝑎\n+\n𝑖\n∂\n𝑦\n∂\n𝑏\n)\nCW:=\n∂z\n∗\n∂y\n\t​\n\n=\n2\n1\n\t​\n\n∗(\n∂a\n∂y\n\t​\n\n+i\n∂b\n∂y\n\t​\n\n)\n\nNote that \n∂\n𝑦\n∂\n𝑎\n∂a\n∂y\n\t​\n\n and \n∂\n𝑦\n∂\n𝑏\n∂b\n∂y\n\t​\n\n, in the above equation, are \n𝑅\n→\n𝑅\nR→R derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. This allows us to compute the \n𝐶\n𝑊\nCW matrix and then multiply it by \n2\n2.\n\nNote that the code, as of time of writing, computes this value in a slightly convoluted way:\n\n# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n# Notation changes in this code block:\n# s here is y above\n# x, y here are a, b above\n\nds_dx = compute_gradient(eps)\nds_dy = compute_gradient(eps * 1j)\n# conjugate wirtinger derivative\nconj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n# wirtinger derivative\nw_d = 0.5 * (ds_dx - ds_dy * 1j)\nd[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\nDefault complex input analytical evaluation\n\nSince backward mode AD computes exactly twice the \n𝐶\n𝑊\nCW derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\n\nFunctions with complex outputs\n\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. To solve this, we will replace the test of the function \nℎ\n:\n𝑃\n𝑁\n→\n𝐶\n𝑀\nh:P\nN\n→C\nM\n (where \n𝑃\nP can be either \n𝑅\nR or \n𝐶\nC), with two functions: \nℎ\n𝑟\nhr and \nℎ\n𝑖\nhi such that:\n\nℎ\n𝑟\n(\n𝑞\n)\n\t\n:\n=\n𝑟\n𝑒\n𝑎\n𝑙\n(\n𝑓\n(\n𝑞\n)\n)\n\n\nℎ\n𝑖\n(\n𝑞\n)\n\t\n:\n=\n𝑖\n𝑚\n𝑎\n𝑔\n(\n𝑓\n(\n𝑞\n)\n)\nhr(q)\nhi(q)\n\t​\n\n:=real(f(q))\n:=imag(f(q))\n\t​\n\n\nwhere \n𝑞\n∈\n𝑃\nq∈P. We then do a basic gradcheck for both \nℎ\n𝑟\nhr and \nℎ\n𝑖\nhi using either the real-to-real or complex-to-real case described above, depending on \n𝑃\nP.\n\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the \n𝑟\n𝑒\n𝑎\n𝑙\nreal or \n𝑖\n𝑚\n𝑎\n𝑔\nimag functions manually by passing the \ngrad_out\ngrad_out arguments to the different functions. When \ngrad_out\n=\n1\ngrad_out=1, then we are considering \nℎ\n𝑟\nhr. When \ngrad_out\n=\n1\n𝑗\ngrad_out=1j, then we are considering \nℎ\n𝑖\nhi.\n\nFast backward mode gradcheck\n\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices. This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\n\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\n\nFast gradcheck for real-to-real functions\n\nThe scalar quantity that we want to compute here is \n𝑣\n𝑇\n𝐽\n𝑓\n𝑢\nv\nT\nJ\nf\n\t​\n\nu for a given random vector \n𝑣\n∈\n𝑅\n𝑀\nv∈R\nM\n and a random unit norm vector \n𝑢\n∈\n𝑅\n𝑁\nu∈R\nN\n.\n\nFor the numerical evaluation, we can efficiently compute\n\n𝐽\n𝑓\n𝑢\n≈\n𝑓\n(\n𝑥\n+\n𝑢\n∗\n𝑒\n𝑝\n𝑠\n)\n−\n𝑓\n(\n𝑥\n−\n𝑢\n∗\n𝑒\n𝑝\n𝑠\n)\n2\n∗\n𝑒\n𝑝\n𝑠\n.\nJ\nf\n\t​\n\nu≈\n2∗eps\nf(x+u∗eps)−f(x−u∗eps)\n\t​\n\n.\n\nWe then perform the dot product between this vector and \n𝑣\nv to get the scalar value of interest.\n\nFor the analytical version, we can use backward mode AD to compute \n𝑣\n𝑇\n𝐽\n𝑓\nv\nT\nJ\nf\n\t​\n\n directly. We then perform the dot product with \n𝑢\nu to get the expected value.\n\nFast gradcheck for complex-to-real functions\n\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the \n2\n∗\n𝐶\n𝑊\n2∗CW matrix is complex-valued and so in this case, we will compare to complex scalars.\n\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\n\n𝑠\n:\n=\n2\n∗\n𝑣\n𝑇\n(\n𝑟\n𝑒\n𝑎\n𝑙\n(\n𝐶\n𝑊\n)\n𝑢\n𝑟\n+\n𝑖\n∗\n𝑖\n𝑚\n𝑎\n𝑔\n(\n𝐶\n𝑊\n)\n𝑢\n𝑖\n)\ns:=2∗v\nT\n(real(CW)ur+i∗imag(CW)ui)\n\nwhere \n𝑣\n∈\n𝑅\n𝑀\nv∈R\nM\n, \n𝑢\n𝑟\n∈\n𝑅\n𝑁\nur∈R\nN\n and \n𝑢\n𝑖\n∈\n𝑅\n𝑁\nui∈R\nN\n.\n\nFast complex input numerical evaluation\n\nWe first consider how to compute \n𝑠\ns with a numerical method. To do so, keeping in mind that we’re considering \n𝑔\n:\n𝐶\n𝑁\n→\n𝑅\n𝑀\n,\n𝑧\n→\n𝑦\ng:C\nN\n→R\nM\n,z→y with \n𝑧\n=\n𝑎\n+\n𝑖\n𝑏\nz=a+ib, and that \n𝐶\n𝑊\n=\n1\n2\n∗\n(\n∂\n𝑦\n∂\n𝑎\n+\n𝑖\n∂\n𝑦\n∂\n𝑏\n)\nCW=\n2\n1\n\t​\n\n∗(\n∂a\n∂y\n\t​\n\n+i\n∂b\n∂y\n\t​\n\n), we rewrite it as follows:\n\n𝑠\n\t\n=\n2\n∗\n𝑣\n𝑇\n(\n𝑟\n𝑒\n𝑎\n𝑙\n(\n𝐶\n𝑊\n)\n𝑢\n𝑟\n+\n𝑖\n∗\n𝑖\n𝑚\n𝑎\n𝑔\n(\n𝐶\n𝑊\n)\n𝑢\n𝑖\n)\n\n\n\t\n=\n2\n∗\n𝑣\n𝑇\n(\n1\n2\n∗\n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑟\n+\n𝑖\n∗\n1\n2\n∗\n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑖\n)\n\n\n\t\n=\n𝑣\n𝑇\n(\n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑟\n+\n𝑖\n∗\n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑖\n)\n\n\n\t\n=\n𝑣\n𝑇\n(\n(\n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑟\n)\n+\n𝑖\n∗\n(\n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑖\n)\n)\ns\n\t​\n\n=2∗v\nT\n(real(CW)ur+i∗imag(CW)ui)\n=2∗v\nT\n(\n2\n1\n\t​\n\n∗\n∂a\n∂y\n\t​\n\nur+i∗\n2\n1\n\t​\n\n∗\n∂b\n∂y\n\t​\n\nui)\n=v\nT\n(\n∂a\n∂y\n\t​\n\nur+i∗\n∂b\n∂y\n\t​\n\nui)\n=v\nT\n((\n∂a\n∂y\n\t​\n\nur)+i∗(\n∂b\n∂y\n\t​\n\nui))\n\t​\n\n\nIn this formula, we can see that \n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑟\n∂a\n∂y\n\t​\n\nur and \n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑖\n∂b\n∂y\n\t​\n\nui can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued \n𝑣\nv vector.\n\nFast complex input analytical evaluation\n\nFor the analytical case, things are simpler and we rewrite the formula as:\n\n𝑠\n\t\n=\n2\n∗\n𝑣\n𝑇\n(\n𝑟\n𝑒\n𝑎\n𝑙\n(\n𝐶\n𝑊\n)\n𝑢\n𝑟\n+\n𝑖\n∗\n𝑖\n𝑚\n𝑎\n𝑔\n(\n𝐶\n𝑊\n)\n𝑢\n𝑖\n)\n\n\n\t\n=\n𝑣\n𝑇\n𝑟\n𝑒\n𝑎\n𝑙\n(\n2\n∗\n𝐶\n𝑊\n)\n𝑢\n𝑟\n+\n𝑖\n∗\n𝑣\n𝑇\n𝑖\n𝑚\n𝑎\n𝑔\n(\n2\n∗\n𝐶\n𝑊\n)\n𝑢\n𝑖\n)\n\n\n\t\n=\n𝑟\n𝑒\n𝑎\n𝑙\n(\n𝑣\n𝑇\n(\n2\n∗\n𝐶\n𝑊\n)\n)\n𝑢\n𝑟\n+\n𝑖\n∗\n𝑖\n𝑚\n𝑎\n𝑔\n(\n𝑣\n𝑇\n(\n2\n∗\n𝐶\n𝑊\n)\n)\n𝑢\n𝑖\ns\n\t​\n\n=2∗v\nT\n(real(CW)ur+i∗imag(CW)ui)\n=v\nT\nreal(2∗CW)ur+i∗v\nT\nimag(2∗CW)ui)\n=real(v\nT\n(2∗CW))ur+i∗imag(v\nT\n(2∗CW))ui\n\t​\n\n\nWe can thus use the fact that the backward mode AD provides us with an efficient way to compute \n𝑣\n𝑇\n(\n2\n∗\n𝐶\n𝑊\n)\nv\nT\n(2∗CW) and then perform a dot product of the real part with \n𝑢\n𝑟\nur and the imaginary part with \n𝑢\n𝑖\nui before reconstructing the final complex scalar \n𝑠\ns.\n\nWhy not use a complex \n𝑢\nu\n\nAt this point, you might be wondering why we did not select a complex \n𝑢\nu and just performed the reduction \n2\n∗\n𝑣\n𝑇\n𝐶\n𝑊\n𝑢\n′\n2∗v\nT\nCWu\n′\n. To dive into this, in this paragraph, we will use the complex version of \n𝑢\nu noted \n𝑢\n′\n=\n𝑢\n𝑟\n′\n+\n𝑖\n𝑢\n𝑖\n′\nu\n′\n=ur\n′\n+iui\n′\n. Using such complex \n𝑢\n′\nu\n′\n, the problem is that when doing the numerical evaluation, we would need to compute:\n\n2\n∗\n𝐶\n𝑊\n𝑢\n′\n\t\n=\n(\n∂\n𝑦\n∂\n𝑎\n+\n𝑖\n∂\n𝑦\n∂\n𝑏\n)\n(\n𝑢\n𝑟\n′\n+\n𝑖\n𝑢\n𝑖\n′\n)\n\n\n\t\n=\n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑟\n′\n+\n𝑖\n∂\n𝑦\n∂\n𝑎\n𝑢\n𝑖\n′\n+\n𝑖\n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑟\n′\n−\n∂\n𝑦\n∂\n𝑏\n𝑢\n𝑖\n′\n2∗CWu\n′\n\t​\n\n=(\n∂a\n∂y\n\t​\n\n+i\n∂b\n∂y\n\t​\n\n)(ur\n′\n+iui\n′\n)\n=\n∂a\n∂y\n\t​\n\nur\n′\n+i\n∂a\n∂y\n\t​\n\nui\n′\n+i\n∂b\n∂y\n\t​\n\nur\n′\n−\n∂b\n∂y\n\t​\n\nui\n′\n\t​\n\n\nWhich would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\n\nFast gradcheck for functions with complex outputs\n\nJust like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.\n\nGradgradcheck implementation\n\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\n\nThis feature is implemented by considering the function \n𝐹\n:\n𝑥\n,\n𝑣\n→\n𝑣\n𝑇\n𝐽\n𝑓\nF:x,v→v\nT\nJ\nf\n\t​\n\n and use the gradcheck defined above on this function. Note that \n𝑣\nv in this case is just a random vector with the same type as \n𝑓\n(\n𝑥\n)\nf(x).\n\nThe fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function \n𝐹\nF.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nGradcheck mechanics\nNotations and background information\nDefault backward mode gradcheck behavior\nFast backward mode gradcheck\nGradgradcheck implementation\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Modules — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/modules.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Modules\nShortcuts\nMODULES\n\nPyTorch uses modules to represent neural networks. Modules are:\n\nBuilding blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n\nTightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n\nEasy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.\n\nA Simple Custom Module\n\nModules as Building Blocks\n\nNeural Network Training with Modules\n\nModule State\n\nModule Initialization\n\nModule Hooks\n\nAdvanced Features\n\nDistributed Training\n\nProfiling Performance\n\nImproving Performance with Quantization\n\nImproving Memory Usage with Pruning\n\nParametrizations\n\nTransforming Modules with FX\n\nA Simple Custom Module\n\nTo get started, let’s look at a simpler, custom version of PyTorch’s Linear module. This module applies an affine transformation to its input.\n\nimport torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n\n\nThis simple module has the following fundamental characteristics of modules:\n\nIt inherits from the base Module class. All modules should subclass Module for composability with other modules.\n\nIt defines some “state” that is used in computation. Here, the state consists of randomly-initialized weight and bias tensors that define the affine transformation. Because each of these is defined as a Parameter, they are registered for the module and will automatically be tracked and returned from calls to parameters(). Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.\n\nIt defines a forward() function that performs the computation. For this affine transformation module, the input is matrix-multiplied with the weight parameter (using the @ short-hand notation) and added to the bias parameter to produce the output. More generally, the forward() implementation for a module can perform arbitrary computation involving any number of inputs and outputs.\n\nThis simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called:\n\nm = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n\n\nNote that the module itself is callable, and that calling it invokes its forward() function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a backward() function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules.\n\nThe full set of parameters registered by the module can be iterated through via a call to parameters() or named_parameters(), where the latter includes each parameter’s name:\n\nfor parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n\n\nIn general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.\n\nModules as Building Blocks\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the Sequential module. It allows us to chain together multiple modules:\n\nnet = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n\n\nNote that Sequential automatically feeds the output of the first MyLinear module as input into the ReLU, and the output of that as input into the second MyLinear module. As shown, it is limited to in-order chaining of modules with a single input and output.\n\nIn general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation.\n\nFor example, here’s a simple neural network implemented as a custom module:\n\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n\n\nThis module is composed of two “children” or “submodules” (l0 and l1) that define the layers of the neural network and are utilized for computation within the module’s forward() method. Immediate children of a module can be iterated through via a call to children() or named_children():\n\nnet = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n\n\nTo go deeper than just the immediate children, modules() and named_modules() recursively iterate through a module and its child modules:\n\nclass BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n\n\nSometimes, it’s necessary for a module to dynamically define submodules. The ModuleList and ModuleDict modules are useful here; they register submodules from a list or dict:\n\nclass DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n\n\nFor any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network:\n\nfor parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))\n\n\nIt’s also easy to move all parameters to a different device or change their precision using to():\n\n# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n\n\nMore generally, an arbitrary function can be applied to a module and its submodules recursively by using the apply() function. For example, to apply custom initialization to parameters of a module and its submodules:\n\n# Define a function to initialize Linear weights.\n# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n@torch.no_grad()\ndef init_weights(m):\n  if isinstance(m, nn.Linear):\n    nn.init.xavier_normal_(m.weight)\n    m.bias.fill_(0.0)\n\n# Apply the function recursively on the module and its submodules.\ndynamic_net.apply(init_weights)\n\n\nThese examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the torch.nn namespace that perform common neural network operations like pooling, convolutions, loss functions, etc.\n\nIn the next section, we give a full example of training a neural network.\n\nFor more information, check out:\n\nLibrary of PyTorch-provided modules: torch.nn\n\nDefining neural net modules: https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html\n\nNeural Network Training with Modules\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from torch.optim:\n\n# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n# After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n# (see discussion below for a description of training and evaluation modes)\n...\nnet.eval()\n...\n\n\nIn this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the key parts of training are present:\n\nA network is created.\n\nAn optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.\n\nA training loop…\n\nacquires an input,\n\nruns the network,\n\ncomputes a loss,\n\nzeros the network’s parameters’ gradients,\n\ncalls loss.backward() to update the parameters’ gradients,\n\ncalls optimizer.step() to apply the gradients to the parameters.\n\nAfter the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of l1‘s weight parameter shows that its values are now much closer to 0 (as may be expected):\n\nprint(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)\n\n\nNote that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using train() and eval(). They can behave differently depending on which mode they are in. For example, the BatchNorm module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes:\n\nclass ModalModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, x):\n    if self.training:\n      # Add a constant only in training mode.\n      return x + 1.\n    else:\n      return x\n\n\nm = ModalModule()\nx = torch.randn(4)\n\nprint('training mode output: {}'.format(m(x)))\n: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\nm.eval()\nprint('evaluation mode output: {}'.format(m(x)))\n: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n\n\nTraining neural networks can often be tricky. For more information, check out:\n\nUsing Optimizers: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.\n\nNeural network training: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n\nIntroduction to autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n\nModule State\n\nIn the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. “state dictionary”):\n\n# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n\n\nA module’s state_dict contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:\n\nParameters: learnable aspects of computation; contained within the state_dict\n\nBuffers: non-learnable aspects of computation\n\nPersistent buffers: contained within the state_dict (i.e. serialized when saving & loading)\n\nNon-persistent buffers: not contained within the state_dict (i.e. left out of serialization)\n\nAs a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s state_dict so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use register_buffer() to accomplish this:\n\nclass RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n\n\nNow, the current value of the running mean is considered part of the module’s state_dict and will be properly restored when loading the module from disk:\n\nm = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n\n\nAs mentioned previously, buffers can be left out of the module’s state_dict by marking them as non-persistent:\n\nself.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n\n\nBoth persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with to():\n\n# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)\n\n\nBuffers of a module can be iterated over using buffers() or named_buffers().\n\nfor buffer in m.named_buffers():\n  print(buffer)\n\n\nThe following class demonstrates the various ways of registering parameters and buffers within a module:\n\nclass StatefulModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n    # as a parameter of the module.\n    self.param1 = nn.Parameter(torch.randn(2))\n\n    # Alternative string-based way to register a parameter.\n    self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n    # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n    # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_parameter('param3', None)\n\n    # Registers a list of parameters.\n    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n    # Registers a dictionary of parameters.\n    self.param_dict = nn.ParameterDict({\n      'foo': nn.Parameter(torch.randn(3)),\n      'bar': nn.Parameter(torch.randn(4))\n    })\n\n    # Registers a persistent buffer (one that appears in the module's state_dict).\n    self.register_buffer('buffer1', torch.randn(4), persistent=True)\n\n    # Registers a non-persistent buffer (one that does not appear in the module's state_dict).\n    self.register_buffer('buffer2', torch.randn(5), persistent=False)\n\n    # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything\n    # except a buffer. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_buffer('buffer3', None)\n\n    # Adding a submodule registers its parameters as parameters of the module.\n    self.linear = nn.Linear(2, 3)\n\nm = StatefulModule()\n\n# Save and load state_dict.\ntorch.save(m.state_dict(), 'state.pt')\nm_loaded = StatefulModule()\nm_loaded.load_state_dict(torch.load('state.pt'))\n\n# Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do\n# not appear in the state_dict.\nprint(m_loaded.state_dict())\n: OrderedDict([('param1', tensor([-0.0322,  0.9066])),\n               ('param2', tensor([-0.4472,  0.1409,  0.4852])),\n               ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),\n               ('param_list.0', tensor([ 0.4202, -0.1953])),\n               ('param_list.1', tensor([ 1.5299, -0.8747])),\n               ('param_list.2', tensor([-1.6289,  1.4898])),\n               ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),\n               ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),\n               ('linear.weight', tensor([[-0.3915, -0.6176],\n                                         [ 0.6062, -0.5992],\n                                         [ 0.4452, -0.2843]])),\n               ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])\n\n\nFor more information, check out:\n\nSaving and loading: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\nSerialization semantics: https://pytorch.org/docs/main/notes/serialization.html\n\nWhat is a state dict? https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n\nModule Initialization\n\nBy default, parameters and floating-point buffers for modules provided by torch.nn are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique.\n\nExamples:\n\n# Initialize module directly onto GPU.\nm = nn.Linear(5, 3, device='cuda')\n\n# Initialize module with 16-bit floating point parameters.\nm = nn.Linear(5, 3, dtype=torch.half)\n\n# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.\nm = torch.nn.utils.skip_init(nn.Linear, 5, 3)\nnn.init.orthogonal_(m.weight)\n\n\nNote that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module:\n\nm = nn.BatchNorm2d(3, dtype=torch.half)\nprint(m.running_mean)\n: tensor([0., 0., 0.], dtype=torch.float16)\n\n\nWhile module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use dtype=torch.float and device='cpu' by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all torch.nn modules follow:\n\nProvide a device constructor kwarg that applies to any parameters / buffers registered by the module.\n\nProvide a dtype constructor kwarg that applies to any parameters / floating-point buffers registered by the module.\n\nOnly use initialization functions (i.e. functions from torch.nn.init) on parameters and buffers within the module’s constructor. Note that this is only required to use skip_init(); see this page for an explanation.\n\nFor more information, check out:\n\nSkipping module parameter initialization: https://pytorch.org/tutorials/prototype/skip_param_init.html\n\nModule Hooks\n\nIn Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.\n\nPyTorch provides two types of hooks for modules:\n\nForward hooks are called during the forward pass. They can be installed for a given module with register_forward_pre_hook() and register_forward_hook(). These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous register_module_forward_pre_hook() and register_module_forward_hook() functions.\n\nBackward hooks are called during the backward pass. They can be installed with register_full_backward_pre_hook() and register_full_backward_hook(). These hooks will be called when the backward for this Module has been computed. register_full_backward_pre_hook() will allow the user to access the gradients for outputs while register_full_backward_hook() will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with register_module_full_backward_hook() and register_module_full_backward_pre_hook().\n\nAll hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s forward() function.\n\nBelow is an example demonstrating usage of forward and backward hooks:\n\ntorch.manual_seed(1)\n\ndef forward_pre_hook(m, inputs):\n  # Allows for examination and modification of the input before the forward pass.\n  # Note that inputs are always wrapped in a tuple.\n  input = inputs[0]\n  return input + 1.\n\ndef forward_hook(m, inputs, output):\n  # Allows for examination of inputs / outputs and modification of the outputs\n  # after the forward pass. Note that inputs are always wrapped in a tuple while outputs\n  # are passed as-is.\n\n  # Residual computation a la ResNet.\n  return output + inputs[0]\n\ndef backward_hook(m, grad_inputs, grad_outputs):\n  # Allows for examination of grad_inputs / grad_outputs and modification of\n  # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and\n  # grad_outputs are always wrapped in tuples.\n  new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n  return new_grad_inputs\n\n# Create sample module & input.\nm = nn.Linear(3, 3)\nx = torch.randn(2, 3, requires_grad=True)\n\n# ==== Demonstrate forward hooks. ====\n# Run input through module before and after adding hooks.\nprint('output with no forward hooks: {}'.format(m(x)))\n: output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# Note that the modified input results in a different output.\nforward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\nprint('output with forward pre hook: {}'.format(m(x)))\n: output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n                                        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)\n\n# Note the modified output.\nforward_hook_handle = m.register_forward_hook(forward_hook)\nprint('output with both forward hooks: {}'.format(m(x)))\n: output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n                                          [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n\n# Remove hooks; note that the output here matches the output before adding hooks.\nforward_pre_hook_handle.remove()\nforward_hook_handle.remove()\nprint('output after removing forward hooks: {}'.format(m(x)))\n: output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                               [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# ==== Demonstrate backward hooks. ====\nm(x).sum().backward()\nprint('x.grad with no backwards hook: {}'.format(x.grad))\n: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n                                         [ 0.4497, -0.5046,  0.3146]])\n\n# Clear gradients before running backward pass again.\nm.zero_grad()\nx.grad.zero_()\n\nm.register_full_backward_hook(backward_hook)\nm(x).sum().backward()\nprint('x.grad with backwards hook: {}'.format(x.grad))\n: x.grad with backwards hook: tensor([[42., 42., 42.],\n                                      [42., 42., 42.]])\n\nAdvanced Features\n\nPyTorch also provides several more advanced features that are designed to work with modules. All these functionalities are available for custom-written modules, with the small caveat that certain features may require modules to conform to particular constraints in order to be supported. In-depth discussion of these features and the corresponding requirements can be found in the links below.\n\nDistributed Training\n\nVarious methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs as well as training across multiple machines. Check out the distributed training overview page for detailed information on how to utilize these.\n\nProfiling Performance\n\nThe PyTorch Profiler can be useful for identifying performance bottlenecks within your models. It measures and outputs performance characteristics for both memory usage and time spent.\n\nImproving Performance with Quantization\n\nApplying quantization techniques to modules can improve performance and memory usage by utilizing lower bitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization here.\n\nImproving Memory Usage with Pruning\n\nLarge deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch provides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The Pruning tutorial describes how to utilize the pruning techniques PyTorch provides or define custom pruning techniques as necessary.\n\nParametrizations\n\nFor certain applications, it can be beneficial to constrain the parameter space during model training. For example, enforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for applying parametrizations such as this, and further allows for custom constraints to be defined.\n\nTransforming Modules with FX\n\nThe FX component of PyTorch provides a flexible way to transform modules by operating directly on module computation graphs. This can be used to programmatically generate or manipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for convolution + batch norm fusion and CPU performance analysis.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nModules\nA Simple Custom Module\nModules as Building Blocks\nNeural Network Training with Modules\nModule State\nModule Initialization\nModule Hooks\nAdvanced Features\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Multiprocessing best practices — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/multiprocessing.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Multiprocessing best practices\nShortcuts\nMULTIPROCESSING BEST PRACTICES\n\ntorch.multiprocessing is a drop in replacement for Python’s multiprocessing module. It supports the exact same operations, but extends it, so that all tensors sent through a multiprocessing.Queue, will have their data moved into shared memory and will only send a handle to another process.\n\nNOTE\n\nWhen a Tensor is sent to another process, the Tensor data is shared. If torch.Tensor.grad is not None, it is also shared. After a Tensor without a torch.Tensor.grad field is sent to the other process, it creates a standard process-specific .grad Tensor that is not automatically shared across all processes, unlike how the Tensor’s data has been shared.\n\nThis allows to implement various training methods, like Hogwild, A3C, or any others that require asynchronous operation.\n\nCUDA in multiprocessing\n\nThe CUDA runtime does not support the fork start method; either the spawn or forkserver start method are required to use CUDA in subprocesses.\n\nNOTE\n\nThe start method can be set via either creating a context with multiprocessing.get_context(...) or directly using multiprocessing.set_start_method(...).\n\nUnlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. It is implemented under the hood but requires users to follow the best practices for the program to run correctly. For example, the sending process must stay alive as long as the consumer process has references to the tensor, and the refcounting can not save you if the consumer process exits abnormally via a fatal signal. See this section.\n\nSee also: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n\nBest practices and tips\nAvoiding and fighting deadlocks\n\nThere are a lot of things that can go wrong when a new process is spawned, with the most common cause of deadlocks being background threads. If there’s any thread that holds a lock or imports a module, and fork is called, it’s very likely that the subprocess will be in a corrupted state and will deadlock or fail in a different way. Note that even if you don’t, Python built in libraries do - no need to look further than multiprocessing. multiprocessing.Queue is actually a very complex class, that spawns multiple threads used to serialize, send and receive objects, and they can cause aforementioned problems too. If you find yourself in such situation try using a SimpleQueue, that doesn’t use any additional threads.\n\nWe’re trying our best to make it easy for you and ensure these deadlocks don’t happen but some things are out of our control. If you have any issues you can’t cope with for a while, try reaching out on forums, and we’ll see if it’s an issue we can fix.\n\nReuse buffers passed through a Queue\n\nRemember that each time you put a Tensor into a multiprocessing.Queue, it has to be moved into shared memory. If it’s already shared, it is a no-op, otherwise it will incur an additional memory copy that can slow down the whole process. Even if you have a pool of processes sending data to a single one, make it send the buffers back - this is nearly free and will let you avoid a copy when sending next batch.\n\nAsynchronous multiprocess training (e.g. Hogwild)\n\nUsing torch.multiprocessing, it is possible to train a model asynchronously, with parameters either shared all the time, or being periodically synchronized. In the first case, we recommend sending over the whole model object, while in the latter, we advise to only send the state_dict().\n\nWe recommend using multiprocessing.Queue for passing all kinds of PyTorch objects between processes. It is possible to e.g. inherit the tensors and storages already in shared memory, when using the fork start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.\n\nWARNING\n\nYou should be careful about having global statements, that are not guarded with an if __name__ == '__main__'. If a different start method than fork is used, they will be executed in all subprocesses.\n\nHogwild\n\nA concrete Hogwild implementation can be found in the examples repository, but to showcase the overall structure of the code, there’s also a minimal example below as well:\n\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\nCPU in multiprocessing\n\nInappropriate multiprocessing can lead to CPU oversubscription, causing different processes to compete for CPU resources, resulting in low efficiency.\n\nThis tutorial will explain what CPU oversubscription is and how to avoid it.\n\nCPU oversubscription\n\nCPU oversubscription is a technical term that refers to a situation where the total number of vCPUs allocated to a system exceeds the total number of vCPUs available on the hardware.\n\nThis leads to severe contention for CPU resources. In such cases, there is frequent switching between processes, which increases processes switching overhead and decreases overall system efficiency.\n\nSee CPU oversubscription with the code examples in the Hogwild implementation found in the example repository.\n\nWhen running the training example with the following command on CPU using 4 processes:\n\npython main.py --num-processes 4\n\n\nAssuming there are N vCPUs available on the machine, executing the above command will generate 4 subprocesses. Each subprocess will allocate N vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the machine only has N vCPUs available. Consequently, the different processes will compete for resources, leading to frequent process switching.\n\nThe following observations indicate the presence of CPU over subscription:\n\nHigh CPU Utilization: By using the htop command, you can observe that the CPU utilization is consistently high, often reaching or exceeding its maximum capacity. This indicates that the demand for CPU resources exceeds the available physical cores, causing contention and competition among processes for CPU time.\n\nFrequent Context Switching with Low System Efficiency: In an oversubscribed CPU scenario, processes compete for CPU time, and the operating system needs to rapidly switch between different processes to allocate resources fairly. This frequent context switching adds overhead and reduces the overall system efficiency.\n\nAvoid CPU oversubscription\n\nA good way to avoid CPU oversubscription is proper resource allocation. Ensure that the number of processes or threads running concurrently does not exceed the available CPU resources.\n\nIn this case, a solution would be to specify the appropriate number of threads in the subprocesses. This can be achieved by setting the number of threads for each process using the torch.set_num_threads(int) function in subprocess.\n\nAssuming there are N vCPUs on the machine and M processes will be generated, the maximum num_threads value used by each process would be floor(N/M). To avoid CPU oversubscription in the mnist_hogwild example, the following changes are needed for the file train.py in example repository.\n\ndef train(rank, args, model, device, dataset, dataloader_kwargs):\n    torch.manual_seed(args.seed + rank)\n\n    #### define the num threads used in current sub-processes\n    torch.set_num_threads(floor(N/M))\n\n    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, device, train_loader, optimizer)\n\n\nSet num_thread for each process using torch.set_num_threads(floor(N/M)). where you replace N with the number of vCPUs available and M with the chosen number of processes. The appropriate num_thread value will vary depending on the specific task at hand. However, as a general guideline, the maximum value for the num_thread should be floor(N/M) to avoid CPU oversubscription. In the mnist_hogwild training example, after avoiding CPU over subscription, you can achieve a 30x performance boost.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nMultiprocessing best practices\nCUDA in multiprocessing\nBest practices and tips\nCPU in multiprocessing\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "MPS backend — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/mps.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > MPS backend\nMPS BACKEND\n\nmps device enables high-performance training on GPU for MacOS devices with Metal programming framework. It introduces a new device to map Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively.\n\nThe new MPS backend extends the PyTorch ecosystem and provides existing scripts capabilities to setup and run operations on GPU.\n\nTo get started, simply move your Tensor and Module to the mps device:\n\n# Check that MPS is available\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not \"\n              \"built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled device on this machine.\")\n\nelse:\n    mps_device = torch.device(\"mps\")\n\n    # Create a Tensor directly on the mps device\n    x = torch.ones(5, device=mps_device)\n    # Or\n    x = torch.ones(5, device=\"mps\")\n\n    # Any operation happens on the GPU\n    y = x * 2\n\n    # Move your model to mps just like any other device\n    model = YourFavoriteNet()\n    model.to(mps_device)\n\n    # Now every call runs on the GPU\n    pred = model(x)\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Features for large-scale deployments — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/large_scale_deployments.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Features for large-scale deployments\nShortcuts\nFEATURES FOR LARGE-SCALE DEPLOYMENTS\n\nFleet-wide operator profiling\n\nAPI usage logging\n\nAttaching metadata to saved TorchScript models\n\nBuild environment considerations\n\nCommon extension points\n\nThis note talks about several extension points and tricks that might be useful when running PyTorch within a larger system or operating multiple systems using PyTorch in a larger organization.\n\nIt doesn’t cover topics of deploying models to production. Check torch.jit or one of the corresponding tutorials.\n\nThe note assumes that you either build PyTorch from source in your organization or have an ability to statically link additional code to be loaded when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that can be triggered once in a centralized place, e.g. in static initialization code.\n\nFleet-wide operator profiling\n\nPyTorch comes with torch.autograd.profiler capable of measuring time taken by individual operators on demand. One can use the same mechanism to do “always ON” measurements for any process running PyTorch. It might be useful for gathering information about PyTorch workloads running in a given process or across the entire set of machines.\n\nNew callbacks for any operator invocation can be added with torch::addGlobalCallback. Hooks will be called with torch::RecordFunction struct that describes invocation context (e.g. name). If enabled, RecordFunction::inputs() contains arguments of the function represented as torch::IValue variant type. Note, that inputs logging is relatively expensive and thus has to be enabled explicitly.\n\nThe operator callbacks also have access to c10::ThreadLocalDebugInfo::get() interface that returns a pointer to the struct holding the debug information. This debug information can be set earlier by using at::DebugInfoGuard object. Debug information is propagated through the forward (including async fork tasks) and backward passes and can be useful for passing some extra information about execution environment (e.g. model id) from the higher layers of the application down to the operator callbacks.\n\nInvoking callbacks adds some overhead, so usually it’s useful to just randomly sample operator invocations. This can be enabled on per-callback basis with an optional sampling rate passed into torch::addGlobalCallback.\n\nNote, that addGlobalCallback is not thread-safe and can be called only when no PyTorch operator is running. Usually, it’s a good idea to call them once during initialization.\n\nHere’s an example:\n\n// Called somewhere in the program beginning\nvoid init() {\n    // Sample one in a hundred operator runs randomly\n    addGlobalCallback(\n      RecordFunctionCallback(\n        &onFunctionEnter,\n        &onFunctionExit)\n      .needsInputs(true)\n      .samplingProb(0.01)\n    );\n    // Note, to enable observers in the model calling thread,\n    // call enableRecordFunction() in the thread before running a model\n}\n\nvoid onFunctionEnter(const RecordFunction& fn) {\n    std::cerr << \"Before function \" << fn.name()\n              << \" with \" << fn.inputs().size() << \" inputs\" << std::endl;\n}\n\nvoid onFunctionExit(const RecordFunction& fn) {\n    std::cerr << \"After function \" << fn.name();\n}\n\nAPI usage logging\n\nWhen running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in one-off python scripts, the callback fires only once for a given process for each of the APIs.\n\nc10::SetAPIUsageHandler can be used to register API usage instrumentation handler. Passed argument is going to be an “api key” identifying used point, for example python.import for PyTorch extension import or torch.script.compile if TorchScript compilation was triggered.\n\nSetAPIUsageLogger([](const std::string& event_name) {\n    std::cerr << \"API was used: \" << event_name << std::endl;\n});\n\n\nNote for developers: new API trigger points can be added in code with C10_LOG_API_USAGE_ONCE(\"my_api\") in C++ or torch._C._log_api_usage_once(\"my.api\") in Python.\n\nAttaching metadata to saved TorchScript models\n\nTorchScript modules can be saved as an archive file that bundles serialized parameters and module code as TorchScript (see torch.jit.save()). It’s often convenient to bundle additional information together with the model, for example, description of model producer or auxiliary artifacts.\n\nIt can be achieved by passing the _extra_files argument to torch.jit.save() and torch::jit::load to store and retrieve arbitrary binary blobs during saving process. Since TorchScript files are regular ZIP archives, extra information gets stored as regular files inside archive’s extra/ directory.\n\nThere’s also a global hook allowing to attach extra files to any TorchScript archive produced in the current process. It might be useful to tag models with producer metadata, akin to JPEG metadata produced by digital cameras. Example usage might look like:\n\nSetExportModuleExtraFilesHook([](const Module&) {\n    ExtraFilesMap files;\n    files[\"producer_info.json\"] = \"{\\\"user\\\": \\\"\" + getenv(\"USER\") + \"\\\"}\";\n    return files;\n});\n\nBuild environment considerations\n\nTorchScript’s compilation needs to have access to the original python files as it uses python’s inspect.getsource call. In certain production environments it might require explicitly deploying .py files along with precompiled .pyc.\n\nCommon extension points\n\nPyTorch APIs are generally loosely coupled and it’s easy to replace a component with specialized version. Common extension points include:\n\nCustom operators implemented in C++ - see tutorial for more details.\n\nCustom data reading can be often integrated directly by invoking corresponding python library. Existing functionality of torch.utils.data can be utilized by extending Dataset or IterableDataset.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nFeatures for large-scale deployments\nFleet-wide operator profiling\nAPI usage logging\nAttaching metadata to saved TorchScript models\nBuild environment considerations\nCommon extension points\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "HIP (ROCm) semantics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/hip.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > HIP (ROCm) semantics\nShortcuts\nHIP (ROCM) SEMANTICS\n\nROCm™ is AMD’s open source software platform for GPU-accelerated high performance computing and machine learning. HIP is ROCm’s C++ dialect designed to ease conversion of CUDA applications to portable C++ code. HIP is used when converting existing CUDA applications like PyTorch to portable C++ and for new projects that require portability between AMD and NVIDIA.\n\nHIP Interfaces Reuse the CUDA Interfaces\n\nPyTorch for HIP intentionally reuses the existing torch.cuda interfaces. This helps to accelerate the porting of existing PyTorch code and models because very few code changes are necessary, if any.\n\nThe example from CUDA semantics will work exactly the same for HIP:\n\ncuda = torch.device('cuda')     # Default HIP device\ncuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\nChecking for HIP\n\nWhether you are using PyTorch for CUDA or HIP, the result of calling is_available() will be the same. If you are using a PyTorch that has been built with GPU support, it will return True. If you must check which version of PyTorch you are using, refer to this example below:\n\nif torch.cuda.is_available() and torch.version.hip:\n    # do something specific for HIP\nelif torch.cuda.is_available() and torch.version.cuda:\n    # do something specific for CUDA\n\nTensorFloat-32(TF32) on ROCm\n\nTF32 is not supported on ROCm.\n\nMemory management\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in rocm-smi. You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\n\nFor more advanced users, we offer more comprehensive memory benchmarking via memory_stats(). We also offer the capability to capture a complete snapshot of the memory allocator state via memory_snapshot(), which can help you understand the underlying allocation patterns produced by your code.\n\nTo debug memory errors, set PYTORCH_NO_CUDA_MEMORY_CACHING=1 in your environment to disable caching.\n\nhipFFT/rocFFT plan cache\n\nSetting the size of the cache for hipFFT/rocFFT plans is not supported.\n\ntorch.distributed backends\n\nCurrently, only the “nccl” and “gloo” backends for torch.distributed are supported on ROCm.\n\nCUDA API to HIP API mappings in C++\n\nPlease refer: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html\n\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\n\nFor example: Instead of using\n\n#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 to implicitly exclude ROCm/HIP,\n\nuse the following to not take the code path for ROCm/HIP:\n\n#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)\n\nAlternatively, if it is desired to take the code path for ROCm/HIP:\n\n#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)\n\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:\n\n#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)\n\nRefer to CUDA Semantics doc\n\nFor any sections not listed here, please refer to the CUDA semantics doc: CUDA semantics\n\nEnabling kernel asserts\n\nKernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled by recompiling the PyTorch from source.\n\nPlease add below line as an argument to cmake command parameters:\n\n-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nHIP (ROCm) semantics\nHIP Interfaces Reuse the CUDA Interfaces\nChecking for HIP\nTensorFloat-32(TF32) on ROCm\nMemory management\nhipFFT/rocFFT plan cache\ntorch.distributed backends\nCUDA API to HIP API mappings in C++\nRefer to CUDA Semantics doc\nEnabling kernel asserts\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Frequently Asked Questions — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/faq.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Frequently Asked Questions\nShortcuts\nFREQUENTLY ASKED QUESTIONS\nMy model reports “cuda runtime error(2): out of memory”\n\nAs the error message suggests, you have run out of memory on your GPU. Since we often deal with large amounts of data in PyTorch, small mistakes can rapidly cause your program to use up all of your GPU; fortunately, the fixes in these cases are often simple. Here are a few common things to check:\n\nDon’t accumulate history across your training loop. By default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.\n\nSometimes, it can be non-obvious when differentiable variables can occur. Consider the following training loop (abridged from source):\n\ntotal_loss = 0\nfor i in range(10000):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss\n\n\nHere, total_loss is accumulating history across your training loop, since loss is a differentiable variable with autograd history. You can fix this by writing total_loss += float(loss) instead.\n\nOther instances of this problem: 1.\n\nDon’t hold onto tensors and variables you don’t need. If you assign a Tensor or Variable to a local, Python will not deallocate until the local goes out of scope. You can free this reference by using del x. Similarly, if you assign a Tensor or Variable to a member variable of an object, it will not deallocate until the object goes out of scope. You will get the best memory usage if you don’t hold onto temporaries you don’t need.\n\nThe scopes of locals can be larger than you expect. For example:\n\nfor i in range(5):\n    intermediate = f(input[i])\n    result += g(intermediate)\noutput = h(result)\nreturn output\n\n\nHere, intermediate remains live even while h is executing, because its scope extrudes past the end of the loop. To free it earlier, you should del intermediate when you are done with it.\n\nAvoid running RNNs on sequences that are too large. The amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long.\n\nThe technical term for this phenomenon is backpropagation through time, and there are plenty of references for how to implement truncated BPTT, including in the word language model example; truncation is handled by the repackage function as described in this forum post.\n\nDon’t use linear layers that are too large. A linear layer nn.Linear(m, n) uses \n𝑂\n(\n𝑛\n𝑚\n)\nO(nm) memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to blow through your memory this way (and remember that you will need at least twice the size of the weights, since you also need to store the gradients.)\n\nConsider checkpointing. You can trade-off memory for compute by using checkpoint.\n\nMy GPU memory isn’t freed properly\n\nPyTorch uses a caching memory allocator to speed up memory allocations. As a result, the values shown in nvidia-smi usually don’t reflect the true memory usage. See Memory management for more details about GPU memory management.\n\nIf your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid].\n\nMy out of memory exception handler can’t allocate memory\n\nYou may have some code that tries to recover from out of memory errors.\n\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    for _ in range(batch_size):\n        run_model(1)\n\n\nBut find that when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the except clause.\n\noom = False\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    oom = True\n\nif oom:\n    for _ in range(batch_size):\n        run_model(1)\n\nMy data loader workers return identical random numbers\n\nYou are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via fork. See torch.utils.data.DataLoader’s documentation for how to properly set up random seeds in workers with its worker_init_fn option.\n\nMy recurrent network doesn’t work with data parallelism\n\nThere is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module with DataParallel or data_parallel(). Input to each the forward() on each device will only be part of the entire input. Because the unpack operation torch.nn.utils.rnn.pad_packed_sequence() by default only pads up to the longest input it sees, i.e., the longest on that particular device, size mismatches will happen when results are gathered together. Therefore, you can instead take advantage of the total_length argument of pad_packed_sequence() to make sure that the forward() calls return sequences of same length. For example, you can write:\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass MyModule(nn.Module):\n    # ... __init__, other methods, etc.\n\n    # padded_input is of shape [B x T x *] (batch_first mode) and contains\n    # the sequences sorted by lengths\n    #   B is the batch size\n    #   T is max sequence length\n    def forward(self, padded_input, input_lengths):\n        total_length = padded_input.size(1)  # get the max sequence length\n        packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                            batch_first=True)\n        packed_output, _ = self.my_lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                        total_length=total_length)\n        return output\n\n\nm = MyModule().cuda()\ndp_m = nn.DataParallel(m)\n\n\nAdditionally, extra care needs to be taken when batch dimension is dim 1 (i.e., batch_first=False) with data parallelism. In this case, the first argument of pack_padded_sequence padding_input will be of shape [T x B x *] and should be scattered along dim 1, but the second argument input_lengths will be of shape [B] and should be scattered along dim 0. Extra code to manipulate the tensor shapes will be needed.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nFrequently Asked Questions\nMy model reports “cuda runtime error(2): out of memory”\nMy GPU memory isn’t freed properly\nMy out of memory exception handler can’t allocate memory\nMy data loader workers return identical random numbers\nMy recurrent network doesn’t work with data parallelism\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Extending PyTorch — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/extending.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Extending PyTorch\nShortcuts\nEXTENDING PYTORCH\n\nIn this note we’ll cover ways of extending torch.nn, torch.autograd, torch, and writing custom C++ extensions.\n\nExtending torch.autograd\n\nAdding operations to autograd requires implementing a new Function subclass for each operation. Recall that Functions are what autograd uses to encode the operation history and compute gradients.\n\nThe first part of this doc is focused on backward mode AD as it is the most widely used feature. A section at the end discusses the extensions for forward mode AD.\n\nWhen to use\n\nIn general, implement a custom function if you want to perform computations in your model that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine.\n\nIn some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a C++ extension, you can wrap them in Function to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together.\n\nWhen not to use\n\nIf you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function.\n\nIf you need to maintain state, i.e., trainable parameters, you should (also) use a custom module. See the section below for more information on extending torch.nn.\n\nIf you’d like to alter the gradients during the backward pass or perform a side effect, consider registering a tensor or Module hook.\n\nHow to use\n\nTake the following steps: 1. Subclass Function and implement the forward(), (optional) setup_context() and backward() methods. 2. Call the proper methods on the ctx argument. 3. Declare whether your function supports double backward. 4. Validate whether your gradients are correct using gradcheck.\n\nStep 1: After subclassing Function, you’ll need to define 3 methods:\n\nforward() is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. Tensor arguments that track history (i.e., with requires_grad=True) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single Tensor output, or a tuple of tensors if there are multiple outputs. Also, please refer to the docs of Function to find descriptions of useful methods that can be called only from forward().\n\nsetup_context() (optional). One can either write a “combined” forward() that accepts a ctx object or (as of PyTorch 2.0) a separate forward() that does not accept ctx and a setup_context() method where the ctx modification happens. The forward() should have the compute and setup_context() should only be responsible for the ctx modification (and not have any compute). In general the separate forward() and setup_context() is closer to how PyTorch native operations work and therefore more composable with various PyTorch subsystems. See Combined or separate forward() and setup_context() for more details.\n\nbackward() (or vjp()) defines the gradient formula. It will be given as many Tensor arguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient (needs_input_grad is a tuple of booleans indicating whether each input needs gradient computation), or were non-Tensor objects, you can return python:None. Also, if you have optional arguments to forward() you can return more gradients than there were inputs, as long as they’re all None.\n\nStep 2: It is your responsibility to use the functions in ctx properly in order to ensure that the new Function works properly with the autograd engine.\n\nsave_for_backward() must be used to save any tensors to be used in the backward pass. Non-tensors should be stored directly on ctx. If tensors that are neither input nor output are saved for backward your Function may not support double backward (see step 3).\n\nmark_dirty() must be used to mark any input that is modified inplace by the forward function.\n\nmark_non_differentiable() must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.\n\nset_materialize_grads() can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True.\n\nStep 3: If your Function does not support double backward you should explicitly declare this by decorating backward with the once_differentiable(). With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward.\n\nStep 4: It is recommended that you use torch.autograd.gradcheck() to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.\n\nExample\n\nBelow you can find code for a Linear function, with additional comments:\n\n# Inherit from Function\nclass LinearFunction(Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(input, weight, bias):\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx, inputs, output):\n        input, weight, bias = inputs\n        ctx.save_for_backward(input, weight, bias)\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\n\nNow, to make it easier to use these custom ops, we recommend either aliasing them or wrapping them in a function. Wrapping in a function lets us support default arguments and keyword arguments:\n\n# Option 1: alias\nlinear = LinearFunction.apply\n\n# Option 2: wrap in a function, to support default args and keyword args.\ndef linear(input, weight, bias=None):\n    return LinearFunction.apply(input, weight, bias)\n\n\nHere, we give an additional example of a function that is parametrized by non-Tensor arguments:\n\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        tensor, constant = inputs\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n\nAnd here, we optimize the above example by calling set_materialize_grads(False):\n\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        tensor, constant = inputs\n        ctx.set_materialize_grads(False)\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Here we must handle None grad_output tensor. In this case we\n        # can skip unnecessary computations and just return None.\n        if grad_output is None:\n            return None, None\n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n\nIf you need any “intermediate” Tensors computed in forward() to be saved, either they must be returned as outputs, or combine forward and setup_context() (see Combined or separate forward() and setup_context()). Note that this means if you want gradients to flow through those intermediate values, you need to define the gradient formula for them (see also the double backward tutorial ):\n\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        # We wish to save dx for backward. In order to do so, it must\n        # be returned as an output.\n        dx = 3 * x ** 2\n        result = x ** 3\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        # which is grad_dx * 6 * x.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n# Wrap MyCube in a function so that it is clearer what the output is\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\n\nNOTE\n\nInputs to backward, i.e., grad_output, can also be tensors that track history. So if backward is implemented with differentiable operations, (e.g., invocation of another custom Function), higher order derivatives will work. In this case, the tensors saved with save_for_backward can also be used in the backward and have gradients flowing back but tensors saved in the ctx won’t have gradients flowing back for them. If you need gradients to flow back for a Tensor saved in the ctx, you should make it an output of the custom Function and save it with save_for_backward.\n\nYou probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences:\n\nfrom torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\ntest = gradcheck(linear, input, eps=1e-6, atol=1e-4)\nprint(test)\n\n\nSee Numerical gradient checking for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the gradgradcheck function from the same package to check higher order derivatives.\n\nCombined or separate forward() and setup_context()\n\nThere are two main ways to define Function. Either:\n\ndefine a forward() that combines the forward compute logic with setup_context()\n\n(as of PyTorch 2.0) define a separate forward() and setup_context()\n\nWe recommend the second option (separate forward() and setup_context()) because that is closer to how PyTorch native operations are implemented and it composes with torch.func transforms. However, we plan to support both approaches going forward; combining forward() with setup_context(): leads to more flexibility since you are able to save intermediates without returning them as output.\n\nPlease see the previous section for how to define Function with separate forward() and setup_context().\n\nHere is an example of how to define a Function with combined forward() and setup_context():\n\nclass LinearFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(ctx, input, weight, bias=None):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\nForward mode AD\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties. You can implement the jvp() function.\n\nIt will be given as many Tensor arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The jvp() will be called just after the forward() method, before the apply() returns.\n\njvp() has a few subtle differences with the backward() function:\n\nYou can use the ctx to pass any data from the forward() to the jvp() function. If that state will not be needed for the backward(), you can explicitly free it by doing del ctx.foo at the end of the jvp() function.\n\nThe implementation of jvp() must be backward differentiable or explicitly check that none of the given forward mode gradient has requires_grad set.\n\nThe jvp() function must match the view/inplace behavior of forward(). For example, if the i th input is modified inplace, then the i th gradient must be updated inplace. Similarly, if the j th output is a view of the k th input. Then the returned j th output gradient must be a view of the given k th input gradient.\n\nBecause the user cannot specify which gradient needs to be computed, the jvp() function should always compute gradients for all the outputs.\n\nThe forward mode gradients do respect the flag set by set_materialize_grads() and you can get None input gradients when this is disabled.\n\ntorch.func transforms and/or torch.vmap()\n\nPlease see Extending torch.func with autograd.Function for details.\n\nExtending torch.nn\n\nnn exports two kinds of interfaces - modules and their functional versions. You can extend it in both ways, but we recommend using modules for all kinds of layers, that hold any parameters or buffers, and recommend using a functional form parameter-less operations like activation functions, pooling, etc.\n\nAdding a functional version of an operation is already fully covered in the section above.\n\nAdding a Module\n\nSince nn heavily utilizes autograd, adding a new Module requires implementing a Function that performs the operation and can compute the gradient. From now on let’s assume that we want to implement a Linear module and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented:\n\n__init__ (optional) - takes in arguments such as kernel sizes, numbers of features, etc. and initializes parameters and buffers.\n\nforward() - instantiates a Function and uses it to perform the operation. It’s very similar to a functional wrapper shown above.\n\nThis is how a Linear module can be implemented:\n\nclass Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super().__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        if self.bias is not None:\n            nn.init.uniform_(self.bias, -0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'input_features={}, output_features={}, bias={}'.format(\n            self.input_features, self.output_features, self.bias is not None\n        )\n\nExtending torch Python API\n\nYou can create custom types that emulate Tensor by defining a custom class with methods that match Tensor. But what if you want to be able to pass these types to functions like torch.add() in the top-level torch namespace that accept Tensor operands?\n\nIf your custom Python type defines a method named __torch_function__, PyTorch will invoke your __torch_function__ implementation when an instance of your custom class is passed to a function in the torch namespace. This makes it possible to define custom implementations for any of the functions in the torch namespace which your __torch_function__ implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for Tensor. This works with “duck” types that are unrelated to Tensor as well as user-defined subclasses of Tensor.\n\nExtending torch with a Tensor-like type\n\nNOTE\n\nThis functionality is inspired by the NumPy __array_function__ protocol. See the NumPy documentation and NEP-0018 for more details.\n\nTo make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order N and value along the diagonal entries, value:\n\nclass ScalarTensor(object):\n   def __init__(self, N, value):\n       self._N = N\n       self._value = value\n\n   def __repr__(self):\n       return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n   def tensor(self):\n       return self._value * torch.eye(self._N)\n\n\nThis first iteration of the design isn’t very useful. The main functionality of ScalarTensor is to provide a more compact string representation of a scalar tensor than in the base tensor class:\n\n>>> d = ScalarTensor(5, 2)\n>>> d\nScalarTensor(N=5, value=2)\n>>> d.tensor()\ntensor([[2., 0., 0., 0., 0.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 0., 2., 0.],\n        [0., 0., 0., 0., 2.]])\n\n\nIf we try to use this object with the torch API, we will run into issues:\n\n>>> import torch\n>>> torch.mean(d)\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor\n\n\nAdding a __torch_function__ implementation to ScalarTensor makes it possible for the above operation to succeed. Let’s re-do our implementation, this time adding a __torch_function__ implementation:\n\nHANDLED_FUNCTIONS = {}\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n\nThe __torch_function__ method takes four arguments: func, a reference to the torch API function that is being overridden, types, the list of types of Tensor-likes that implement __torch_function__, args, the tuple of arguments passed to the function, and kwargs, the dict of keyword arguments passed to the function. It uses a global dispatch table named HANDLED_FUNCTIONS to store custom implementations. The keys of this dictionary are functions in the torch namespace and the values are implementations for ScalarTensor.\n\nNOTE\n\nUsing a global dispatch table is not a mandated part of the __torch_function__ API, it is just a useful design pattern for structuring your override implementations.\n\nThis class definition isn’t quite enough to make torch.mean do the right thing when we pass it a ScalarTensor – we also need to define an implementation for torch.mean for ScalarTensor operands and add the implementation to the HANDLED_FUNCTIONS dispatch table dictionary. One way of doing this is to define a decorator:\n\nimport functools\ndef implements(torch_function):\n    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n    def decorator(func):\n        functools.update_wrapper(func, torch_function)\n        HANDLED_FUNCTIONS[torch_function] = func\n        return func\n    return decorator\n\n\nwhich can be applied to the implementation of our override:\n\n@implements(torch.mean)\ndef mean(input):\n    return float(input._value) / input._N\n\n\nWith this change we can now use torch.mean with ScalarTensor:\n\n>>> d = ScalarTensor(5, 2)\n>>> torch.mean(d)\n0.4\n\n\nOf course torch.mean is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines __torch_function__, for example for torch.add():\n\ndef ensure_tensor(data):\n    if isinstance(data, ScalarTensor):\n        return data.tensor()\n    return torch.as_tensor(data)\n\n@implements(torch.add)\ndef add(input, other):\n   try:\n       if input._N == other._N:\n           return ScalarTensor(input._N, input._value + other._value)\n       else:\n           raise ValueError(\"Shape mismatch!\")\n   except AttributeError:\n       return torch.add(ensure_tensor(input), ensure_tensor(other))\n\n\nThis version has a fast path for when both operands are ScalarTensor instances and also a slower path which degrades to converting the data to tensors when either operand is not a ScalarTensor. That makes the override function correctly when either operand is a ScalarTensor or a regular Tensor:\n\n>>> s = ScalarTensor(2, 2)\n>>> torch.add(s, s)\nScalarTensor(N=2, value=4)\n>>> t = torch.tensor([[1, 1,], [1, 1]])\n>>> torch.add(s, t)\ntensor([[3., 1.],\n        [1., 3.]])\n\n\nNote that our implementation of add does not take alpha or out as keyword arguments like torch.add() does:\n\n>>> torch.add(s, s, alpha=2)\nTypeError: add() got an unexpected keyword argument 'alpha'\n\n\nFor speed and flexibility the __torch_function__ dispatch mechanism does not check that the signature of an override function matches the signature of the function being overrided in the torch API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with Tensor, user implementations of torch API functions should take care to exactly emulate the API of the function that is being overrided.\n\nFunctions in the torch API that do not have explicit overrides will return NotImplemented from __torch_function__. If all operands with __torch_function__ defined on them return NotImplemented, PyTorch will raise a TypeError. This means that most of the time operations that do not have explicit overrides for a type will raise a TypeError when an instance of such a type is passed:\n\n>>> torch.mul(s, 3)\nTypeError: no implementation found for 'torch.mul' on types that\nimplement __torch_function__: [ScalarTensor]\n\n\nIn practice this means that if you would like to implement your overrides using a __torch_function__ implementation along these lines, you will need to explicitly implement the full torch API or the entire subset of the API that you care about for your use case. This may be a tall order as the full torch API is quite extensive.\n\nAnother option is to not return NotImplemented for operations that are not handled but to instead pass a Tensor to the original torch function when no override is available. For example, if we change our implementation of __torch_function__ for ScalarTensor to the one below:\n\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n    if kwargs is None:\n        kwargs = {}\n    if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n        return func(*args, **kwargs)\n    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n\nThen torch.mul() will work correctly, although the return type will always be a Tensor rather than a ScalarTensor, even if both operands are ScalarTensor instances:\n\n>>> s = ScalarTensor(2, 2)\n>>> torch.mul(s, s)\ntensor([[4., 0.],\n        [0., 4.]])\n\n\nAlso see the MetadataTensor example below for another variation on this pattern but instead always returns a MetadataTensor to propagate metadata through operations in the torch API.\n\nThe __torch_function__ protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a TypeError. This is especially true for subclasses, where all three of torch.add, torch.Tensor.__add__ and torch.Tensor.add must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. If one requires the implementation of a function from torch.Tensor subclasses, they must use super().__torch_function__ inside their implementation.\n\nSubclassing torch.Tensor\n\nAs of version 1.7.0, methods on torch.Tensor and functions in public torch.* namespaces applied on torch.Tensor subclasses will return subclass instances instead of torch.Tensor instances:\n\n>>> class SubTensor(torch.Tensor):\n...     pass\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\n'SubTensor'\n>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__\n'SubTensor'\n\n\nIf multiple subclasses exist, the lowest one in the hierarchy will be chosen by default. If there is no unique way to determine such a case, then a TypeError is raised:\n\n>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__\n'SubTensor2'\n>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__\n'SubTensor2'\n>>> torch.add(SubTensor([0]), OtherSubTensor([1]))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n\n\nIf one wishes to have a global override for all tensor methods, one can use __torch_function__. Here is an example that logs all function/method calls:\n\nclass LoggingTensor(torch.Tensor):\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n        if func is not torch.Tensor.__repr__:\n            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n        if kwargs is None:\n            kwargs = {}\n        return super().__torch_function__(func, types, args, kwargs)\n\n\nHowever, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using __torch_function__ and matching with func.\n\nOne should be careful within __torch_function__ for subclasses to always call super().__torch_function__(func, ...) instead of func directly, as was the case before version 1.7.0. Failing to do this may cause func to recurse back into __torch_function__ and therefore cause infinite recursion.\n\nExtending torch with a Tensor wrapper type\n\nAnother useful case is a type that wraps a Tensor, either as an attribute or via subclassing. Below we implement a special case of this sort of type, a MetadataTensor that attaches a dictionary of metadata to a Tensor that is propagated through torch operations. Since this is a generic sort of wrapping for the full torch API, we do not need to individually implement each override so we can make the __torch_function__ implementation more permissive about what operations are allowed:\n\nclass MetadataTensor(object):\n    def __init__(self, data, metadata=None, **kwargs):\n        self._t = torch.as_tensor(data, **kwargs)\n        self._metadata = metadata\n\n    def __repr__(self):\n        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n        args = [getattr(a, '_t', a) for a in args]\n        assert len(metadatas) > 0\n        ret = func(*args, **kwargs)\n        return MetadataTensor(ret, metadata=metadatas[0])\n\n\nThis simple implementation won’t necessarily work with every function in the torch API but it is good enough to capture most common operations:\n\n>>> metadata = {'owner': 'Ministry of Silly Walks'}\n>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)\n>>> t = torch.tensor([[1, 2], [1, 2]])\n>>> torch.add(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[2, 4],\n        [4, 6]])\n>>> torch.mul(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[1, 4],\n        [3, 8]])\n\nOperations on multiple types that define __torch_function__\n\nIt is possible to use the torch API with multiple distinct types that each have a __torch_function__ implementation, but special care must be taken. In such a case the rules are:\n\nThe dispatch operation gathers all distinct implementations of __torch_function__ for each operand and calls them in order: subclasses before superclasses, and otherwise left to right in the operator expression.\n\nIf any value other than NotImplemented is returned, that value is returned as the result. Implementations can register that they do not implement an operation by returning NotImplemented.\n\nIf all of the __torch_function__ implementations return NotImplemented, PyTorch raises a TypeError.\n\nTesting Coverage of Overrides for the PyTorch API\n\nOne troublesome aspect of implementing __torch_function__ is that if some operations do and others do not have overrides, users will at best see an inconsistent experience, or at worst will see errors raised at runtime when they use a function that does not have an override. To ease this process, PyTorch provides a developer-facing API for ensuring full support for __torch_function__ overrides. This API is private and may be subject to changes without warning in the future.\n\nFirst, to get a listing of all overridable functions, use torch.overrides._get_overridable_functions. This returns a dictionary whose keys are namespaces in the PyTorch Python API and whose values are a list of functions in that namespace that can be overriden. For example, let’s print the names of the first 5 functions in torch.nn.functional that can be overriden:\n\n>>> from torch.overrides import get_overridable_functions\n>>> func_dict = get_overridable_functions()\n>>> nn_funcs = func_dict[torch.nn.functional]\n>>> print([f.__name__ for f in nn_funcs[:5])\n['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',\n 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']\n\n\nThis listing of functions makes it possible to iterate over all overridable functions, however in practice this is not enough to write tests for all of these functions without laboriously and manually copying the signature of each function for each test. To ease this process, the torch.overrides._get_testing_overrides function returns a dictionary mapping overridable functions in the PyTorch API to dummy lambda functions that have the same signature as the original function but unconditionally return -1. These functions are most useful to use with inspect to analyze the function signature of the original PyTorch function:\n\n>>> import inspect\n>>> from torch.overrides import get_testing_overrides\n>>> override_dict = get_testing_overrides()\n>>> dummy_add = override_dict[torch.add]\n>>> inspect.signature(dummy_add)\n<Signature (input, other, out=None)>\n\n\nFinally, torch.overrides.get_ignored_functions returns a tuple of functions that explicitly cannot be overrided by __torch_function__. This list can be useful to confirm that a function that isn’t present in the dictionary returned by get_overridable_functions cannot be overriden.\n\nExtending torch native API\n\nWhile __torch_function__ allows one to effectively extend PyTorch’s pure Python components’ behavior, it does not allow one to extend the parts of PyTorch implemented in C++. To that end, a Tensor subclass can also define __torch_dispatch__ which will be able to override the behavior at the C++ level.\n\nTo effectively use this feature, it is important to know how the native part of PyTorch is implemented. The most important component there is what we call the “dispatcher” (the best description can be found in this [blog post](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/) even though it is slightly outdated). As hinted by its name, it is responsible for calling the right backend function for a specific call of a function. For example, when calling torch.add(a, b), the dispatcher will inspect both arguments, figure out which “feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally call all the right kernels. A very common thing done by a kernel is to “redispatch”. For example, when running your neural network on GPU with autocast, the first call will be the autocast kernel that will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit.\n\nOne configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in DispatchKey.h inside the DispatchKey enum. For the purpose of extending torch, the important subset of the ordering for this discussion is: vmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends. The most important key for the purpose of this discussion is Python as every Tensor subclass with the __torch_dispatch__ method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided func again will perform a “redispatch”.\n\nSome important implications of this implementation are: - This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc). - If any high level feature implements a given function without redispatching, it will never reach the Python key and so the __torch_dispatch__ callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead. - When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None). - Our native functions are lazily populated as torch.ops.{namespace}.{func_name}.{overload_name} as callable Python objects to enable easily interacting with them from Python. The func object given to __torch_dispatch__ is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.\n\nIn a similar way where __torch_function__ is able to interpose on all of torch’s Python API and Tensor methods, __torch_dispatch__ is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: torch.add(a, 2) and a + 2 will lead to exactly the same aten call. Most of these functions are defined in native_functions.yaml which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen. Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.\n\nIt is also possible to add new native functions using torch.library. This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.\n\nYou can find many examples of __torch_dispatch__-based subclasses in the [subclass zoo](https://github.com/albanD/subclass_zoo) repo.\n\nExtending all torch API with Modes\n\nTODO Q: what about functions that don’t take tensor inputs?\n\nTODO Intro to the concept of mode\n\nTODO Example of logging mode\n\nWriting custom C++ extensions\n\nSee this PyTorch tutorial for a detailed explanation and examples.\n\nDocumentations are available at torch.utils.cpp_extension.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nExtending PyTorch\nExtending torch.autograd\nExtending torch.nn\nExtending torch Python API\nExtending torch native API\nExtending all torch API with Modes\nWriting custom C++ extensions\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Extending torch.func with autograd.Function — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/extending.func.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Extending torch.func with autograd.Function\nShortcuts\nEXTENDING TORCH.FUNC WITH AUTOGRAD.FUNCTION\n\nSo you’d like to use torch.autograd.Function with the torch.func transforms like torch.vmap(), torch.func.grad(), etc.\n\nThere are two main use cases:\n\nyou wish to call code that does not contain PyTorch operations and have it work with function transforms. That is, the torch.autograd.Function’s forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.\n\nyou wish to specify custom gradient rules, like JAX’s custom_vjp/custom_jvp\n\nPyTorch combines both of these concepts into torch.autograd.Function.\n\nBasic Usage\n\nThis guide assumes you are familiar with Extending torch.autograd, which explains how to use torch.autograd.Function.\n\ntorch.autograd.Function can either have a forward() that accepts a ctx object, or it can have separate forward() (that does not accept ctx) and a setup_context() staticmethod that modifies the ctx object.\n\nOnly the latter is supported with function transforms:\n\nforward() is the code that performs the operation and it should not accept a ctx object.\n\nsetup_context(ctx, inputs, output) is the code where you can call methods on ctx. Here is where you should save Tensors for backward (by calling ctx.save_for_backward(*tensors)), or save non-Tensors (by assigning them to the ctx object).\n\nBecause setup_context() accepts only inputs and output, the only quantities that can be saved are either objects (such as Tensors) in the inputs or outputs or quantities (like Tensor.shape) derived from them. If you wish to save a non-input intermediate activation from Function.forward() for backward, then you’ll need to return it as an output from forward() so that it gets passed to setup_context().\n\nDepending on the transform,\n\nto support reverse-mode AD (torch.func.grad(), torch.func.vjp()), the torch.autograd.Function needs a backward() staticmethod.\n\nto support torch.vmap(), the torch.autograd.Function needs a vmap() staticmethod.\n\nto support torch.func.jvp(), the torch.autograd.Function needs a jvp() staticmethod.\n\nto support compositions of transforms (like torch.func.jacrev(), torch.func.jacfwd(), torch.func.hessian()) – you may need multiple of the above.\n\nIn order for the torch.autograd.Function to be arbitrarily composable with function transforms, we recommend that all other staticmethods other than forward() and setup_context() must be transformable: that is, they must consist of only PyTorch operators or call other torch.autograd.Function (that may call into C++/CUDA/etc).\n\nLet’s go over some examples of common use cases.\n\nExample 1: autograd.Function calls into another system\n\nA common case is a torch.autograd.Function with both forward() and backward() calling into another system (like C++, CUDA, numpy, triton).\n\nimport torch\nimport numpy as np\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    # Note that forward does not take ctx\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        # Any intermediates to be saved in backward must be returned as\n        # outputs.\n        return (\n            # The desired output\n            torch.tensor(result, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind_inv, device=device),\n        )\n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    # Tensors together) in setup_context.\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        # Tuple with a single Tensor.\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        # assign them directly onto the ctx object.\n        ctx.save_for_backward(ind, ind_inv)\n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        # For the autograd.Function to be arbitrarily composable with function\n        # transforms, all staticmethod other than forward and setup_context\n        # must be implemented in a \"transformable\" way; that is, they must\n        # only consist of PyTorch operations or autograd.Function.\n        #\n        # For example, this allows us to do double backwards and/or compute\n        # second order gradients.\n        #\n        # We've written the backward pass of NumpySort in terms of another\n        # autograd.Function, NumpyTake.\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n\nNow, to make it easier to use NumpySort (to hide away the intermediates we returned as outputs, as well as allow default args and kwargs), we create a new function that invokes it:\n\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\n\nAnd here’s a sanity check:\n\nx = torch.randn(2, 3)\ngrad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\nassert torch.allclose(grad_x, torch.ones_like(x))\n\nExample 2: autograd.Function specifies custom gradient rules\n\nAnother common case is an torch.autograd.Function that is implemented with PyTorch operations. PyTorch is able to compute gradients for PyTorch operations automatically, but perhaps we wish to customize how the gradients are computed. Some reasons why we may want a custom backward different from the one PyTorch gives us are:\n\nimproving numeric stability\n\nchanging the performance characteristics of the backward\n\nchanging how edge cases are handled (e.g. nans, inf)\n\nmodifying the gradient (e.g. gradient clipping)\n\nHere’s an example of an torch.autograd.Function for the function y = x ** 3 where we change the performance characteristics (some computation that would normally happen during the backward pass, computing dx, happens in the forward pass).\n\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n        # that computation here in the forward pass instead.\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n\nNow, to make it easier to use NumpySort (and hide away the intermediates we returned as outputs) we create a new function that invokes it:\n\ndef my_cube(x):\n    result, _ = MyCube.apply(x)\n    return result\n\n\nHere’s a sanity check computing the second-order gradients:\n\nx = torch.randn([])\nggx = torch.func.grad(torch.func.grad(my_cube))(x)\nassert torch.allclose(ggx, 6 * x)\n\nLimitations and gotchas\n\nWARNING\n\nPlease read these limitations of torch.autograd.Function with torch.func transforms carefully. We are not able to catch many of these situations and error out gracefully so they will lead to undefined behavior.\n\nPlease do not capture Tensors that are being transformed over, have requires_grad=True, or are dual tensors, into the methods of the torch.autograd.Function. The way to be completely safe is to ensure that the only Tensors being used inside any method of the torch.autograd.Function must be directly passed as inputs (or via the ctx object) rather than come from outside the torch.autograd.Function.\n\ntorch.autograd.Function does not handle Tensors in pytrees (arbitrary nested Python data structures that may or may not contain Tensors). For those Tensors to be tracked by autograd, they must be passed directly as an argument to torch.autograd.Function. This is in contrast to jax.{custom_vjp, custom_jvp}, which do accept pytrees.\n\nPlease only use save_for_backward() or save_for_forward() to save Tensors. Please do not assign Tensors or collections of Tensors directly onto the ctx object - these Tensors will not get tracked\n\ntorch.vmap() Support\n\nTo use an torch.autograd.Function with torch.vmap(), you must either:\n\nprovide a vmap() staticmethod that tells us the behavior of the torch.autograd.Function under torch.vmap()\n\nask us to autogenerate it by setting generate_vmap_rule=True.\n\nAutomatically generate a vmap rule\n\nIf your torch.autograd.Function fulfills the following additional constraints, then we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you want custom behavior under vmap, please manually define a vmap staticmethod (see next section).\n\nWARNING\n\nWe are not easily able to check for the following constraints and error out gracefully. Violation of the constraints may lead to undefined behavior.\n\nThe torch.autograd.Function’s forward(), backward() (if it exists) and jvp() (if it exists) staticmethods must be transformable via torch.vmap(). That is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom CUDA kernels).\n\nExample:\n\nclass MyCube(torch.autograd.Function):\n    # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n    # a vmap rule.\n    generate_vmap_rule = True\n\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\nx = torch.randn(3)\nresult = torch.vmap(my_cube)(x)\nassert torch.allclose(result, x ** 3)\n\nDefining the vmap staticmethod\n\nIf your torch.autograd.Function calls into another system (like NumPy, C++, CUDA, triton), then to get it to work with torch.vmap() or transforms that use it, you’ll need to manually define a vmap() staticmethod.\n\nDepending on what transforms you want to use and your use case, you may not need to add a vmap() staticmethod to all of your torch.autograd.Function:\n\nFor example, torch.func.jacrev() performs vmap() over the backward pass. So if you’re only interested in using torch.func.jacrev(), only the backward() staticmethod needs to be vmappable.\n\nWe do recommend ensuring all of your torch.autograd.Function have support for torch.vmap() though, especially if you are writing a third-party library and you want your torch.autograd.Function to work with all combinations of torch.func() transforms.\n\nConceptually, the vmap staticmethod is responsible for defining how the forward() should behave under torch.vmap(). That is, it defines how to transform the forward() to run over inputs with an additional dimension (the dimension being vmapped over). This is similar to how torch.vmap() is implemented over PyTorch operations: for each operation, we define a vmap rule (sometimes also referred to as a “batching rule”).\n\nHere’s how to define the vmap() staticmethod:\n\nthe signature is vmap(info, in_dims: Tuple[Optional[int]], *args), where *args is the same as the args to forward().\n\nThe vmap staticmethod is responsible for defining how the forward() should behave under torch.vmap(). That is, given inputs with an additional dimension (specified by in_dims), how do we compute the batched version of forward()?\n\nFor each arg in args, in_dims has a corresponding Optional[int]. It is None if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.\n\ninfo is a collection of additional metadata that may be helpful: info.batch_size specifies the size of the dimension being vmapped over, while info.randomness is the randomness option that was passed to torch.vmap().\n\nThe return of the vmap staticmethod is a tuple of (output, out_dims). Similar to in_dims, out_dims should be of the same structure as output and contain one out_dim per output that specifies if the output has the vmapped dimension and what index it is in.\n\nExample:\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        return (\n            torch.tensor(result, device=device),\n            torch.tensor(ind, device=device),\n            torch.tensor(ind_inv, device=device),\n        )\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n    # The signature of the vmap staticmethod is:\n    # vmap(info, in_dims: Tuple[Optional[int]], *args)\n    # where *args is the same as the arguments to `forward`.\n    @staticmethod\n    def vmap(info, in_dims, x, dim):\n        # For every input (x and dim), in_dims stores an Optional[int]\n        # that is:\n        # - None if the input is not being vmapped over or if the input\n        #   is not a Tensor\n        # - an integer if the input is being vmapped over that represents\n        #   the index of the dimension being vmapped over.\n        x_bdim, _ = in_dims\n\n        # A \"vmap rule\" is the logic of how to perform the operation given\n        # inputs with one additional dimension. In NumpySort, x has an\n        # additional dimension (x_bdim). The vmap rule is simply\n        # to call NumpySort again but pass it a different `dim`.\n        x = x.movedim(x_bdim, 0)\n        # Handle negative dims correctly\n        dim = dim if dim >= 0 else dim + x.dim() - 1\n        result = NumpySort.apply(x, dim + 1)\n\n        # The vmap rule must return a tuple of two things\n        # 1. the output. Should be the same amount of things\n        #    as returned by the forward().\n        # 2. one Optional[int] for each output specifying if each output\n        # is being vmapped over, and if so, the index of the\n        # dimension being vmapped over.\n        #\n        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n        # dimension being vmapped over to the front of `x`, that appears at\n        # dimension 0 of all outputs.\n        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n        # and out_dims is a Tuple of 3 Optional[int]\n        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n    @staticmethod\n    def vmap(info, in_dims, x, ind, ind_inv, dim):\n        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n\n        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n        # being vmapped over.\n        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n\n        # Handle negative dims by wrapping them to be positive\n        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n        dim = dim if dim >= 0 else dim + logical_dim\n\n        def maybe_expand_bdim_at_front(x, x_bdim):\n            if x_bdim is None:\n                return x.expand(info.batch_size, *x.shape)\n            return x.movedim(x_bdim, 0)\n\n        # If the Tensor doesn't have the dimension being vmapped over,\n        # expand it out. Otherwise, move it to the front of the Tensor\n        x = maybe_expand_bdim_at_front(x, x_bdim)\n        ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n        # The return is a tuple (output, out_dims). Since output is a Tensor,\n        # then out_dims is an Optional[int] (instead of being a Tuple).\n        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\nx = torch.randn(2, 3)\nresult = torch.vmap(numpy_sort)(x)\nassert torch.allclose(result, numpy_sort(result, 1))\n\n\nNOTE\n\nThe vmap staticmethod should aim to preserve the semantics of the entire Function. That is, (pseudocode) grad(vmap(MyFunc)) should be replaceable with a grad(map(MyFunc)).\n\nIf your autograd.Function has any custom behavior in the backward pass, please keep this in mind.\n\nNOTE\n\nIt is a legitimate use case to write a custom vmap staticmethod for a Function that PyTorch is able to generate a vmap rule for via generate_vmap_rule=True. You may wish to do this if the generated vmap rule doesn’t have the semantics you’re looking for.\n\ntorch.func.jvp() Support\n\nTo support forward-mode AD, a torch.autograd.Function must have a jvp() staticmethod. Please see Forward mode AD for details.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nExtending torch.func with autograd.Function\nBasic Usage\ntorch.vmap() Support\ntorch.func.jvp() Support\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "CUDA semantics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/cuda.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > CUDA semantics\nShortcuts\nCUDA SEMANTICS\n\ntorch.cuda is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a torch.cuda.device context manager.\n\nHowever, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor.\n\nCross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality such as to() and cuda(). Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error.\n\nBelow you can find a small example showcasing this:\n\ncuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\nTensorFloat-32(TF32) on Ampere devices\n\nStarting in PyTorch 1.7, there is a new flag called allow_tf32. This flag defaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later. This flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores, available on new NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies and batched matrix multiplies) and convolutions.\n\nTF32 tensor cores are designed to achieve better performance on matmul and convolutions on torch.float32 tensors by rounding input data to have 10 bits of mantissa, and accumulating results with FP32 precision, maintaining FP32 dynamic range.\n\nmatmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:\n\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\ntorch.backends.cudnn.allow_tf32 = True\n\n\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses matmuls or convolutions are also affected. These include nn.Linear, nn.Conv*, cdist, tensordot, affine grid and grid sample, adaptive log softmax, GRU and LSTM.\n\nTo get an idea of the precision and speed, see the example code below:\n\na_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nb_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7277\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at TF32 mode.\ntorch.backends.cuda.matmul.allow_tf32 = True\nab_tf32 = a @ b  # takes 0.016s on GA100\nerror = (ab_tf32 - ab_full).abs().max()  # 0.1747\nrelative_error = error / mean  # 0.0022\n\n# Do matmul with TF32 disabled.\ntorch.backends.cuda.matmul.allow_tf32 = False\nab_fp32 = a @ b  # takes 0.11s on GA100\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0031\nrelative_error = error / mean  # 0.000039\n\n\nFrom the above example, we can see that with TF32 enabled, the speed is ~7x faster, relative error compared to double precision is approximately 2 orders of magnitude larger. If full FP32 precision is needed, users can disable TF32 by:\n\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\n\nTo toggle the TF32 flags off in C++, you can do\n\nat::globalContext().setAllowTF32CuBLAS(false);\nat::globalContext().setAllowTF32CuDNN(false);\n\n\nFor more information about TF32, see:\n\nTensorFloat-32\n\nCUDA 11\n\nAmpere architecture\n\nReduced Precision Reduction in FP16 GEMMs\n\nfp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large k dimension) and GPU architectures at the cost of numerical precision and potential for overflow.\n\nSome example benchmark data on V100:\n\n[--------------------------- bench_gemm_transformer --------------------------]\n      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False\n1 threads: --------------------------------------------------------------------\n      [4096, 4048, 4096]    |           1634.6        |           1639.8\n      [4096, 4056, 4096]    |           1670.8        |           1661.9\n      [4096, 4080, 4096]    |           1664.2        |           1658.3\n      [4096, 4096, 4096]    |           1639.4        |           1651.0\n      [4096, 4104, 4096]    |           1677.4        |           1674.9\n      [4096, 4128, 4096]    |           1655.7        |           1646.0\n      [4096, 4144, 4096]    |           1796.8        |           2519.6\n      [4096, 5096, 4096]    |           2094.6        |           3190.0\n      [4096, 5104, 4096]    |           2144.0        |           2663.5\n      [4096, 5112, 4096]    |           2149.1        |           2766.9\n      [4096, 5120, 4096]    |           2142.8        |           2631.0\n      [4096, 9728, 4096]    |           3875.1        |           5779.8\n      [4096, 16384, 4096]   |           6182.9        |           9656.5\n(times in microseconds).\n\n\nIf full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:\n\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\nat::globalContext().setAllowFP16ReductionCuBLAS(false);\n\nReduced Precision Reduction in BF16 GEMMs\n\nA similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is set to True by default for BF16, if you observe numerical instability in your workload, you may wish to set it to False.\n\nIf reduced precision reductions are not desired, users can disable reduced precision reductions in bf16 GEMMs with:\n\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\nat::globalContext().setAllowBF16ReductionCuBLAS(true);\n\nAsynchronous execution\n\nBy default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs.\n\nIn general, the effect of asynchronous computation is invisible to the caller, because (1) each device executes operations in the order they are queued, and (2) PyTorch automatically performs necessary synchronization when copying data between CPU and GPU or between two GPUs. Hence, computation will proceed as if every operation was executed synchronously.\n\nYou can force synchronous computation by setting environment variable CUDA_LAUNCH_BLOCKING=1. This can be handy when an error occurs on the GPU. (With asynchronous execution, such an error isn’t reported until after the operation is actually executed, so the stack trace does not show where it was requested.)\n\nA consequence of the asynchronous computation is that time measurements without synchronizations are not accurate. To get precise measurements, one should either call torch.cuda.synchronize() before measuring, or use torch.cuda.Event to record times as following:\n\nstart_event = torch.cuda.Event(enable_timing=True)\nend_event = torch.cuda.Event(enable_timing=True)\nstart_event.record()\n\n# Run some things here\n\nend_event.record()\ntorch.cuda.synchronize()  # Wait for the events to be recorded!\nelapsed_time_ms = start_event.elapsed_time(end_event)\n\n\nAs an exception, several functions such as to() and copy_() admit an explicit non_blocking argument, which lets the caller bypass synchronization when it is unnecessary. Another exception is CUDA streams, explained below.\n\nCUDA streams\n\nA CUDA stream is a linear sequence of execution that belongs to a specific device. You normally do not need to create one explicitly: by default, each device uses its own “default” stream.\n\nOperations inside each stream are serialized in the order they are created, but operations from different streams can execute concurrently in any relative order, unless explicit synchronization functions (such as synchronize() or wait_stream()) are used. For example, the following code is incorrect:\n\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\nwith torch.cuda.stream(s):\n    # sum() may start execution before normal_() finishes!\n    B = torch.sum(A)\n\n\nWhen the “current stream” is the default stream, PyTorch automatically performs necessary synchronization when data is moved around, as explained above. However, when using non-default streams, it is the user’s responsibility to ensure proper synchronization.\n\nStream semantics of backward passes\n\nEach backward CUDA op runs on the same stream that was used for its corresponding forward op. If your forward pass runs independent ops in parallel on different streams, this helps the backward pass exploit that same parallelism.\n\nThe stream semantics of a backward call with respect to surrounding ops are the same as for any other call. The backward pass inserts internal syncs to ensure this even when backward ops run on multiple streams as described in the previous paragraph. More concretely, when calling autograd.backward, autograd.grad, or tensor.backward, and optionally supplying CUDA tensor(s) as the initial gradient(s) (e.g., autograd.backward(..., grad_tensors=initial_grads), autograd.grad(..., grad_outputs=initial_grads), or tensor.backward(..., gradient=initial_grad)), the acts of\n\noptionally populating initial gradient(s),\n\ninvoking the backward pass, and\n\nusing the gradients\n\nhave the same stream-semantics relationship as any group of ops:\n\ns = torch.cuda.Stream()\n\n# Safe, grads are used in the same stream context as backward()\nwith torch.cuda.stream(s):\n    loss.backward()\n    use grads\n\n# Unsafe\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n# Safe, with synchronization\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n# Safe, populating initial grad and invoking backward are in the same stream context\nwith torch.cuda.stream(s):\n    loss.backward(gradient=torch.ones_like(loss))\n\n# Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n# without synchronization\ninitial_grad = torch.ones_like(loss)\nwith torch.cuda.stream(s):\n    loss.backward(gradient=initial_grad)\n\n# Safe, with synchronization\ninitial_grad = torch.ones_like(loss)\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    initial_grad.record_stream(s)\n    loss.backward(gradient=initial_grad)\n\nBC note: Using grads on the default stream\n\nIn prior versions of PyTorch (1.9 and earlier), the autograd engine always synced the default stream with all backward ops, so the following pattern:\n\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n\nwas safe as long as use grads happened on the default stream. In present PyTorch, that pattern is no longer safe. If backward() and use grads are in different stream contexts, you must sync the streams:\n\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n\neven if use grads is on the default stream.\n\nMemory management\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\n\nTo better understand how CUDA memory is being used over time, Understanding CUDA Memory Usage describes tools for capturing and visualizing traces of memory use.\n\nFor more advanced users, we offer more comprehensive memory benchmarking via memory_stats(). We also offer the capability to capture a complete snapshot of the memory allocator state via memory_snapshot(), which can help you understand the underlying allocation patterns produced by your code.\n\nEnvironment variables\n\nUse of a caching allocator can interfere with memory checking tools such as cuda-memcheck. To debug memory errors using cuda-memcheck, set PYTORCH_NO_CUDA_MEMORY_CACHING=1 in your environment to disable caching.\n\nThe behavior of the caching allocator can be controlled via the environment variable PYTORCH_CUDA_ALLOC_CONF. The format is PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>... Available options:\n\nbackend allows selecting the underlying allocator implementation. Currently, valid options are native, which uses PyTorch’s native implementation, and cudaMallocAsync, which uses CUDA’s built-in asynchronous allocator. cudaMallocAsync requires CUDA 11.4 or newer. The default is native. backend applies to all devices used by the process, and can’t be specified on a per-device basis.\n\nmax_split_size_mb prevents the native allocator from splitting blocks larger than this size (in MB). This can reduce fragmentation and may allow some borderline workloads to complete without running out of memory. Performance cost can range from ‘zero’ to ‘substantial’ depending on allocation patterns. Default value is unlimited, i.e. all blocks can be split. The memory_stats() and memory_summary() methods are useful for tuning. This option should be used as a last resort for a workload that is aborting due to ‘out of memory’ and showing a large amount of inactive split blocks. max_split_size_mb is only meaningful with backend:native. With backend:cudaMallocAsync, max_split_size_mb is ignored.\n\nroundup_power2_divisions helps with rounding the requested allocation size to nearest power-2 division and making better use of the blocks. In the native CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works fine for smaller sizes. However, this can be inefficient for large near-by allocations as each will go to different size of blocks and re-use of those blocks are minimized. This might create lots of unused blocks and will waste GPU memory capacity. This option enables the rounding of allocation size to nearest power-2 division. For example, if we need to round-up size of 1200 and if number of divisions is 4, the size 1200 lies between 1024 and 2048 and if we do 4 divisions between them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division. Specify a single value to apply for all allocation sizes or specify an array of key value pairs to set power-2 division individually for each power of two interval. For example to set 1 division for all allocations under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions for allocations between 512MB and 1GB and 8 divisions for any larger allocations, set the knob value to: [256:1,512:2,1024:4,>:8]. roundup_power2_divisions is only meaningful with backend:native. With backend:cudaMallocAsync, roundup_power2_divisions is ignored.\n\ngarbage_collection_threshold helps actively reclaiming unused GPU memory to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks), which can be unfavorable to latency-critical GPU applications (e.g., servers). Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80% of the total memory allocated to the GPU application). The algorithm prefers to free old & unused blocks first to avoid freeing blocks that are actively being reused. The threshold value should be between greater than 0.0 and less than 1.0. garbage_collection_threshold is only meaningful with backend:native. With backend:cudaMallocAsync, garbage_collection_threshold is ignored.\n\nexpandable_segments (experimental, default: False) If set to True, this setting instructs the allocator to create CUDA allocations that can later be expanded to better handle cases where a job changing allocation sizes frequently, such as having a changing batch size. Normally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations that are the same size as what the user requests. In the future, parts of these allocations can be reused for other requests if they are free. This works well when the program makes many requests of exactly the same size or of sizes that even multiples of that size. Many deep learning models follow this behavior. However, one common exception is when the batch size changes slightly from one iteration to the next, e.g. in batched inference. When the program runs initially with batch size N, it will make allocations appropriate for that size. If in the future, it runs at size N - 1, the existing allocations will still be big enough. However, if it runs at size N + 1, then it will have to make new allocations that are slightly larger. Not all the tensors are the same size. Some might be (N + 1)*A and others (N + 1)*A*B where A and B are some non-batch dimensions in the model. Because the allocator reuses existing allocations when they are big enough, some number of (N + 1)*A allocations will actually fit in the already existing N*B*A segments, though not perfectly. As the model runs it will partially fill up all of these segments leaving unusable free slices of memory at the end of these segments. The allocator at some point will need to cudaMalloc a new (N + 1)*A*B segment. If there is not enough memory, there is now no way to recover the slices of memory that are free at the end of existing segments. With models 50+ layers deep, this pattern might repeat 50+ times creating many slivers.\n\nexpandable_segments allows the allocator to create a segment initially and then expand its size later when more memory is needed. Instead of making one segment per allocation, it tries to make one segment (per stream) that grows as necessary. Now when the N + 1 case runs, the allocations will tile nicely into the one large segment until it fills up. Then more memory is requested and appended to the end of the segment. This process does not create as many slivers of unusable memory, so it is more likely to succeed at finding this memory.\n\nNOTE\n\nSome stats reported by the CUDA memory management API are specific to backend:native, and are not meaningful with backend:cudaMallocAsync. See each function’s docstring for details.\n\nUsing custom memory allocators for CUDA\n\nIt is possible to define allocators as simple functions in C/C++ and compile them as a shared library, the code below shows a basic allocator that just traces all the memory operations.\n\n#include <sys/types.h>\n#include <cuda_runtime_api.h>\n#include <iostream>\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\nextern \"C\" {\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream) {\n   void *ptr;\n   cudaMalloc(&ptr, size);\n   std::cout<<\"alloc \"<<ptr<<size<<std::endl;\n   return ptr;\n}\n\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\n   std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl;\n   cudaFree(ptr);\n}\n}\n\n\nThis can be used in python through the torch.cuda.memory.CUDAPluggableAllocator. The user is responsible for supplying the path to the .so file and the name of the alloc/free functions that match the signatures specified above.\n\nimport torch\n\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# Swap the current allocator\ntorch.cuda.memory.change_current_allocator(new_alloc)\n# This will allocate memory in the device using the new allocator\nb = torch.zeros(10, device='cuda')\n\nimport torch\n\n# Do an initial memory allocator\nb = torch.zeros(10, device='cuda')\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# This will error since the current allocator was already instantiated\ntorch.cuda.memory.change_current_allocator(new_alloc)\n\ncuBLAS workspaces\n\nFor each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated if that handle and stream combination executes a cuBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless torch._C._cuda_clearCublasWorkspaces() is called. The workspace size per allocation can be specified via the environment variable CUBLAS_WORKSPACE_CONFIG with the format :[SIZE]:[COUNT]. As an example, the default workspace size per allocation is CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8 which specifies a total size of 2 * 4096 + 8 * 16 KiB. To force cuBLAS to avoid using workspaces, set CUBLAS_WORKSPACE_CONFIG=:0:0.\n\ncuFFT plan cache\n\nFor each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly running FFT methods (e.g., torch.fft.fft()) on CUDA tensors of same geometry with same configuration. Because some cuFFT plans may allocate GPU memory, these caches have a maximum capacity.\n\nYou may control and query the properties of the cache of current device with the following APIs:\n\ntorch.backends.cuda.cufft_plan_cache.max_size gives the capacity of the cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions). Setting this value directly modifies the capacity.\n\ntorch.backends.cuda.cufft_plan_cache.size gives the number of plans currently residing in the cache.\n\ntorch.backends.cuda.cufft_plan_cache.clear() clears the cache.\n\nTo control and query plan caches of a non-default device, you can index the torch.backends.cuda.cufft_plan_cache object with either a torch.device object or a device index, and access one of the above attributes. E.g., to set the capacity of the cache for device 1, one can write torch.backends.cuda.cufft_plan_cache[1].max_size = 10.\n\nJust-in-Time Compilation\n\nPyTorch just-in-time compiles some operations, like torch.special.zeta, when performed on CUDA tensors. This compilation can be time consuming (up to a few seconds depending on your hardware and software) and may occur multiple times for a single operator since many PyTorch operators actually select from a variety of kernels, each of which must be compiled once, depending on their input. This compilation occurs once per process, or just once if a kernel cache is used.\n\nBy default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except on Windows, where the kernel cache is not yet supported). The caching behavior can be directly controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used as a kernel cache instead of the default location.\n\nBest practices\nDevice-agnostic code\n\nDue to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code; an example may be creating a new tensor as the initial hidden state of a recurrent neural network.\n\nThe first step is to determine whether the GPU should be used or not. A common pattern is to use Python’s argparse module to read in user arguments, and have a flag that can be used to disable CUDA, in combination with is_available(). In the following, args.device results in a torch.device object that can be used to move tensors to CPU or CUDA.\n\nimport argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='PyTorch Example')\nparser.add_argument('--disable-cuda', action='store_true',\n                    help='Disable CUDA')\nargs = parser.parse_args()\nargs.device = None\nif not args.disable_cuda and torch.cuda.is_available():\n    args.device = torch.device('cuda')\nelse:\n    args.device = torch.device('cpu')\n\n\nNOTE\n\nWhen assessing the availability of CUDA in a given environment (is_available()), PyTorch’s default behavior is to call the CUDA Runtime API method cudaGetDeviceCount. Because this call in turn initializes the CUDA Driver API (via cuInit) if it is not already initialized, subsequent forks of a process that has run is_available() will fail with a CUDA initialization error.\n\nOne can set PYTORCH_NVML_BASED_CUDA_CHECK=1 in your environment before importing PyTorch modules that execute is_available() (or before executing it directly) in order to direct is_available() to attempt an NVML-based assessment (nvmlDeviceGetCount_v2). If the NVML-based assessment is successful (i.e. NVML discovery/initialization does not fail), is_available() calls will not poison subsequent forks.\n\nIf NVML discovery/initialization fails, is_available() will fallback to the standard CUDA Runtime API assessment and the aforementioned fork constraint will apply.\n\nNote that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA Runtime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check may succeed while later CUDA initialization fails.\n\nNow that we have args.device, we can use it to create a Tensor on the desired device.\n\nx = torch.empty((8, 42), device=args.device)\nnet = Network().to(device=args.device)\n\n\nThis can be used in a number of cases to produce device agnostic code. Below is an example when using a dataloader:\n\ncuda0 = torch.device('cuda:0')  # CUDA GPU 0\nfor i, x in enumerate(train_loader):\n    x = x.to(cuda0)\n\n\nWhen working with multiple GPUs on a system, you can use the CUDA_VISIBLE_DEVICES environment flag to manage which GPUs are available to PyTorch. As mentioned above, to manually control which GPU a tensor is created on, the best practice is to use a torch.cuda.device context manager.\n\nprint(\"Outside device is 0\")  # On device 0 (default in most scenarios)\nwith torch.cuda.device(1):\n    print(\"Inside device is 1\")  # On device 1\nprint(\"Outside device is still 0\")  # On device 0\n\n\nIf you have a tensor and would like to create a new tensor of the same type on the same device, then you can use a torch.Tensor.new_* method (see torch.Tensor). Whilst the previously mentioned torch.* factory functions (Creation Ops) depend on the current GPU context and the attributes arguments you pass in, torch.Tensor.new_* methods preserve the device and other attributes of the tensor.\n\nThis is the recommended practice when creating modules in which new tensors need to be created internally during the forward pass.\n\ncuda = torch.device('cuda')\nx_cpu = torch.empty(2)\nx_gpu = torch.empty(2, device=cuda)\nx_cpu_long = torch.empty(2, dtype=torch.int64)\n\ny_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\nprint(y_cpu)\n\n    tensor([[ 0.3000,  0.3000],\n            [ 0.3000,  0.3000],\n            [ 0.3000,  0.3000]])\n\ny_gpu = x_gpu.new_full([3, 2], fill_value=-5)\nprint(y_gpu)\n\n    tensor([[-5.0000, -5.0000],\n            [-5.0000, -5.0000],\n            [-5.0000, -5.0000]], device='cuda:0')\n\ny_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\nprint(y_cpu_long)\n\n    tensor([[ 1,  2,  3]])\n\n\nIf you want to create a tensor of the same type and size of another tensor, and fill it with either ones or zeros, ones_like() or zeros_like() are provided as convenient helper functions (which also preserve torch.device and torch.dtype of a Tensor).\n\nx_cpu = torch.empty(2, 3)\nx_gpu = torch.empty(2, 3)\n\ny_cpu = torch.ones_like(x_cpu)\ny_gpu = torch.zeros_like(x_gpu)\n\nUse pinned memory buffers\n\nWARNING\n\nThis is an advanced tip. If you overuse pinned memory, it can cause serious problems when running low on RAM, and you should be aware that pinning is often an expensive operation.\n\nHost to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n\nAlso, once you pin a tensor or storage, you can use asynchronous GPU copies. Just pass an additional non_blocking=True argument to a to() or a cuda() call. This can be used to overlap data transfers with computation.\n\nYou can make the DataLoader return batches placed in pinned memory by passing pin_memory=True to its constructor.\n\nUse nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n\nMost use cases involving batched inputs and multiple GPUs should default to using DistributedDataParallel to utilize more than one GPU.\n\nThere are significant caveats to using CUDA models with multiprocessing; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n\nIt is recommended to use DistributedDataParallel, instead of DataParallel to do multi-GPU training, even if there is only a single node.\n\nThe difference between DistributedDataParallel and DataParallel is: DistributedDataParallel uses multiprocessing where a process is created for each GPU, while DataParallel uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n\nIf you use DistributedDataParallel, you could use torch.distributed.launch utility to launch your program, see Third-party backends.\n\nCUDA Graphs\n\nA CUDA graph is a record of the work (mostly kernels and their arguments) that a CUDA stream and its dependent streams perform. For general principles and details on the underlying CUDA API, see Getting Started with CUDA Graphs and the Graphs section of the CUDA C Programming Guide.\n\nPyTorch supports the construction of CUDA graphs using stream capture, which puts a CUDA stream in capture mode. CUDA work issued to a capturing stream doesn’t actually run on the GPU. Instead, the work is recorded in a graph.\n\nAfter capture, the graph can be launched to run the GPU work as many times as needed. Each replay runs the same kernels with the same arguments. For pointer arguments this means the same memory addresses are used. By filling input memory with new data (e.g., from a new batch) before each replay, you can rerun the same work on new data.\n\nWhy CUDA Graphs?\n\nReplaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for greatly reduced CPU overhead. A graph’s arguments and kernels are fixed, so a graph replay skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver overheads. Under the hood, a replay submits the entire graph’s work to the GPU with a single call to cudaGraphLaunch. Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead is the main benefit.\n\nYou should try CUDA graphs if all or part of your network is graph-safe (usually this means static shapes and static control flow, but see the other constraints) and you suspect its runtime is at least somewhat CPU-limited.\n\nPyTorch API\n\nWARNING\n\nThis API is in beta and may change in future releases.\n\nPyTorch exposes graphs via a raw torch.cuda.CUDAGraph class and two convenience wrappers, torch.cuda.graph and torch.cuda.make_graphed_callables.\n\ntorch.cuda.graph is a simple, versatile context manager that captures CUDA work in its context. Before capture, warm up the workload to be captured by running a few eager iterations. Warmup must occur on a side stream. Because the graph reads from and writes to the same memory addresses in every replay, you must maintain long-lived references to tensors that hold input and output data during capture. To run the graph on new input data, copy new data to the capture’s input tensor(s), replay the graph, then read the new output from the capture’s output tensor(s). Example:\n\ng = torch.cuda.CUDAGraph()\n\n# Placeholder input used for capture\nstatic_input = torch.empty((5,), device=\"cuda\")\n\n# Warmup before capture\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for _ in range(3):\n        static_output = static_input * 2\ntorch.cuda.current_stream().wait_stream(s)\n\n# Captures the graph\n# To allow capture, automatically sets a side stream as the current stream in the context\nwith torch.cuda.graph(g):\n    static_output = static_input * 2\n\n# Fills the graph's input memory with new data to compute on\nstatic_input.copy_(torch.full((5,), 3, device=\"cuda\"))\ng.replay()\n# static_output holds the results\nprint(static_output)  # full of 3 * 2 = 6\n\n# Fills the graph's input memory with more data to compute on\nstatic_input.copy_(torch.full((5,), 4, device=\"cuda\"))\ng.replay()\nprint(static_output)  # full of 4 * 2 = 8\n\n\nSee Whole-network capture, Usage with torch.cuda.amp, and Usage with multiple streams for realistic and advanced patterns.\n\nmake_graphed_callables is more sophisticated. make_graphed_callables accepts Python functions and torch.nn.Modules. For each passed function or Module, it creates separate graphs of the forward-pass and backward-pass work. See Partial-network capture.\n\nConstraints\n\nA set of ops is capturable if it doesn’t violate any of the following constraints.\n\nConstraints apply to all work in a torch.cuda.graph context and all work in the forward and backward passes of any callable you pass to torch.cuda.make_graphed_callables().\n\nViolating any of these will likely cause a runtime error:\n\nCapture must occur on a non-default stream. (This is only a concern if you use the raw CUDAGraph.capture_begin and CUDAGraph.capture_end calls. graph and make_graphed_callables() set a side stream for you.)\n\nOps that synchronize the CPU with the GPU (e.g., .item() calls) are prohibited.\n\nCUDA RNG ops are allowed, but must use default generators. For example, explicitly constructing a new torch.Generator instance and passing it as the generator argument to an RNG function is prohibited.\n\nViolating any of these will likely cause silent numerical errors or undefined behavior:\n\nWithin a process, only one capture may be underway at a time.\n\nNo non-captured CUDA work may run in this process (on any thread) while capture is underway.\n\nCPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.\n\nEvery replay reads from and writes to the same (virtual) memory addresses.\n\nDynamic control flow (based on CPU or GPU data) is prohibited.\n\nDynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence has the same size and layout in every replay.\n\nUsing multiple streams in a capture is allowed, but there are restrictions.\n\nNon-constraints\n\nOnce captured, the graph may be replayed on any stream.\n\nWhole-network capture\n\nIf your entire network is capturable, you can capture and replay an entire iteration:\n\nN, D_in, H, D_out = 640, 4096, 2048, 1024\nmodel = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n                            torch.nn.Dropout(p=0.2),\n                            torch.nn.Linear(H, D_out),\n                            torch.nn.Dropout(p=0.1)).cuda()\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n# Placeholders used for capture\nstatic_input = torch.randn(N, D_in, device='cuda')\nstatic_target = torch.randn(N, D_out, device='cuda')\n\n# warmup\n# Uses static_input and static_target here for convenience,\n# but in a real setting, because the warmup includes optimizer.step()\n# you must use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        y_pred = model(static_input)\n        loss = loss_fn(y_pred, static_target)\n        loss.backward()\n        optimizer.step()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\n# Sets grads to None before capture, so backward() will create\n# .grad attributes with allocations from the graph's private pool\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    static_y_pred = model(static_input)\n    static_loss = loss_fn(static_y_pred, static_target)\n    static_loss.backward()\n    optimizer.step()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    # Fills the graph's input memory with new data to compute on\n    static_input.copy_(data)\n    static_target.copy_(target)\n    # replay() includes forward, backward, and step.\n    # You don't even need to call optimizer.zero_grad() between iterations\n    # because the captured backward refills static .grad tensors in place.\n    g.replay()\n    # Params have been updated. static_y_pred, static_loss, and .grad\n    # attributes hold values from computing on this iteration's data.\n\nPartial-network capture\n\nIf some of your network is unsafe to capture (e.g., due to dynamic control flow, dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe part(s) eagerly and use torch.cuda.make_graphed_callables() to graph only the capture-safe part(s).\n\nBy default, callables returned by make_graphed_callables() are autograd-aware, and can be used in the training loop as direct replacements for the functions or nn.Modules you passed.\n\nmake_graphed_callables() internally creates CUDAGraph objects, runs warmup iterations, and maintains static inputs and outputs as needed. Therefore (unlike with torch.cuda.graph) you don’t need to handle those manually.\n\nIn the following example, data-dependent dynamic control flow means the network isn’t capturable end-to-end, but make_graphed_callables() lets us capture and run graph-safe sections as graphs regardless:\n\nN, D_in, H, D_out = 640, 4096, 2048, 1024\n\nmodule1 = torch.nn.Linear(D_in, H).cuda()\nmodule2 = torch.nn.Linear(H, D_out).cuda()\nmodule3 = torch.nn.Linear(H, D_out).cuda()\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(chain(module1.parameters(),\n                                  module2.parameters(),\n                                  module3.parameters()),\n                            lr=0.1)\n\n# Sample inputs used for capture\n# requires_grad state of sample inputs must match\n# requires_grad state of real inputs each callable will see.\nx = torch.randn(N, D_in, device='cuda')\nh = torch.randn(N, H, device='cuda', requires_grad=True)\n\nmodule1 = torch.cuda.make_graphed_callables(module1, (x,))\nmodule2 = torch.cuda.make_graphed_callables(module2, (h,))\nmodule3 = torch.cuda.make_graphed_callables(module3, (h,))\n\nreal_inputs = [torch.rand_like(x) for _ in range(10)]\nreal_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    optimizer.zero_grad(set_to_none=True)\n\n    tmp = module1(data)  # forward ops run as a graph\n\n    if tmp.sum().item() > 0:\n        tmp = module2(tmp)  # forward ops run as a graph\n    else:\n        tmp = module3(tmp)  # forward ops run as a graph\n\n    loss = loss_fn(tmp, target)\n    # module2's or module3's (whichever was chosen) backward ops,\n    # as well as module1's backward ops, run as graphs\n    loss.backward()\n    optimizer.step()\n\nUsage with torch.cuda.amp\n\nFor typical optimizers, GradScaler.step syncs the CPU with the GPU, which is prohibited during capture. To avoid errors, either use partial-network capture, or (if forward, loss, and backward are capture-safe) capture forward, loss, and backward but not the optimizer step:\n\n# warmup\n# In a real setting, use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            y_pred = model(static_input)\n            loss = loss_fn(y_pred, static_target)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    with torch.cuda.amp.autocast():\n        static_y_pred = model(static_input)\n        static_loss = loss_fn(static_y_pred, static_target)\n    scaler.scale(static_loss).backward()\n    # don't capture scaler.step(optimizer) or scaler.update()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    static_input.copy_(data)\n    static_target.copy_(target)\n    g.replay()\n    # Runs scaler.step and scaler.update eagerly\n    scaler.step(optimizer)\n    scaler.update()\n\nUsage with multiple streams\n\nCapture mode automatically propagates to any streams that sync with a capturing stream. Within capture, you may expose parallelism by issuing calls to different streams, but the overall stream dependency DAG must branch out from the initial capturing stream after capture begins and rejoin the initial stream before capture ends:\n\nwith torch.cuda.graph(g):\n    # at context manager entrance, torch.cuda.current_stream()\n    # is the initial capturing stream\n\n    # INCORRECT (does not branch out from or rejoin initial stream)\n    with torch.cuda.stream(s):\n        cuda_work()\n\n    # CORRECT:\n    # branches out from initial stream\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        cuda_work()\n    # rejoins initial stream before capture ends\n    torch.cuda.current_stream().wait_stream(s)\n\n\nNOTE\n\nTo avoid confusion for power users looking at replays in nsight systems or nvprof: Unlike eager execution, the graph interprets a nontrivial stream DAG in capture as a hint, not a command. During replay, the graph may reorganize independent ops onto different streams or enqueue them in a different order (while respecting your original DAG’s overall dependencies).\n\nUsage with DistributedDataParallel\nNCCL < 2.9.6\n\nNCCL versions earlier than 2.9.6 don’t allow collectives to be captured. You must use partial-network capture, which defers allreduces to happen outside graphed sections of backward.\n\nCall make_graphed_callables() on graphable network sections before wrapping the network with DDP.\n\nNCCL >= 2.9.6\n\nNCCL versions 2.9.6 or later allow collectives in the graph. Approaches that capture an entire backward pass are a viable option, but need three setup steps.\n\nDisable DDP’s internal async error handling:\n\nos.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\ntorch.distributed.init_process_group(...)\n\n\nBefore full-backward capture, DDP must be constructed in a side-stream context:\n\nwith torch.cuda.stream(s):\n    model = DistributedDataParallel(model)\n\n\nYour warmup must run at least 11 DDP-enabled eager iterations before capture.\n\nGraph memory management\n\nA captured graph acts on the same virtual addresses every time it replays. If PyTorch frees the memory, a later replay can hit an illegal memory access. If PyTorch reassigns the memory to new tensors, the replay can corrupt the values seen by those tensors. Therefore, the virtual addresses used by the graph must be reserved for the graph across replays. The PyTorch caching allocator achieves this by detecting when capture is underway and satisfying the capture’s allocations from a graph-private memory pool. The private pool stays alive until its CUDAGraph object and all tensors created during capture go out of scope.\n\nPrivate pools are maintained automatically. By default, the allocator creates a separate private pool for each capture. If you capture multiple graphs, this conservative approach ensures graph replays never corrupt each other’s values, but sometimes needlessly wastes memory.\n\nSharing memory across captures\n\nTo economize the memory stashed in private pools, torch.cuda.graph and torch.cuda.make_graphed_callables() optionally allow different captures to share the same private pool. It’s safe for a set of graphs to share a private pool if you know they’ll always be replayed in the same order they were captured, and never be replayed concurrently.\n\ntorch.cuda.graph’s pool argument is a hint to use a particular private pool, and can be used to share memory across graphs as shown:\n\ng1 = torch.cuda.CUDAGraph()\ng2 = torch.cuda.CUDAGraph()\n\n# (create static inputs for g1 and g2, run warmups of their workloads...)\n\n# Captures g1\nwith torch.cuda.graph(g1):\n    static_out_1 = g1_workload(static_in_1)\n\n# Captures g2, hinting that g2 may share a memory pool with g1\nwith torch.cuda.graph(g2, pool=g1.pool()):\n    static_out_2 = g2_workload(static_in_2)\n\nstatic_in_1.copy_(real_data_1)\nstatic_in_2.copy_(real_data_2)\ng1.replay()\ng2.replay()\n\n\nWith torch.cuda.make_graphed_callables(), if you want to graph several callables and you know they’ll always run in the same order (and never concurrently) pass them as a tuple in the same order they’ll run in the live workload, and make_graphed_callables() will capture their graphs using a shared private pool.\n\nIf, in the live workload, your callables will run in an order that occasionally changes, or if they’ll run concurrently, passing them as a tuple to a single invocation of make_graphed_callables() is not allowed. Instead, you must call make_graphed_callables() separately for each one.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nCUDA semantics\nTensorFloat-32(TF32) on Ampere devices\nReduced Precision Reduction in FP16 GEMMs\nReduced Precision Reduction in BF16 GEMMs\nAsynchronous execution\nMemory management\nUsing custom memory allocators for CUDA\ncuBLAS workspaces\ncuFFT plan cache\nJust-in-Time Compilation\nBest practices\nCUDA Graphs\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Autograd mechanics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/autograd.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Autograd mechanics\nShortcuts\nAUTOGRAD MECHANICS\n\nThis note will present an overview of how autograd works and records the operations. It’s not strictly necessary to understand all this, but we recommend getting familiar with it, as it will help you write more efficient, cleaner programs, and can aid you in debugging.\n\nHow autograd encodes the history\n\nAutograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n\nInternally, autograd represents this graph as a graph of Function objects (really expressions), which can be apply() ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forward pass is completed, we evaluate this graph in the backwards pass to compute the gradients.\n\nAn important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.\n\nSaved tensors\n\nSome operations need intermediary results to be saved during the forward pass in order to execute the backward pass. For example, the function \n𝑥\n↦\n𝑥\n2\nx↦x\n2\n saves the input \n𝑥\nx to compute the gradient.\n\nWhen defining a custom Python Function, you can use save_for_backward() to save tensors during the forward pass and saved_tensors to retrieve them during the backward pass. See Extending PyTorch for more information.\n\nFor operations that PyTorch defines (e.g. torch.pow()), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain grad_fn by looking for its attributes starting with the prefix _saved.\n\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\nprint(x.equal(y.grad_fn._saved_self))  # True\nprint(x is y.grad_fn._saved_self)  # True\n\n\nIn the previous code, y.grad_fn._saved_self refers to the same Tensor object as x. But that may not always be the case. For instance:\n\nx = torch.randn(5, requires_grad=True)\ny = x.exp()\nprint(y.equal(y.grad_fn._saved_result))  # True\nprint(y is y.grad_fn._saved_result)  # False\n\n\nUnder the hood, to prevent reference cycles, PyTorch has packed the tensor upon saving and unpacked it into a different tensor for reading. Here, the tensor you get from accessing y.grad_fn._saved_result is a different tensor object than y (but they still share the same storage).\n\nWhether a tensor will be packed into a different tensor object depends on whether it is an output of its own grad_fn, which is an implementation detail subject to change and that users should not rely on.\n\nYou can control how PyTorch does packing / unpacking with Hooks for saved tensors.\n\nGradients for non-differentiable functions\n\nThe gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable. Unfortunately many of the functions we use in practice do not have this property (relu or sqrt at 0, for example). To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:\n\nIf the function is differentiable and thus a gradient exists at the current point, use it.\n\nIf the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).\n\nIf the function is concave (at least locally), use the super-gradient of minimum norm (consider -f(x) and apply the previous point).\n\nIf the function is defined, define the gradient at the current point by continuity (note that inf is possible here, for example for sqrt(0)). If multiple values are possible, pick one arbitrarily.\n\nIf the function is not defined (sqrt(-1), log(-1) or most functions when the input is NaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use NaN as the gradient, but for performance reasons, some functions will use other values (log(-1), for example).\n\nIf the function is not a deterministic mapping (i.e. it is not a mathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of a no_grad environment.\n\nLocally disabling gradient computation\n\nThere are several mechanisms available from Python to locally disable gradient computation:\n\nTo disable gradients across entire blocks of code, there are context managers like no-grad mode and inference mode. For more fine-grained exclusion of subgraphs from gradient computation, there is setting the requires_grad field of a tensor.\n\nBelow, in addition to discussing the mechanisms above, we also describe evaluation mode (nn.Module.eval()), a method that is not used to disable gradient computation but, because of its name, is often mixed up with the three.\n\nSetting requires_grad\n\nrequires_grad is a flag, defaulting to false unless wrapped in a nn.Parameter, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:\n\nDuring the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass (.backward()), only leaf tensors with requires_grad=True will have gradients accumulated into their .grad fields.\n\nIt is important to note that even though every tensor has this flag, setting it only makes sense for leaf tensors (tensors that do not have a grad_fn, e.g., a nn.Module’s parameters). Non-leaf tensors (tensors that do have grad_fn) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. From this definition, it is clear that all non-leaf tensors will automatically have require_grad=True.\n\nSetting requires_grad should be the main way you control which parts of the model are part of the gradient computation, for example, if you need to freeze parts of your pretrained model during model fine-tuning.\n\nTo freeze parts of your model, simply apply .requires_grad_(False) to the parameters that you don’t want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won’t have their .grad fields updated in the backward pass because they won’t be part of the backward graph in the first place, as desired.\n\nBecause this is such a common pattern, requires_grad can also be set at the module level with nn.Module.requires_grad_(). When applied to a module, .requires_grad_() takes effect on all of the module’s parameters (which have requires_grad=True by default).\n\nGrad Modes\n\nApart from setting requires_grad there are also three grad modes that can be selected from Python that can affect how computations in PyTorch are processed by autograd internally: default mode (grad mode), no-grad mode, and inference mode, all of which can be togglable via context managers and decorators.\n\nMode\n\n\t\n\nExcludes operations from being recorded in backward graph\n\n\t\n\nSkips additional autograd tracking overhead\n\n\t\n\nTensors created while the mode is enabled can be used in grad-mode later\n\n\t\n\nExamples\n\n\n\n\ndefault\n\n\t\t\t\n\n✓\n\n\t\n\nForward pass\n\n\n\n\nno-grad\n\n\t\n\n✓\n\n\t\t\n\n✓\n\n\t\n\nOptimizer updates\n\n\n\n\ninference\n\n\t\n\n✓\n\n\t\n\n✓\n\n\t\t\n\nData processing, model evaluation\n\nDefault Mode (Grad Mode)\n\nThe “default mode” is the mode we are implicitly in when no other modes like no-grad and inference mode are enabled. To be contrasted with “no-grad mode” the default mode is also sometimes called “grad mode”.\n\nThe most important thing to know about the default mode is that it is the only mode in which requires_grad takes effect. requires_grad is always overridden to be False in both the two other modes.\n\nNo-grad Mode\n\nComputations in no-grad mode behave as if none of the inputs require grad. In other words, computations in no-grad mode are never recorded in the backward graph even if there are inputs that have require_grad=True.\n\nEnable no-grad mode when you need to perform operations that should not be recorded by autograd, but you’d still like to use the outputs of these computations in grad mode later. This context manager makes it convenient to disable gradients for a block of code or function without having to temporarily set tensors to have requires_grad=False, and then back to True.\n\nFor example, no-grad mode might be useful when writing an optimizer: when performing the training update you’d like to update parameters in-place without the update being recorded by autograd. You also intend to use the updated parameters for computations in grad mode in the next forward pass.\n\nThe implementations in torch.nn.init also rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the initialized parameters in-place.\n\nInference Mode\n\nInference mode is the extreme version of no-grad mode. Just like in no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more. This better runtime comes with a drawback: tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode.\n\nEnable inference mode when you are performing computations that don’t need to be recorded in the backward graph, AND you don’t plan on using the tensors created in inference mode in any computation that is to be recorded by autograd later.\n\nIt is recommended that you try out inference mode in the parts of your code that do not require autograd tracking (e.g., data processing and model evaluation). If it works out of the box for your use case it’s a free performance win. If you run into errors after enabling inference mode, check that you are not using tensors created in inference mode in computations that are recorded by autograd after exiting inference mode. If you cannot avoid such use in your case, you can always switch back to no-grad mode.\n\nFor details on inference mode please see Inference Mode.\n\nFor implementation details of inference mode see RFC-0011-InferenceMode.\n\nEvaluation Mode (nn.Module.eval())\n\nEvaluation mode is not a mechanism to locally disable gradient computation. It is included here anyway because it is sometimes confused to be such a mechanism.\n\nFunctionally, module.eval() (or equivalently module.train(False)) are completely orthogonal to no-grad mode and inference mode. How model.eval() affects your model depends entirely on the specific modules used in your model and whether they define any training-mode specific behavior.\n\nYou are responsible for calling model.eval() and model.train() if your model relies on modules such as torch.nn.Dropout and torch.nn.BatchNorm2d that may behave differently depending on training mode, for example, to avoid updating your BatchNorm running statistics on validation data.\n\nIt is recommended that you always use model.train() when training and model.eval() when evaluating your model (validation/testing) even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes.\n\nIn-place operations with autograd\n\nSupporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\n\nThere are two main reasons that limit the applicability of in-place operations:\n\nIn-place operations can potentially overwrite values required to compute gradients.\n\nEvery in-place operation requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the Function representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will raise an error if the storage of modified inputs is referenced by any other Tensor.\n\nIn-place correctness checks\n\nEvery tensor keeps a version counter, that is incremented every time it is marked dirty in any operation. When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access self.saved_tensors it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.\n\nMultithreaded Autograd\n\nThe autograd engine is responsible for running all the backward operations necessary to compute the backward pass. This section will describe all the details that can help you make the best use of it in a multithreaded environment. (This is relevant only for PyTorch 1.6+ as the behavior in previous version was different.)\n\nUser could train their model with multithreading code (e.g. Hogwild training), and does not block on the concurrent backward computations, example code could be:\n\n# Define a train function to be used in different threads\ndef train_fn():\n    x = torch.ones(5, 5, requires_grad=True)\n    # forward\n    y = (x + 3) * (x + 4) * 0.5\n    # backward\n    y.sum().backward()\n    # potential optimizer update\n\n\n# User write their own threading code to drive the train_fn\nthreads = []\nfor _ in range(10):\n    p = threading.Thread(target=train_fn, args=())\n    p.start()\n    threads.append(p)\n\nfor p in threads:\n    p.join()\n\n\nNote that some behaviors that user should be aware of:\n\nConcurrency on CPU\n\nWhen you run backward() or grad() via python or C++ API in multiple threads on CPU, you are expecting to see extra concurrency instead of serializing all the backward calls in a specific order during execution (behavior before PyTorch 1.6).\n\nNon-determinism\n\nIf you are calling backward() from multiple threads concurrently and have shared inputs (i.e. Hogwild CPU training), then non-determinism should be expected. This can occur because parameters are automatically shared across threads, as such, multiple threads may access and try to accumulate the same .grad attribute during gradient accumulation. This is technically not safe, and it might result in race condition and the result might be invalid to use.\n\nUsers developing multithreaded models featuring shared parameters should have the threading model in mind and should understand the issues described above.\n\nThe functional API torch.autograd.grad() may be used to calculate the gradients instead of backward() to avoid non-determinism.\n\nGraph retaining\n\nIf part of the autograd graph is shared between threads, i.e. run first part of forward single thread, then run second part in multiple threads, then the first part of graph is shared. In this case different threads execute grad() or backward() on the same graph might have issue of destroying the graph on the fly of one thread, and the other thread will crash in this case. Autograd will error out to the user similar to what call backward() twice with out retain_graph=True, and let the user know they should use retain_graph=True.\n\nThread Safety on Autograd Node\n\nSince Autograd allows the caller thread to drive its backward execution for potential parallelism, it’s important that we ensure thread safety on CPU with parallel backward() calls that share part/whole of the GraphTask.\n\nCustom Python autograd.Functions are automatically thread safe because of GIL. For built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom autograd::Functions, the Autograd Engine uses thread mutex locking to ensure thread safety on autograd Nodes that might have state write/read.\n\nNo thread safety on C++ hooks\n\nAutograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe.\n\nAutograd for Complex Numbers\n\nThe short version:\n\nWhen you use PyTorch to differentiate any function \n𝑓\n(\n𝑧\n)\nf(z) with complex domain and/or codomain, the gradients are computed under the assumption that the function is a part of a larger real-valued loss function \n𝑔\n(\n𝑖\n𝑛\n𝑝\n𝑢\n𝑡\n)\n=\n𝐿\ng(input)=L. The gradient computed is \n∂\n𝐿\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n (note the conjugation of z), the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers work out of the box with complex parameters.\n\nThis convention matches TensorFlow’s convention for complex differentiation, but is different from JAX (which computes \n∂\n𝐿\n∂\n𝑧\n∂z\n∂L\n\t​\n\n).\n\nIf you have a real-to-real function which internally uses complex operations, the convention here doesn’t matter: you will always get the same result that you would have gotten if it had been implemented with only real operations.\n\nIf you are curious about the mathematical details, or want to know how to define complex derivatives in PyTorch, read on.\n\nWhat are complex derivatives?\n\nThe mathematical definition of complex-differentiability takes the limit definition of a derivative and generalizes it to operate on complex numbers. Consider a function \n𝑓\n:\nC\n→\nC\nf:C→C,\n\n𝑓\n(\n𝑧\n=\n𝑥\n+\n𝑦\n𝑗\n)\n=\n𝑢\n(\n𝑥\n,\n𝑦\n)\n+\n𝑣\n(\n𝑥\n,\n𝑦\n)\n𝑗\nf(z=x+yj)=u(x,y)+v(x,y)j\n\nwhere \n𝑢\nu and \n𝑣\nv are two variable real valued functions and \n𝑗\nj is the imaginary unit.\n\nUsing the derivative definition, we can write:\n\n𝑓\n′\n(\n𝑧\n)\n=\nlim\n⁡\nℎ\n→\n0\n,\nℎ\n∈\n𝐶\n𝑓\n(\n𝑧\n+\nℎ\n)\n−\n𝑓\n(\n𝑧\n)\nℎ\nf\n′\n(z)=\nh→0,h∈C\nlim\n\t​\n\nh\nf(z+h)−f(z)\n\t​\n\n\nIn order for this limit to exist, not only must \n𝑢\nu and \n𝑣\nv must be real differentiable, but \n𝑓\nf must also satisfy the Cauchy-Riemann equations. In other words: the limit computed for real and imaginary steps (\nℎ\nh) must be equal. This is a more restrictive condition.\n\nThe complex differentiable functions are commonly known as holomorphic functions. They are well behaved, have all the nice properties that you’ve seen from real differentiable functions, but are practically of no use in the optimization world. For optimization problems, only real valued objective functions are used in the research community since complex numbers are not part of any ordered field and so having complex valued loss does not make much sense.\n\nIt also turns out that no interesting real-valued objective fulfill the Cauchy-Riemann equations. So the theory with homomorphic function cannot be used for optimization and most people therefore use the Wirtinger calculus.\n\nWirtinger Calculus comes into the picture …\n\nSo, we have this great theory of complex differentiability and holomorphic functions, and we can’t use any of it at all, because many of the commonly used functions are not holomorphic. What’s a poor mathematician to do? Well, Wirtinger observed that even if \n𝑓\n(\n𝑧\n)\nf(z) isn’t holomorphic, one could rewrite it as a two variable function \n𝑓\n(\n𝑧\n,\n𝑧\n∗\n)\nf(z,z∗) which is always holomorphic. This is because real and imaginary of the components of \n𝑧\nz can be expressed in terms of \n𝑧\nz and \n𝑧\n∗\nz\n∗\n as:\n\nR\ne\n(\n𝑧\n)\n\t\n=\n𝑧\n+\n𝑧\n∗\n2\n\n\nI\nm\n(\n𝑧\n)\n\t\n=\n𝑧\n−\n𝑧\n∗\n2\n𝑗\nRe(z)\nIm(z)\n\t​\n\n=\n2\nz+z\n∗\n\t​\n\n=\n2j\nz−z\n∗\n\t​\n\n\t​\n\n\nWirtinger calculus suggests to study \n𝑓\n(\n𝑧\n,\n𝑧\n∗\n)\nf(z,z\n∗\n) instead, which is guaranteed to be holomorphic if \n𝑓\nf was real differentiable (another way to think of it is as a change of coordinate system, from \n𝑓\n(\n𝑥\n,\n𝑦\n)\nf(x,y) to \n𝑓\n(\n𝑧\n,\n𝑧\n∗\n)\nf(z,z\n∗\n).) This function has partial derivatives \n∂\n∂\n𝑧\n∂z\n∂\n\t​\n\n and \n∂\n∂\n𝑧\n∗\n∂z\n∗\n∂\n\t​\n\n. We can use the chain rule to establish a relationship between these partial derivatives and the partial derivatives w.r.t., the real and imaginary components of \n𝑧\nz.\n\n∂\n∂\n𝑥\n\t\n=\n∂\n𝑧\n∂\n𝑥\n∗\n∂\n∂\n𝑧\n+\n∂\n𝑧\n∗\n∂\n𝑥\n∗\n∂\n∂\n𝑧\n∗\n\n\n\t\n=\n∂\n∂\n𝑧\n+\n∂\n∂\n𝑧\n∗\n\n\n\n\n∂\n∂\n𝑦\n\t\n=\n∂\n𝑧\n∂\n𝑦\n∗\n∂\n∂\n𝑧\n+\n∂\n𝑧\n∗\n∂\n𝑦\n∗\n∂\n∂\n𝑧\n∗\n\n\n\t\n=\n1\n𝑗\n∗\n(\n∂\n∂\n𝑧\n−\n∂\n∂\n𝑧\n∗\n)\n∂x\n∂\n\t​\n\n∂y\n∂\n\t​\n\n\t​\n\n=\n∂x\n∂z\n\t​\n\n∗\n∂z\n∂\n\t​\n\n+\n∂x\n∂z\n∗\n\t​\n\n∗\n∂z\n∗\n∂\n\t​\n\n=\n∂z\n∂\n\t​\n\n+\n∂z\n∗\n∂\n\t​\n\n=\n∂y\n∂z\n\t​\n\n∗\n∂z\n∂\n\t​\n\n+\n∂y\n∂z\n∗\n\t​\n\n∗\n∂z\n∗\n∂\n\t​\n\n=1j∗(\n∂z\n∂\n\t​\n\n−\n∂z\n∗\n∂\n\t​\n\n)\n\t​\n\n\nFrom the above equations, we get:\n\n∂\n∂\n𝑧\n\t\n=\n1\n/\n2\n∗\n(\n∂\n∂\n𝑥\n−\n1\n𝑗\n∗\n∂\n∂\n𝑦\n)\n\n\n∂\n∂\n𝑧\n∗\n\t\n=\n1\n/\n2\n∗\n(\n∂\n∂\n𝑥\n+\n1\n𝑗\n∗\n∂\n∂\n𝑦\n)\n∂z\n∂\n\t​\n\n∂z\n∗\n∂\n\t​\n\n\t​\n\n=1/2∗(\n∂x\n∂\n\t​\n\n−1j∗\n∂y\n∂\n\t​\n\n)\n=1/2∗(\n∂x\n∂\n\t​\n\n+1j∗\n∂y\n∂\n\t​\n\n)\n\t​\n\n\nwhich is the classic definition of Wirtinger calculus that you would find on Wikipedia.\n\nThere are a lot of beautiful consequences of this change.\n\nFor one, the Cauchy-Riemann equations translate into simply saying that \n∂\n𝑓\n∂\n𝑧\n∗\n=\n0\n∂z\n∗\n∂f\n\t​\n\n=0 (that is to say, the function \n𝑓\nf can be written entirely in terms of \n𝑧\nz, without making reference to \n𝑧\n∗\nz\n∗\n).\n\nAnother important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should take while making variable update is given by \n∂\n𝐿\n𝑜\n𝑠\n𝑠\n∂\n𝑧\n∗\n∂z\n∗\n∂Loss\n\t​\n\n (not \n∂\n𝐿\n𝑜\n𝑠\n𝑠\n∂\n𝑧\n∂z\n∂Loss\n\t​\n\n).\n\nFor more reading, check out: https://arxiv.org/pdf/0906.4835.pdf\n\nHow is Wirtinger Calculus useful in optimization?\n\nResearchers in audio and other fields, more commonly, use gradient descent to optimize real valued loss functions with complex variables. Typically, these people treat the real and imaginary values as separate channels that can be updated. For a step size \n𝛼\n/\n2\nα/2 and loss \n𝐿\nL, we can write the following equations in \nR\n2\nR\n2\n:\n\n𝑥\n𝑛\n+\n1\n\t\n=\n𝑥\n𝑛\n−\n(\n𝛼\n/\n2\n)\n∗\n∂\n𝐿\n∂\n𝑥\n\n\n𝑦\n𝑛\n+\n1\n\t\n=\n𝑦\n𝑛\n−\n(\n𝛼\n/\n2\n)\n∗\n∂\n𝐿\n∂\n𝑦\nx\nn+1\n\t​\n\ny\nn+1\n\t​\n\n\t​\n\n=x\nn\n\t​\n\n−(α/2)∗\n∂x\n∂L\n\t​\n\n=y\nn\n\t​\n\n−(α/2)∗\n∂y\n∂L\n\t​\n\n\t​\n\n\nHow do these equations translate into complex space \nC\nC?\n\n𝑧\n𝑛\n+\n1\n\t\n=\n𝑥\n𝑛\n−\n(\n𝛼\n/\n2\n)\n∗\n∂\n𝐿\n∂\n𝑥\n+\n1\n𝑗\n∗\n(\n𝑦\n𝑛\n−\n(\n𝛼\n/\n2\n)\n∗\n∂\n𝐿\n∂\n𝑦\n)\n\n\n\t\n=\n𝑧\n𝑛\n−\n𝛼\n∗\n1\n/\n2\n∗\n(\n∂\n𝐿\n∂\n𝑥\n+\n𝑗\n∂\n𝐿\n∂\n𝑦\n)\n\n\n\t\n=\n𝑧\n𝑛\n−\n𝛼\n∗\n∂\n𝐿\n∂\n𝑧\n∗\nz\nn+1\n\t​\n\n\t​\n\n=x\nn\n\t​\n\n−(α/2)∗\n∂x\n∂L\n\t​\n\n+1j∗(y\nn\n\t​\n\n−(α/2)∗\n∂y\n∂L\n\t​\n\n)\n=z\nn\n\t​\n\n−α∗1/2∗(\n∂x\n∂L\n\t​\n\n+j\n∂y\n∂L\n\t​\n\n)\n=z\nn\n\t​\n\n−α∗\n∂z\n∗\n∂L\n\t​\n\n\t​\n\n\nSomething very interesting has happened: Wirtinger calculus tells us that we can simplify the complex variable update formula above to only refer to the conjugate Wirtinger derivative \n∂\n𝐿\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n, giving us exactly the step we take in optimization.\n\nBecause the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative when you differentiate a function with a real valued loss.\n\nHow does PyTorch compute the conjugate Wirtinger derivative?\n\nTypically, our derivative formulas take in grad_output as an input, representing the incoming Vector-Jacobian product that we’ve already computed, aka, \n∂\n𝐿\n∂\n𝑠\n∗\n∂s\n∗\n∂L\n\t​\n\n, where \n𝐿\nL is the loss of the entire computation (producing a real loss) and \n𝑠\ns is the output of our function. The goal here is to compute \n∂\n𝐿\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n, where \n𝑧\nz is the input of the function. It turns out that in the case of real loss, we can get away with only calculating \n∂\n𝐿\n∂\n𝑠\n∗\n∂s\n∗\n∂L\n\t​\n\n, even though the chain rule implies that we also need to have access to \n∂\n𝐿\n∂\n𝑠\n∂s\n∂L\n\t​\n\n. If you want to skip this derivation, look at the last equation in this section and then skip to the next section.\n\nLet’s continue working with \n𝑓\n:\nC\n→\nC\nf:C→C defined as \n𝑓\n(\n𝑧\n)\n=\n𝑓\n(\n𝑥\n+\n𝑦\n𝑗\n)\n=\n𝑢\n(\n𝑥\n,\n𝑦\n)\n+\n𝑣\n(\n𝑥\n,\n𝑦\n)\n𝑗\nf(z)=f(x+yj)=u(x,y)+v(x,y)j. As discussed above, autograd’s gradient convention is centered around optimization for real valued loss functions, so let’s assume \n𝑓\nf is a part of larger real valued loss function \n𝑔\ng. Using chain rule, we can write:\n\n(1)\n∂\n𝐿\n∂\n𝑧\n∗\n=\n∂\n𝐿\n∂\n𝑢\n∗\n∂\n𝑢\n∂\n𝑧\n∗\n+\n∂\n𝐿\n∂\n𝑣\n∗\n∂\n𝑣\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n=\n∂u\n∂L\n\t​\n\n∗\n∂z\n∗\n∂u\n\t​\n\n+\n∂v\n∂L\n\t​\n\n∗\n∂z\n∗\n∂v\n\t​\n\n\nNow using Wirtinger derivative definition, we can write:\n\n∂\n𝐿\n∂\n𝑠\n=\n1\n/\n2\n∗\n(\n∂\n𝐿\n∂\n𝑢\n−\n∂\n𝐿\n∂\n𝑣\n𝑗\n)\n\n\n∂\n𝐿\n∂\n𝑠\n∗\n=\n1\n/\n2\n∗\n(\n∂\n𝐿\n∂\n𝑢\n+\n∂\n𝐿\n∂\n𝑣\n𝑗\n)\n∂s\n∂L\n\t​\n\n=1/2∗(\n∂u\n∂L\n\t​\n\n−\n∂v\n∂L\n\t​\n\nj)\n∂s\n∗\n∂L\n\t​\n\n=1/2∗(\n∂u\n∂L\n\t​\n\n+\n∂v\n∂L\n\t​\n\nj)\n\t​\n\n\nIt should be noted here that since \n𝑢\nu and \n𝑣\nv are real functions, and \n𝐿\nL is real by our assumption that \n𝑓\nf is a part of a real valued function, we have:\n\n(2)\n(\n∂\n𝐿\n∂\n𝑠\n)\n∗\n=\n∂\n𝐿\n∂\n𝑠\n∗\n(\n∂s\n∂L\n\t​\n\n)\n∗\n=\n∂s\n∗\n∂L\n\t​\n\n\ni.e., \n∂\n𝐿\n∂\n𝑠\n∂s\n∂L\n\t​\n\n equals to \n𝑔\n𝑟\n𝑎\n𝑑\n_\n𝑜\n𝑢\n𝑡\n𝑝\n𝑢\n𝑡\n∗\ngrad_output\n∗\n.\n\nSolving the above equations for \n∂\n𝐿\n∂\n𝑢\n∂u\n∂L\n\t​\n\n and \n∂\n𝐿\n∂\n𝑣\n∂v\n∂L\n\t​\n\n, we get:\n\n(3)\n∂\n𝐿\n∂\n𝑢\n=\n∂\n𝐿\n∂\n𝑠\n+\n∂\n𝐿\n∂\n𝑠\n∗\n\n\n∂\n𝐿\n∂\n𝑣\n=\n−\n1\n𝑗\n∗\n(\n∂\n𝐿\n∂\n𝑠\n−\n∂\n𝐿\n∂\n𝑠\n∗\n)\n∂u\n∂L\n\t​\n\n=\n∂s\n∂L\n\t​\n\n+\n∂s\n∗\n∂L\n\t​\n\n∂v\n∂L\n\t​\n\n=−1j∗(\n∂s\n∂L\n\t​\n\n−\n∂s\n∗\n∂L\n\t​\n\n)\n\t​\n\n\nSubstituting (3) in (1), we get:\n\n∂\n𝐿\n∂\n𝑧\n∗\n\t\n=\n(\n∂\n𝐿\n∂\n𝑠\n+\n∂\n𝐿\n∂\n𝑠\n∗\n)\n∗\n∂\n𝑢\n∂\n𝑧\n∗\n−\n1\n𝑗\n∗\n(\n∂\n𝐿\n∂\n𝑠\n−\n∂\n𝐿\n∂\n𝑠\n∗\n)\n∗\n∂\n𝑣\n∂\n𝑧\n∗\n\n\n\t\n=\n∂\n𝐿\n∂\n𝑠\n∗\n(\n∂\n𝑢\n∂\n𝑧\n∗\n+\n∂\n𝑣\n∂\n𝑧\n∗\n𝑗\n)\n+\n∂\n𝐿\n∂\n𝑠\n∗\n∗\n(\n∂\n𝑢\n∂\n𝑧\n∗\n−\n∂\n𝑣\n∂\n𝑧\n∗\n𝑗\n)\n\n\n\t\n=\n∂\n𝐿\n∂\n𝑠\n∗\n∗\n∂\n(\n𝑢\n+\n𝑣\n𝑗\n)\n∂\n𝑧\n∗\n+\n∂\n𝐿\n∂\n𝑠\n∗\n∂\n(\n𝑢\n+\n𝑣\n𝑗\n)\n∗\n∂\n𝑧\n∗\n\n\n\t\n=\n∂\n𝐿\n∂\n𝑠\n∗\n∂\n𝑠\n∂\n𝑧\n∗\n+\n∂\n𝐿\n∂\n𝑠\n∗\n∗\n∂\n𝑠\n∗\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n\t​\n\n=(\n∂s\n∂L\n\t​\n\n+\n∂s\n∗\n∂L\n\t​\n\n)∗\n∂z\n∗\n∂u\n\t​\n\n−1j∗(\n∂s\n∂L\n\t​\n\n−\n∂s\n∗\n∂L\n\t​\n\n)∗\n∂z\n∗\n∂v\n\t​\n\n=\n∂s\n∂L\n\t​\n\n∗(\n∂z\n∗\n∂u\n\t​\n\n+\n∂z\n∗\n∂v\n\t​\n\nj)+\n∂s\n∗\n∂L\n\t​\n\n∗(\n∂z\n∗\n∂u\n\t​\n\n−\n∂z\n∗\n∂v\n\t​\n\nj)\n=\n∂s\n∗\n∂L\n\t​\n\n∗\n∂z\n∗\n∂(u+vj)\n\t​\n\n+\n∂s\n∂L\n\t​\n\n∗\n∂z\n∗\n∂(u+vj)\n∗\n\t​\n\n=\n∂s\n∂L\n\t​\n\n∗\n∂z\n∗\n∂s\n\t​\n\n+\n∂s\n∗\n∂L\n\t​\n\n∗\n∂z\n∗\n∂s\n∗\n\t​\n\n\t​\n\n\nUsing (2), we get:\n\n(4)\n∂\n𝐿\n∂\n𝑧\n∗\n\t\n=\n(\n∂\n𝐿\n∂\n𝑠\n∗\n)\n∗\n∗\n∂\n𝑠\n∂\n𝑧\n∗\n+\n∂\n𝐿\n∂\n𝑠\n∗\n∗\n(\n∂\n𝑠\n∂\n𝑧\n)\n∗\n\n\n\t\n=\n(\n𝑔\n𝑟\n𝑎\n𝑑\n_\n𝑜\n𝑢\n𝑡\n𝑝\n𝑢\n𝑡\n)\n∗\n∗\n∂\n𝑠\n∂\n𝑧\n∗\n+\n𝑔\n𝑟\n𝑎\n𝑑\n_\n𝑜\n𝑢\n𝑡\n𝑝\n𝑢\n𝑡\n∗\n(\n∂\n𝑠\n∂\n𝑧\n)\n∗\n∂z\n∗\n∂L\n\t​\n\n\t​\n\n=(\n∂s\n∗\n∂L\n\t​\n\n)\n∗\n∗\n∂z\n∗\n∂s\n\t​\n\n+\n∂s\n∗\n∂L\n\t​\n\n∗(\n∂z\n∂s\n\t​\n\n)\n∗\n=\n(grad_output)\n∗\n∗\n∂z\n∗\n∂s\n\t​\n\n+grad_output∗(\n∂z\n∂s\n\t​\n\n)\n∗\n\t​\n\n\t​\n\n\nThis last equation is the important one for writing your own gradients, as it decomposes our derivative formula into a simpler one that is easy to compute by hand.\n\nHow can I write my own derivative formula for a complex function?\n\nThe above boxed equation gives us the general formula for all derivatives on complex functions. However, we still need to compute \n∂\n𝑠\n∂\n𝑧\n∂z\n∂s\n\t​\n\n and \n∂\n𝑠\n∂\n𝑧\n∗\n∂z\n∗\n∂s\n\t​\n\n. There are two ways you could do this:\n\nThe first way is to just use the definition of Wirtinger derivatives directly and calculate \n∂\n𝑠\n∂\n𝑧\n∂z\n∂s\n\t​\n\n and \n∂\n𝑠\n∂\n𝑧\n∗\n∂z\n∗\n∂s\n\t​\n\n by using \n∂\n𝑠\n∂\n𝑥\n∂x\n∂s\n\t​\n\n and \n∂\n𝑠\n∂\n𝑦\n∂y\n∂s\n\t​\n\n (which you can compute in the normal way).\n\nThe second way is to use the change of variables trick and rewrite \n𝑓\n(\n𝑧\n)\nf(z) as a two variable function \n𝑓\n(\n𝑧\n,\n𝑧\n∗\n)\nf(z,z\n∗\n), and compute the conjugate Wirtinger derivatives by treating \n𝑧\nz and \n𝑧\n∗\nz\n∗\n as independent variables. This is often easier; for example, if the function in question is holomorphic, only \n𝑧\nz will be used (and \n∂\n𝑠\n∂\n𝑧\n∗\n∂z\n∗\n∂s\n\t​\n\n will be zero).\n\nLet’s consider the function \n𝑓\n(\n𝑧\n=\n𝑥\n+\n𝑦\n𝑗\n)\n=\n𝑐\n∗\n𝑧\n=\n𝑐\n∗\n(\n𝑥\n+\n𝑦\n𝑗\n)\nf(z=x+yj)=c∗z=c∗(x+yj) as an example, where \n𝑐\n∈\nR\nc∈R.\n\nUsing the first way to compute the Wirtinger derivatives, we have.\n\n∂\n𝑠\n∂\n𝑧\n\t\n=\n1\n/\n2\n∗\n(\n∂\n𝑠\n∂\n𝑥\n−\n∂\n𝑠\n∂\n𝑦\n𝑗\n)\n\n\n\t\n=\n1\n/\n2\n∗\n(\n𝑐\n−\n(\n𝑐\n∗\n1\n𝑗\n)\n∗\n1\n𝑗\n)\n\n\n\t\n=\n𝑐\n\n\n\n\n\n\n∂\n𝑠\n∂\n𝑧\n∗\n\t\n=\n1\n/\n2\n∗\n(\n∂\n𝑠\n∂\n𝑥\n+\n∂\n𝑠\n∂\n𝑦\n𝑗\n)\n\n\n\t\n=\n1\n/\n2\n∗\n(\n𝑐\n+\n(\n𝑐\n∗\n1\n𝑗\n)\n∗\n1\n𝑗\n)\n\n\n\t\n=\n0\n∂z\n∂s\n\t​\n\n∂z\n∗\n∂s\n\t​\n\n\t​\n\n=1/2∗(\n∂x\n∂s\n\t​\n\n−\n∂y\n∂s\n\t​\n\nj)\n=1/2∗(c−(c∗1j)∗1j)\n=c\n=1/2∗(\n∂x\n∂s\n\t​\n\n+\n∂y\n∂s\n\t​\n\nj)\n=1/2∗(c+(c∗1j)∗1j)\n=0\n\t​\n\n\nUsing (4), and grad_output = 1.0 (which is the default grad output value used when backward() is called on a scalar output in PyTorch), we get:\n\n∂\n𝐿\n∂\n𝑧\n∗\n=\n1\n∗\n0\n+\n1\n∗\n𝑐\n=\n𝑐\n∂z\n∗\n∂L\n\t​\n\n=1∗0+1∗c=c\n\nUsing the second way to compute Wirtinger derivatives, we directly get:\n\n∂\n𝑠\n∂\n𝑧\n\t\n=\n∂\n(\n𝑐\n∗\n𝑧\n)\n∂\n𝑧\n\n\n\t\n=\n𝑐\n\n\n∂\n𝑠\n∂\n𝑧\n∗\n\t\n=\n∂\n(\n𝑐\n∗\n𝑧\n)\n∂\n𝑧\n∗\n\n\n\t\n=\n0\n∂z\n∂s\n\t​\n\n∂z\n∗\n∂s\n\t​\n\n\t​\n\n=\n∂z\n∂(c∗z)\n\t​\n\n=c\n=\n∂z\n∗\n∂(c∗z)\n\t​\n\n=0\n\t​\n\n\nAnd using (4) again, we get \n∂\n𝐿\n∂\n𝑧\n∗\n=\n𝑐\n∂z\n∗\n∂L\n\t​\n\n=c. As you can see, the second way involves lesser calculations, and comes in more handy for faster calculations.\n\nWhat about cross-domain functions?\n\nSome functions map from complex inputs to real outputs, or vice versa. These functions form a special case of (4), which we can derive using the chain rule:\n\nFor \n𝑓\n:\nC\n→\nR\nf:C→R, we get:\n\n∂\n𝐿\n∂\n𝑧\n∗\n=\n2\n∗\n𝑔\n𝑟\n𝑎\n𝑑\n_\n𝑜\n𝑢\n𝑡\n𝑝\n𝑢\n𝑡\n∗\n∂\n𝑠\n∂\n𝑧\n∗\n∂z\n∗\n∂L\n\t​\n\n=2∗grad_output∗\n∂z\n∗\n∂s\n\t​\n\n\nFor \n𝑓\n:\nR\n→\nC\nf:R→C, we get:\n\n∂\n𝐿\n∂\n𝑧\n∗\n=\n2\n∗\nR\ne\n(\n𝑔\n𝑟\n𝑎\n𝑑\n_\n𝑜\n𝑢\n𝑡\n𝑝\n𝑢\n𝑡\n∗\n∗\n∂\n𝑠\n∂\n𝑧\n∗\n)\n∂z\n∗\n∂L\n\t​\n\n=2∗Re(grad_output\n∗\n∗\n∂z\n∗\n∂s\n\t​\n\n)\nHooks for saved tensors\n\nYou can control how saved tensors are packed / unpacked by defining a pair of pack_hook / unpack_hook hooks. The pack_hook function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). The unpack_hook function takes as its single argument the output of pack_hook and should return a tensor to be used in the backward pass. The tensor returned by unpack_hook only needs to have the same content as the tensor passed as input to pack_hook. In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking.\n\nAn example of such pair is:\n\nclass SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name)\n\n\nNotice that the unpack_hook should not delete the temporary file because it might be called multiple times: the temporary file should be alive for as long as the returned SelfDeletingTempFile object is alive. In the above example, we prevent leaking the temporary file by closing it when it is no longer needed (on deletion of the SelfDeletingTempFile object).\n\nNOTE\n\nWe guarantee that pack_hook will only be called once but unpack_hook can be called as many times as the backward pass requires it and we expect it to return the same data each time.\n\nWARNING\n\nPerforming inplace operations on the input of any of the functions is forbidden as they may lead to unexpected side-effects. PyTorch will throw an error if the input to a pack hook is modified inplace but does not catch the case where the input to an unpack hook is modified inplace.\n\nRegistering hooks for a saved tensor\n\nYou can register a pair of hooks on a saved tensor by calling the register_hooks() method on a SavedTensor object. Those objects are exposed as attributes of a grad_fn and start with the _raw_saved_ prefix.\n\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\ny.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n\n\nThe pack_hook method is called as soon as the pair is registered. The unpack_hook method is called each time the saved tensor needs to be accessed, either by means of y.grad_fn._saved_self or during the backward pass.\n\nWARNING\n\nIf you maintain a reference to a SavedTensor after the saved tensors have been released (i.e. after backward has been called), calling its register_hooks() is forbidden. PyTorch will throw an error most of the time but it may fail to do so in some cases and undefined behavior may arise.\n\nRegistering default hooks for saved tensors\n\nAlternatively, you can use the context-manager saved_tensors_hooks to register a pair of hooks which will be applied to all saved tensors that are created in that context.\n\nExample:\n\n# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass Model(nn.Module):\n    def forward(self, x):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          # ... compute output\n          output = x\n        return output\n\nmodel = Model()\nnet = nn.DataParallel(model)\n\n\nThe hooks defined with this context manager are thread-local. Hence, the following code will not produce the desired effects because the hooks do not go through DataParallel.\n\n# Example what NOT to do\n\nnet = nn.DataParallel(model)\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    output = net(input)\n\n\nNote that using those hooks disables all the optimization in place to reduce Tensor object creation. For example:\n\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):\n    x = torch.randn(5, requires_grad=True)\n    y = x * x\n\n\nWithout the hooks, x, y.grad_fn._saved_self and y.grad_fn._saved_other all refer to the same tensor object. With the hooks, PyTorch will pack and unpack x into two new tensor objects that share the same storage with the original x (no copy performed).\n\nBackward Hooks execution\n\nThis section will discuss when different hooks fire or don’t fire. Then it will discuss the order in which they are fired. The hooks that will be covered are: backward hooks registered to Tensor via torch.Tensor.register_hook(), post-accumulate-grad hooks registered to Tensor via torch.Tensor.register_post_accumulate_grad_hook(), post-hooks registered to Node via torch.autograd.graph.Node.register_hook(), and pre-hooks registered to Node via torch.autograd.graph.Node.register_prehook().\n\nWhether a particular hook will be fired\n\nHooks registered to a Tensor via torch.Tensor.register_hook() are executed when gradients are being computed for that Tensor. (Note that this does not require the Tensor’s grad_fn to be executed. For example, if the Tensor is passed as part of the inputs argument to torch.autograd.grad(), the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)\n\nHooks registered to a Tensor via torch.Tensor.register_post_accumulate_grad_hook() are executed after the gradients have been accumulated for that Tensor, meaning the Tensor’s grad field has been set. Whereas hooks registered via torch.Tensor.register_hook() are run as gradients are being computed, hooks registered via torch.Tensor.register_post_accumulate_grad_hook() are only triggered once the Tensor’s grad field is updated by autograd at the end of the backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf Tensors. Registering a hook via torch.Tensor.register_post_accumulate_grad_hook() on a non-leaf Tensor will error, even if you call backward(retain_graph=True).\n\nHooks registered to torch.autograd.graph.Node using torch.autograd.graph.Node.register_hook() or torch.autograd.graph.Node.register_prehook() are only fired if the Node it was registered to is executed.\n\nWhether a particular Node is executed may depend on whether the backward pass was called with torch.autograd.grad() or torch.autograd.backward(). Specifically, you should be aware of these differences when you register a hook on a Node corresponding to a Tensor that you are passing to torch.autograd.grad() or torch.autograd.backward() as part of the inputs argument.\n\nIf you are using torch.autograd.backward(), all of the above mentioned hooks will be executed, whether or not you specified the inputs argument. This is because .backward() executes all Nodes, even if they correspond to a Tensor specified as an input. (Note that the execution of this additional Node corresponding to Tensors passed as inputs is usually unnecessary, but done anyway. This behavior is subject to change; you should not depend on it.)\n\nOn the other hand, if you are using torch.autograd.grad(), the backward hooks registered to Nodes that correspond to the Tensors passed to input may not be executed, because those Nodes will not be executed unless there is another input that depends on the gradient result of this Node.\n\nThe order in which the different hooks are fired\n\nThe order in which things happen are:\n\nhooks registered to Tensor are executed\n\npre-hooks registered to Node are executed (if Node is executed).\n\nthe .grad field is updated for Tensors that retain_grad\n\nNode is executed (subject to rules above)\n\nfor leaf Tensors that have .grad accumulated, post-accumulate-grad hooks are executed\n\npost-hooks registered to Node are executed (if Node is executed)\n\nIf multiple hooks of the same type are registered on the same Tensor or Node they are executed in the order in which they are registered. Hooks that are executed later can observe the modifications to the gradient made by earlier hooks.\n\nSpecial hooks\n\ntorch.autograd.graph.register_multi_grad_hook() is implemented using hooks registered to Tensors. Each individual Tensor hook is fired following the Tensor hook ordering defined above and the registered multi-grad hook is called when the last Tensor gradient is computed.\n\ntorch.nn.modules.module.register_module_full_backward_hook() is implemented using hooks registered to Node. As the forward is computed, hooks are registered to grad_fn corresponding to the inputs and outputs of the module. Because a module may take multiple inputs and return multiple outputs, a dummy custom autograd Function is first applied to the inputs of the module before forward and the outputs of the module before the output of forward is returned to ensure that those Tensors share a single grad_fn, which we can then attach our hooks to.\n\nBehavior of Tensor hooks when Tensor is modified in-place\n\nUsually hooks registered to a Tensor receive the gradient of the outputs with respect to that Tensor, where the value of the Tensor is taken to be its value at the time backward is computed.\n\nHowever, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks registered before in-place modification similarly receive gradients of the outputs with respect to the Tensor, but the value of the Tensor is taken to be its value before in-place modification.\n\nIf you prefer the behavior in the former case, you should register them to the Tensor after all in-place modifications to it have been made. For example:\n\nt = torch.tensor(1., requires_grad=True).sin()\nt.cos_()\nt.register_hook(fn)\nt.backward()\n\n\nFurthermore, it can be helpful to know that under the hood, when hooks are registered to a Tensor, they actually become permanently bound to the grad_fn of that Tensor, so if that Tensor is then modified in-place, even though the Tensor now has a new grad_fn, hooks registered before it was modified in-place will continue to be associated with the old grad_fn, e.g. they will fire when that Tensor’s old grad_fn is reached in the graph by the autograd engine.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nAutograd mechanics\nHow autograd encodes the history\nGradients for non-differentiable functions\nLocally disabling gradient computation\nIn-place operations with autograd\nMultithreaded Autograd\nAutograd for Complex Numbers\nHooks for saved tensors\nBackward Hooks execution\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Distributed Data Parallel — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/ddp.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Distributed Data Parallel\nShortcuts\nDISTRIBUTED DATA PARALLEL\n\nWARNING\n\nThe implementation of torch.nn.parallel.DistributedDataParallel evolves over time. This design note is written based on the state as of v1.4.\n\ntorch.nn.parallel.DistributedDataParallel (DDP) transparently performs distributed data parallel training. This page describes how it works and reveals implementation details.\n\nExample\n\nLet us start with a simple torch.nn.parallel.DistributedDataParallel example. This example uses a torch.nn.Linear as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\ndef example(rank, world_size):\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    # create local model\n    model = nn.Linear(10, 10).to(rank)\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 10).to(rank))\n    labels = torch.randn(20, 10).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n    # update parameters\n    optimizer.step()\n\ndef main():\n    world_size = 2\n    mp.spawn(example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    # Environment variables which need to be\n    # set when using c10d's default \"env\"\n    # initialization mode.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    main()\n\n\nDDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model wrapper before compiling the model, such that torchdynamo can apply DDPOptimizer (graph-break optimizations) based on DDP bucket sizes. (See TorchDynamo DDPOptimizer for more information.)\n\nTorchDynamo support for DDP currently requires setting static_graph=False, due to interactions between the graph tracing process and DDP’s mechanism for observing operations happening on its module, but this should be fixed ultimately.\n\nddp_model = DDP(model, device_ids=[rank])\nddp_model = torch.compile(ddp_model)\n\nInternal Design\n\nThis section reveals how it works under the hood of torch.nn.parallel.DistributedDataParallel by diving into details of every step in one iteration.\n\nPrerequisite: DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP.\n\nConstruction: The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. Then, each DDP process creates a local Reducer, which later will take care of the gradients synchronization during the backward pass. To improve communication efficiency, the Reducer organizes parameter gradients into buckets, and reduces one bucket at a time. Bucket size can be configured by setting the bucket_cap_mb argument in DDP constructor. The mapping from parameter gradients to buckets is determined at the construction time, based on the bucket size limit and parameter sizes. Model parameters are allocated into buckets in (roughly) the reverse order of Model.parameters() from the given model. The reason for using the reverse order is because DDP expects gradients to become ready during the backward pass in approximately that order. The figure below shows an example. Note that, the grad0 and grad1 are in bucket1, and the other two gradients are in bucket0. Of course, this assumption might not always be true, and when that happens it could hurt DDP backward speed as the Reducer cannot kick off the communication at the earliest possible time. Besides bucketing, the Reducer also registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.\n\nForward Pass: The DDP takes the input and passes it to the local model, and then analyzes the output from the local model if find_unused_parameters is set to True. This mode allows running backward on a subgraph of the model, and DDP finds out which parameters are involved in the backward pass by traversing the autograd graph from the model output and marking all unused parameters as ready for reduction. During the backward pass, the Reducer would only wait for unready parameters, but it would still reduce all buckets. Marking a parameter gradient as ready does not help DDP skip buckets as for now, but it will prevent DDP from waiting for absent gradients forever during the backward pass. Note that traversing the autograd graph introduces extra overheads, so applications should only set find_unused_parameters to True when necessary.\n\nBackward Pass: The backward() function is directly invoked on the loss Tensor, which is out of DDP’s control, and DDP uses autograd hooks registered at construction time to trigger gradients synchronizations. When one gradient becomes ready, its corresponding DDP hook on that grad accumulator will fire, and DDP will then mark that parameter gradient as ready for reduction. When gradients in one bucket are all ready, the Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes. When all buckets are ready, the Reducer will block waiting for all allreduce operations to finish. When this is done, averaged gradients are written to the param.grad field of all parameters. So after the backward pass, the grad field on the same corresponding parameter across different DDP processes should be the same.\n\nOptimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.\n\nNOTE\n\nDDP requires Reducer instances on all processes to invoke allreduce in exactly the same order, which is done by always running allreduce in the bucket index order instead of actual bucket ready order. Mismatched allreduce order across processes can lead to wrong results or DDP backward hang.\n\nImplementation\n\nBelow are pointers to the DDP implementation components. The stacked graph shows the structure of the code.\n\nProcessGroup\n\nProcessGroup.hpp: contains the abstract API of all process group implementations. The c10d library provides 3 implementations out of the box, namely, ProcessGroupGloo, ProcessGroupNCCL, and ProcessGroupMPI. DistributedDataParallel uses ProcessGroup::broadcast() to send model states from the process with rank 0 to others during initialization and ProcessGroup::allreduce() to sum gradients.\n\nStore.hpp: assists the rendezvous service for process group instances to find each other.\n\nDistributedDataParallel\n\ndistributed.py: is the Python entry point for DDP. It implements the initialization steps and the forward function for the nn.parallel.DistributedDataParallel module which call into C++ libraries. Its _sync_param function performs intra-process parameter synchronization when one DDP process works on multiple devices, and it also broadcasts model buffers from the process with rank 0 to all other processes. The inter-process parameter synchronization happens in Reducer.cpp.\n\ncomm.h: implements the coalesced broadcast helper function which is invoked to broadcast model states during initialization and synchronize model buffers before the forward pass.\n\nreducer.h: provides the core implementation for gradient synchronization in the backward pass. It has three entry point functions:\n\nReducer: The constructor is called in distributed.py which registers Reducer::autograd_hook() to gradient accumulators.\n\nautograd_hook() function will be invoked by the autograd engine when a gradient becomes ready.\n\nprepare_for_backward() is called at the end of DDP forward pass in distributed.py. It traverses the autograd graph to find unused parameters when find_unused_parameters is set to True in DDP constructor.\n\nTorchDynamo DDPOptimizer\n\nDDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards. AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph, because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\n\nTorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break the graph during backwards, and the simplest implementation is to break the forward graphs and then call AotAutograd and compilation on each section. This allows DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications to overlap with compute.\n\nSee this blog post for a more in-depth explanation and experimental results, or read the docs and code at torch/_dynamo/optimizations/distributed.py\n\nTo Debug DDPOptimizer, set torch._dynamo.config.log_level to DEBUG (for full graph dumps) or INFO (for basic info about bucket boundaries). To disable DDPOptimizer, set torch._dynamo.config.optimize_ddp=False. DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nDistributed Data Parallel\nExample\nInternal Design\nImplementation\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "CPU threading and TorchScript inference — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > CPU threading and TorchScript inference\nShortcuts\nCPU THREADING AND TORCHSCRIPT INFERENCE\n\nPyTorch allows using multiple CPU threads during TorchScript model inference. The following figure shows different levels of parallelism one would find in a typical application:\n\nOne or more inference threads execute a model’s forward pass on the given inputs. Each inference thread invokes a JIT interpreter that executes the ops of a model inline, one by one. A model can utilize a fork TorchScript primitive to launch an asynchronous task. Forking several operations at once results in a task that is executed in parallel. The fork operator returns a Future object which can be used to synchronize on later, for example:\n\n@torch.jit.script\ndef compute_z(x):\n    return torch.mm(x, self.w_z)\n\n@torch.jit.script\ndef forward(x):\n    # launch compute_z asynchronously:\n    fut = torch.jit._fork(compute_z, x)\n    # execute the next operation in parallel to compute_z:\n    y = torch.mm(x, self.w_y)\n    # wait for the result of compute_z:\n    z = torch.jit._wait(fut)\n    return y + z\n\n\nPyTorch uses a single thread pool for the inter-op parallelism, this thread pool is shared by all inference tasks that are forked within the application process.\n\nIn addition to the inter-op parallelism, PyTorch can also utilize multiple threads within the ops (intra-op parallelism). This can be useful in many cases, including element-wise ops on large tensors, convolutions, GEMMs, embedding lookups and others.\n\nBuild options\n\nPyTorch uses an internal ATen library to implement ops. In addition to that, PyTorch can also be built with support of external libraries, such as MKL and MKL-DNN, to speed up computations on CPU.\n\nATen, MKL and MKL-DNN support intra-op parallelism and depend on the following parallelization libraries to implement it:\n\nOpenMP - a standard (and a library, usually shipped with a compiler), widely used in external libraries;\n\nTBB - a newer parallelization library optimized for task-based parallelism and concurrent environments.\n\nOpenMP historically has been used by a large number of libraries. It is known for a relative ease of use and support for loop-based parallelism and other primitives.\n\nTBB is used to a lesser extent in external libraries, but, at the same time, is optimized for the concurrent environments. PyTorch’s TBB backend guarantees that there’s a separate, single, per-process intra-op thread pool used by all of the ops running in the application.\n\nDepending of the use case, one might find one or another parallelization library a better choice in their application.\n\nPyTorch allows selecting of the parallelization backend used by ATen and other libraries at the build time with the following build options:\n\nLibrary\n\n\t\n\nBuild Option\n\n\t\n\nValues\n\n\t\n\nNotes\n\n\n\n\nATen\n\n\t\n\nATEN_THREADING\n\n\t\n\nOMP (default), TBB\n\n\t\n\n\nMKL\n\n\t\n\nMKL_THREADING\n\n\t\n\n(same)\n\n\t\n\nTo enable MKL use BLAS=MKL\n\n\n\n\nMKL-DNN\n\n\t\n\nMKLDNN_CPU_RUNTIME\n\n\t\n\n(same)\n\n\t\n\nTo enable MKL-DNN use USE_MKLDNN=1\n\nIt is recommended not to mix OpenMP and TBB within one build.\n\nAny of the TBB values above require USE_TBB=1 build setting (default: OFF). A separate setting USE_OPENMP=1 (default: ON) is required for OpenMP parallelism.\n\nRuntime API\n\nThe following API is used to control thread settings:\n\nType of parallelism\n\n\t\n\nSettings\n\n\t\n\nNotes\n\n\n\n\nInter-op parallelism\n\n\t\n\nat::set_num_interop_threads, at::get_num_interop_threads (C++)\n\nset_num_interop_threads, get_num_interop_threads (Python, torch module)\n\n\t\n\nDefault number of threads: number of CPU cores.\n\n\n\n\nIntra-op parallelism\n\n\t\n\nat::set_num_threads, at::get_num_threads (C++) set_num_threads, get_num_threads (Python, torch module)\n\nEnvironment variables: OMP_NUM_THREADS and MKL_NUM_THREADS\n\nFor the intra-op parallelism settings, at::set_num_threads, torch.set_num_threads always take precedence over environment variables, MKL_NUM_THREADS variable takes precedence over OMP_NUM_THREADS.\n\nTuning the number of threads\n\nThe following simple script shows how a runtime of matrix multiplication changes with the number of threads:\n\nimport timeit\nruntimes = []\nthreads = [1] + [t for t in range(2, 49, 2)]\nfor t in threads:\n    torch.set_num_threads(t)\n    r = timeit.timeit(setup = \"import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)\", stmt=\"torch.mm(x, y)\", number=100)\n    runtimes.append(r)\n# ... plotting (threads, runtimes) ...\n\n\nRunning the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes:\n\nThe following considerations should be taken into account when tuning the number of intra- and inter-op threads:\n\nWhen choosing the number of threads one needs to avoid oversubscription (using too many threads, leads to performance degradation). For example, in an application that uses a large application thread pool or heavily relies on inter-op parallelism, one might find disabling intra-op parallelism as a possible option (i.e. by calling set_num_threads(1));\n\nIn a typical application one might encounter a trade off between latency (time spent on processing an inference request) and throughput (amount of work done per unit of time). Tuning the number of threads can be a useful tool to adjust this trade off in one way or another. For example, in latency critical applications one might want to increase the number of intra-op threads to process each request as fast as possible. At the same time, parallel implementations of ops may add an extra overhead that increases amount work done per single request and thus reduces the overall throughput.\n\nWARNING\n\nOpenMP does not guarantee that a single per-process intra-op thread pool is going to be used in the application. On the contrary, two different application or inter-op threads may use different OpenMP thread pools for intra-op work. This might result in a large number of threads used by the application. Extra care in tuning the number of threads is needed to avoid oversubscription in multi-threaded applications in OpenMP case.\n\nNOTE\n\nPre-built PyTorch releases are compiled with OpenMP support.\n\nNOTE\n\nparallel_info utility prints information about thread settings and can be used for debugging. Similar output can be also obtained in Python with torch.__config__.parallel_info() call.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nCPU threading and TorchScript inference\nBuild options\nRuntime API\nTuning the number of threads\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "Broadcasting semantics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/broadcasting.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > Broadcasting semantics\nShortcuts\nBROADCASTING SEMANTICS\n\nMany PyTorch operations support NumPy’s broadcasting semantics. See https://numpy.org/doc/stable/user/basics.broadcasting.html for details.\n\nIn short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\n\nGeneral semantics\n\nTwo tensors are “broadcastable” if the following rules hold:\n\nEach tensor has at least one dimension.\n\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\nFor Example:\n\n>>> x=torch.empty(5,7,3)\n>>> y=torch.empty(5,7,3)\n# same shapes are always broadcastable (i.e. the above rules always hold)\n\n>>> x=torch.empty((0,))\n>>> y=torch.empty(2,2)\n# x and y are not broadcastable, because x does not have at least 1 dimension\n\n# can line up trailing dimensions\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are broadcastable.\n# 1st trailing dimension: both have size 1\n# 2nd trailing dimension: y has size 1\n# 3rd trailing dimension: x size == y size\n# 4th trailing dimension: y dimension doesn't exist\n\n# but:\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\n\nIf two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n\nIf the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n\nThen, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n\nFor Example:\n\n# can line up trailing dimensions to make reading easier\n>>> x=torch.empty(5,1,4,1)\n>>> y=torch.empty(  3,1,1)\n>>> (x+y).size()\ntorch.Size([5, 3, 4, 1])\n\n# but not necessary:\n>>> x=torch.empty(1)\n>>> y=torch.empty(3,1,7)\n>>> (x+y).size()\ntorch.Size([3, 1, 7])\n\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x+y).size()\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\nIn-place semantics\n\nOne complication is that in-place operations do not allow the in-place tensor to change shape as a result of the broadcast.\n\nFor Example:\n\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x.add_(y)).size()\ntorch.Size([5, 3, 4, 1])\n\n# but:\n>>> x=torch.empty(1,3,1)\n>>> y=torch.empty(3,1,7)\n>>> (x.add_(y)).size()\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n\nBackwards compatibility\n\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal. The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements.\n\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape, but are broadcastable and have the same number of elements. For Example:\n\n>>> torch.add(torch.ones(4,1), torch.randn(4))\n\n\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]). In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set torch.utils.backcompat.broadcast_warning.enabled to True, which will generate a python warning in such cases.\n\nFor Example:\n\n>>> torch.utils.backcompat.broadcast_warning.enabled=True\n>>> torch.add(torch.ones(4,1), torch.ones(4))\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nBroadcasting semantics\nGeneral semantics\nIn-place semantics\nBackwards compatibility\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "CUDA Automatic Mixed Precision examples — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/notes/amp_examples.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > CUDA Automatic Mixed Precision examples\nShortcuts\nCUDA AUTOMATIC MIXED PRECISION EXAMPLES\n\nOrdinarily, “automatic mixed precision training” means training with torch.autocast and torch.cuda.amp.GradScaler together.\n\nInstances of torch.autocast enable autocasting for chosen regions. Autocasting automatically chooses the precision for GPU operations to improve performance while maintaining accuracy.\n\nInstances of torch.cuda.amp.GradScaler help perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with float16 gradients by minimizing gradient underflow, as explained here.\n\ntorch.autocast and torch.cuda.amp.GradScaler are modular. In the samples below, each is used as its individual documentation suggests.\n\n(Samples here are illustrative. See the Automatic Mixed Precision recipe for a runnable walkthrough.)\n\nTypical Mixed Precision Training\n\nWorking with Unscaled Gradients\n\nGradient clipping\n\nWorking with Scaled Gradients\n\nGradient accumulation\n\nGradient penalty\n\nWorking with Multiple Models, Losses, and Optimizers\n\nWorking with Multiple GPUs\n\nDataParallel in a single process\n\nDistributedDataParallel, one GPU per process\n\nDistributedDataParallel, multiple GPUs per process\n\nAutocast and Custom Autograd Functions\n\nFunctions with multiple inputs or autocastable ops\n\nFunctions that need a particular dtype\n\nTypical Mixed Precision Training\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\n# Creates a GradScaler once at the beginning of training.\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\nWorking with Unscaled Gradients\n\nAll gradients produced by scaler.scale(loss).backward() are scaled. If you wish to modify or inspect the parameters’ .grad attributes between backward() and scaler.step(optimizer), you should unscale them first. For example, gradient clipping manipulates a set of gradients such that their global norm (see torch.nn.utils.clip_grad_norm_()) or maximum magnitude (see torch.nn.utils.clip_grad_value_()) is \n<\n=\n<= some user-imposed threshold. If you attempted to clip without unscaling, the gradients’ norm/maximum magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for unscaled gradients) would be invalid.\n\nscaler.unscale_(optimizer) unscales gradients held by optimizer’s assigned parameters. If your model or models contain other parameters that were assigned to another optimizer (say optimizer2), you may call scaler.unscale_(optimizer2) separately to unscale those parameters’ gradients as well.\n\nGradient clipping\n\nCalling scaler.unscale_(optimizer) before clipping enables you to clip unscaled gradients as usual:\n\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(optimizer)\n\n        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n\nscaler records that scaler.unscale_(optimizer) was already called for this optimizer this iteration, so scaler.step(optimizer) knows not to redundantly unscale gradients before (internally) calling optimizer.step().\n\nWARNING\n\nunscale_ should only be called once per optimizer per step call, and only after all gradients for that optimizer’s assigned parameters have been accumulated. Calling unscale_ twice for a given optimizer between each step triggers a RuntimeError.\n\nWorking with Scaled Gradients\nGradient accumulation\n\nGradient accumulation adds gradients over an effective batch of size batch_per_iter * iters_to_accumulate (* num_procs if distributed). The scale should be calibrated for the effective batch, which means inf/NaN checking, step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity. Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective batch are accumulated. If grads are unscaled (or the scale factor changes) before accumulation is complete, the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor) after which it’s impossible to recover the accumulated unscaled grads step must apply.\n\nTherefore, if you want to unscale_ grads (e.g., to allow clipping unscaled grads), call unscale_ just before step, after all (scaled) grads for the upcoming step have been accumulated. Also, only call update at the end of iterations where you called step for a full effective batch:\n\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for i, (input, target) in enumerate(data):\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss = loss / iters_to_accumulate\n\n        # Accumulates scaled gradients.\n        scaler.scale(loss).backward()\n\n        if (i + 1) % iters_to_accumulate == 0:\n            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\nGradient penalty\n\nA gradient penalty implementation commonly creates gradients using torch.autograd.grad(), combines them to create the penalty value, and adds the penalty value to the loss.\n\nHere’s an ordinary example of an L2 penalty without gradient scaling or autocasting:\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n\n        # Creates gradients\n        grad_params = torch.autograd.grad(outputs=loss,\n                                          inputs=model.parameters(),\n                                          create_graph=True)\n\n        # Computes the penalty term and adds it to the loss\n        grad_norm = 0\n        for grad in grad_params:\n            grad_norm += grad.pow(2).sum()\n        grad_norm = grad_norm.sqrt()\n        loss = loss + grad_norm\n\n        loss.backward()\n\n        # clip gradients here, if desired\n\n        optimizer.step()\n\n\nTo implement a gradient penalty with gradient scaling, the outputs Tensor(s) passed to torch.autograd.grad() should be scaled. The resulting gradients will therefore be scaled, and should be unscaled before being combined to create the penalty value.\n\nAlso, the penalty term computation is part of the forward pass, and therefore should be inside an autocast context.\n\nHere’s how that looks for the same L2 penalty:\n\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\n        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\n                                                 inputs=model.parameters(),\n                                                 create_graph=True)\n\n        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\n        inv_scale = 1./scaler.get_scale()\n        grad_params = [p * inv_scale for p in scaled_grad_params]\n\n        # Computes the penalty term and adds it to the loss\n        with autocast(device_type='cuda', dtype=torch.float16):\n            grad_norm = 0\n            for grad in grad_params:\n                grad_norm += grad.pow(2).sum()\n            grad_norm = grad_norm.sqrt()\n            loss = loss + grad_norm\n\n        # Applies scaling to the backward call as usual.\n        # Accumulates leaf gradients that are correctly scaled.\n        scaler.scale(loss).backward()\n\n        # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n        # step() and update() proceed as usual.\n        scaler.step(optimizer)\n        scaler.update()\n\nWorking with Multiple Models, Losses, and Optimizers\n\nIf your network has multiple losses, you must call scaler.scale on each of them individually. If your network has multiple optimizers, you may call scaler.unscale_ on any of them individually, and you must call scaler.step on each of them individually.\n\nHowever, scaler.update should only be called once, after all optimizers used this iteration have been stepped:\n\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer0.zero_grad()\n        optimizer1.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output0 = model0(input)\n            output1 = model1(input)\n            loss0 = loss_fn(2 * output0 + 3 * output1, target)\n            loss1 = loss_fn(3 * output0 - 5 * output1, target)\n\n        # (retain_graph here is unrelated to amp, it's present because in this\n        # example, both backward() calls share some sections of graph.)\n        scaler.scale(loss0).backward(retain_graph=True)\n        scaler.scale(loss1).backward()\n\n        # You can choose which optimizers receive explicit unscaling, if you\n        # want to inspect or modify the gradients of the params they own.\n        scaler.unscale_(optimizer0)\n\n        scaler.step(optimizer0)\n        scaler.step(optimizer1)\n\n        scaler.update()\n\n\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision whether or not to skip the step. This may result in one optimizer skipping the step while the other one does not. Since step skipping occurs rarely (every several hundred iterations) this should not impede convergence. If you observe poor convergence after adding gradient scaling to a multiple-optimizer model, please report a bug.\n\nWorking with Multiple GPUs\n\nThe issues described here only affect autocast. GradScaler‘s usage is unchanged.\n\nDataParallel in a single process\n\nEven if torch.nn.DataParallel spawns threads to run the forward pass on each device. The autocast state is propagated in each one and the following will work:\n\nmodel = MyModel()\ndp_model = nn.DataParallel(model)\n\n# Sets autocast in the main thread\nwith autocast(device_type='cuda', dtype=torch.float16):\n    # dp_model's internal threads will autocast.\n    output = dp_model(input)\n    # loss_fn also autocast\n    loss = loss_fn(output)\n\nDistributedDataParallel, one GPU per process\n\ntorch.nn.parallel.DistributedDataParallel’s documentation recommends one GPU per process for best performance. In this case, DistributedDataParallel does not spawn threads internally, so usages of autocast and GradScaler are not affected.\n\nDistributedDataParallel, multiple GPUs per process\n\nHere torch.nn.parallel.DistributedDataParallel may spawn a side thread to run the forward pass on each device, like torch.nn.DataParallel. The fix is the same: apply autocast as part of your model’s forward method to ensure it’s enabled in side threads.\n\nAutocast and Custom Autograd Functions\n\nIf your network uses custom autograd functions (subclasses of torch.autograd.Function), changes are required for autocast compatibility if any function\n\ntakes multiple floating-point Tensor inputs,\n\nwraps any autocastable op (see the Autocast Op Reference), or\n\nrequires a particular dtype (for example, if it wraps CUDA extensions that were only compiled for dtype).\n\nIn all cases, if you’re importing the function and can’t alter its definition, a safe fallback is to disable autocast and force execution in float32 ( or dtype) at any points of use where errors occur:\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    ...\n    with autocast(device_type='cuda', dtype=torch.float16, enabled=False):\n        output = imported_function(input1.float(), input2.float())\n\n\nIf you’re the function’s author (or can alter its definition) a better solution is to use the torch.cuda.amp.custom_fwd() and torch.cuda.amp.custom_bwd() decorators as shown in the relevant case below.\n\nFunctions with multiple inputs or autocastable ops\n\nApply custom_fwd and custom_bwd (with no arguments) to forward and backward respectively. These ensure forward executes with the current autocast state and backward executes with the same autocast state as forward (which can prevent type mismatch errors):\n\nclass MyMM(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(ctx, a, b):\n        ctx.save_for_backward(a, b)\n        return a.mm(b)\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad):\n        a, b = ctx.saved_tensors\n        return grad.mm(b.t()), a.t().mm(grad)\n\n\nNow MyMM can be invoked anywhere, without disabling autocast or manually casting inputs:\n\nmymm = MyMM.apply\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = mymm(input1, input2)\n\nFunctions that need a particular dtype\n\nConsider a custom function that requires torch.float32 inputs. Apply custom_fwd(cast_inputs=torch.float32) to forward and custom_bwd (with no arguments) to backward. If forward runs in an autocast-enabled region, the decorators cast floating-point CUDA Tensor inputs to float32, and locally disable autocast during forward and backward:\n\nclass MyFloat32Func(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        ...\n        return fwd_output\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad):\n        ...\n\n\nNow MyFloat32Func can be invoked anywhere, without manually disabling autocast or casting inputs:\n\nfunc = MyFloat32Func.apply\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    # func will run in float32, regardless of the surrounding autocast state\n    output = func(input)\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nCUDA Automatic Mixed Precision examples\nTypical Mixed Precision Training\nWorking with Unscaled Gradients\nWorking with Scaled Gradients\nWorking with Multiple Models, Losses, and Optimizers\nWorking with Multiple GPUs\nAutocast and Custom Autograd Functions\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch Governance | Maintainers — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/community/persons_of_interest.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch Governance | Maintainers\nShortcuts\nPYTORCH GOVERNANCE | MAINTAINERS\nResponsibilities\n\nTriage and fix high priority issues assigned to the module or library\n\nTriage, review, and land high priority pull requests assigned to the module or library\n\nAnswer module or library questions on discuss.pytorch.org and dev-discuss.pytorch.org\n\nMaintain public user and development documentation\n\nRun meetings and share minutes plus roadmap on a half or quarterly basis\n\nLead Core Maintainer (BDFL)\n\nSoumith Chintala (soumith)\n\nCore Maintainers\n\nSoumith Chintala (soumith)\n\nEdward Yang (ezyang)\n\nGreg Chanan (gchanan)\n\nDmytro Dzhulgakov (dzhulgakov)\n\nNikita Shulga (malfet)\n\nModule-level maintainers\nNN APIs (torch.nn)\n\nGreg Chanan (gchanan)\n\nSoumith Chintala (soumith)\n\nJoel Schlosser (jbschlosser)\n\nAlban Desmaison (albanD)\n\n(emeritus) Sam Gross (colesbury)\n\n(emeritus) Adam Paszke (apaszke)\n\nOptimizers (torch.optim)\n\nAlban Desmaison (albanD)\n\nJoel Schlosser (jbschlosser)\n\nSoumith Chintala (soumith)\n\n(emeritus) Ilqar Ramazanli (iramazanli)\n\n(emeritus) Vincent Quenneville-Belair (vincentqb)\n\nAutograd (torch.autograd)\n\nEdward Yang (ezyang)\n\nAlban Desmaison (alband)\n\nJeffrey Wan (soulitzer)\n\n(emeritus) Adam Paszke (apaszke)\n\nCompilers (JIT / TorchScript / FX / TorchDynamo)\n\nElias Ellison (eellison)\n\nMichael Suo (suo)\n\nYanan Cao (gmagogsfm)\n\nJames Reed (jamesr66a)\n\nJason Ansel (jansel)\n\nJiong Gong (jgong5)\n\n(emeritus) Zach Devito (zdevito)\n\nDistributions & RNG\n\nFritz Obermeyer (fritzo)\n\nNeeraj Pradhan (neerajprad)\n\nAlican Bozkurt (alicanb)\n\n(emeritus) Vishwak Srinivasan (vishwakftw)\n\nDistributed\n\nShen Li (mrshenli)\n\nPritam Damania (pritamdamania87)\n\nYanli Zhao (zhaojuanmao)\n\nRohan Varma (rohan-varma)\n\nWanchao Liang (wanchaol)\n\nJunjie Wang (fduwjj)\n\nHoward Huang (H-Huang)\n\nTristan Rice (d4l3k)\n\nAlisson Azzolini (aazzolini)\n\nKe Wen (kwen2501)\n\nJames Reed (jamesr66a)\n\nKiuk Chung (kiukchung)\n\n(emeritus) Pieter Noordhuis (pietern)\n\n(emeritus) Mingzhe Li (mingzhe09088)\n\n(emeritus) Omkar Salpekar (osalpekar)\n\nMultiprocessing and DataLoaders\n\nSimon Wang (SsnL)\n\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n\n(emeritus) Adam Paszke (apaszke)\n\nLinear Algebra (torch.linalg)\n\nMike Ruberry (mruberry)\n\nMario Lezcano (lezcano)\n\nIvan Yashchuk (IvanYashchuk)\n\n(emeritus) Vishwak Srinivasan (vishwakftw)\n\nSparse (torch.sparse)\n\nPearu Peterson (pearu)\n\nNikita Vedeneev (nikitaved)\n\nIvan Yashchuk (IvanYashchuk)\n\nChristian Puhrsch (cpuhrsch)\n\nAndrew James (amjames)\n\nNestedTensor (torch.nested)\n\nAlban Desmaison (albanD)\n\nChristian Puhrsch (cpuhrsch)\n\nDriss Guessous (drisspg)\n\nJoel Schlosser (jbschlosser)\n\nMikayla Gawarecki (mikaylagawarecki)\n\nNatalia Gimelshein (ngimel)\n\nMaskedTensor (torch.masked)\n\nChristian Puhrsch (cpuhrsch)\n\n(emeritus) George Qi (george-qi)\n\nFast Fourier Transform (torch.fft)\n\nMike Ruberry (mruberry)\n\nPeter Bell (peterbell10)\n\nCPU Performance (Torch Inductor / MKLDNN)\n\nMingfei Ma (mingfeima)\n\nJiong Gong (jgong5)\n\nXiaobing Zhang (XiaobingSuper)\n\n(emeritus) Xiaoqiang Zheng (zheng-xq)\n\n(emeritus) Sam Gross (colesbury)\n\n(emeritus) Christian Puhrsch (cpuhrsch)\n\n(emeritus) Ilia Cherniavskii (ilia-cher)\n\n(emeritus) Junjie Bai (bddppq)\n\n(emeritus) Yinghai Lu (yinghai)\n\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n\n(emeritus) Jianhui Li (Jianhui-Li)\n\nGPU Performance (Torch Inductor / Triton / CUDA)\n\nNatalia Gimelshein (ngimel)\n\nEdward Yang (ezyang)\n\nPiotr Bialecki (ptrblck)\n\nChristian Sarofeen (csarofeen)\n\nAndrew Tulloch (ajtulloch)\n\n(emeritus) Xiaoqiang Zheng (zheng-xq)\n\nNVFuser\n\nChristian Sarofeen (csarofeen)\n\nAlex Jann (jjsjann123)\n\nPiotr Bialecki (ptrblck)\n\nNatalia Gimelshein (ngimel)\n\nAMD/ROCm/HIP\n\nPeng Sun (sunway513)\n\nJithun Nair (jithunnair-amd)\n\nJeff Daily (jeffdaily)\n\n(emeritus) Junjie Bai (bddppq)\n\nBuild + CI\n\nNikita Shulga (malfet)\n\nEli Uriegas (seemethere)\n\nAlban Desmaison (alband)\n\nMikey Dagitses (dagitses)\n\nOmkar Salpekar (osalpekar)\n\nZain Rizvi (ZainRizvi)\n\nNirav Mehta (mehtanirav)\n\nAndrey Talman (atalman)\n\n(emeritus) Zhuojie Zhou (zhouzhuojie)\n\n(emeritus) Edward Yang (ezyang)\n\n(emeritus) Karl Ostmo (kostmo)\n\nPerformance Tools\n\nAdnan Aziz (adnanaziz)\n\nCK Luk (ckluk)\n\nTaylor Robie (robieta)\n\nXu Zhao (xuzhao9)\n\nGeeta Chauhan (chauhang)\n\n(emeritus) Victor Bittorf (bitfort)\n\n(emeritus) Gisle Dankel (gdankel)\n\n(emeritus) Natalia Gimelshein (ngimel)\n\n(emeritus) Mingzhe Li (mingzhe09088)\n\nC++ API\n\nJoel Schlosser (jbschlosser)\n\n(emeritus) Will Feng (yf225)\n\nC10 utils and operator dispatch\n\nBrian Hirsh (bdhirsh)\n\nEdward Yang (ezyang)\n\nDmytro Dzhulgakov (dzhulgakov)\n\n(emeritus) Sebastian Messmer (smessmer)\n\nONNX exporter\n\nBowen Bao (BowenBao)\n\nAaron Bockover (abock)\n\n(emeritus) Gary Miguel (garymm)\n\n(emeritus) Lara Haidar (lara-hdr)\n\n(emeritus) Lu Fang (houseroad)\n\n(emeritus) Negin Raoof (neginraoof)\n\n(emeritus) Spandan Tiwari (spandantiwari)\n\nMobile / Edge\n\nDavid Reiss (dreiss)\n\nRaziel Guevara (raziel)\n\nLinbin Yu (linbinyu)\n\nIvan Kobzarev (IvanKobzarev)\n\nTao Xu (xta0)\n\nModel Compression & Optimization\n\nVasiliy Kuznetsov (vkuzo)\n\nJerry Zhang (jerryzh168)\n\nSupriya Rao (supriyar)\n\n(emeritus) Zafar Takhirov (z-a-f)\n\n(emeritus) Raghuraman Krishnamoorthi (raghuramank100)\n\nWindows\n\nGuoliang Hua (nbcsm)\n\n(emeritus) Teng Gao (gaoteng-git)\n\n(emeritus) Peter Johnson (peterjc123)\n\nApple M1/MPS\n\nAlban Desmaison (alband)\n\nNikita Shulga (malfet)\n\nKulin Seth (kulinseth)\n\nRamin Azarmehr (razarmehr)\n\nPowerPC\n\nAlfredo Mendoza (avmgithub)\n\nDocs / Tutorials\n\nSvetlana Karslioglu (svekars)\n\nLibrary-level maintainers\nXLA\n\nJack Cao (JackCaoG)\n\nDaniel Sohn (jysohn23)\n\nZach Cain (zcain117)\n\nBrian Hirsch (bdhirsh)\n\nGregory Chanan (gchanan)\n\n(emeritus) Ailing Zhang (ailzhang)\n\n(emeritus) Davide Libenzi (dlibenzi)\n\n(emeritus) Alex Suhan (asuhan)\n\nTorchServe\n\nGeeta Chauhan (chauhang)\n\nManoj Rao (mycpuorg)\n\nVamshi Dantu (vdantu)\n\nDhanasekar Karuppasamy (dhanainme)\n\nTorchVision\n\nFrancisco Massa (fmassa)\n\nVasilis Vryniotis (datumbox)\n\nNicolas Hug (NicolasHug)\n\nYosua Michael Maranatha (YosuaMichael)\n\nJoao Gomes (jdsgomes)\n\nPhilip Meier (pmeier)\n\nVictor Fomin (vfdev-5)\n\nTorchText\n\nNayef Ahmed (Nayef211)\n\n(emeritus) Parmeet Singh Bhatia (parmeet)\n\n(emeritus) Guanheng George Zhang (zhangguanheng66)\n\n(emeritus) Christian Puhrsch (cpuhrsch)\n\nTorchAudio\n\nMoto Hira (mthrok)\n\nJeff Hwang (hwangjeff)\n\n(emeritus) Caroline Chen (carolineechen)\n\n(emeritus) Xiaohui Zhang (xiaohui-zhang)\n\n(emeritus) Zhaoheng Ni (nateanl)\n\n(emeritus) Christian Puhrsch (cpuhrsch)\n\n(emeritus) Vincent QB (vincentqb)\n\nTorchRec\n\nDmytro Ivchenko (divchenko)\n\nColin Taylor (colin2328)\n\nTorchX\n\nTristan Rice (d4l3k)\n\nKiuk Chung (kiukchung)\n\nTorchData / TorchArrow\n\nWenlei Xie (wenleix)\n\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch Governance | Maintainers\nResponsibilities\nLead Core Maintainer (BDFL)\nCore Maintainers\nModule-level maintainers\nLibrary-level maintainers\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch Governance | Mechanics — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/community/governance.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch Governance | Mechanics\nShortcuts\nPYTORCH GOVERNANCE | MECHANICS\nSummary\n\nPyTorch adopts a technical governance structure that is hierarchical.\n\nA community of contributors who file issues, make pull requests, and contribute to the project.\n\nA small set of module maintainers drive each module of the PyTorch project.\n\nThey are overseen by core maintainers, who drive the overall project direction.\n\nThe core maintainers have a lead core maintainer who is the catch-all decision maker.\n\nAll maintainers are expected to have a strong bias towards PyTorch’s design philosophy.\n\nBeyond the maintainers, the community is encouraged to contribute, file issues, make proposals, review pull requests and be present in the community. Given contributions and willingness to invest, anyone can be accepted as a maintainer and provided write access or ownership of parts of the codebase.\n\nTechnical governance is strictly separated from business governance. Separating technical from business governance ensures that there is no way for any person or company to “buy their way into” the technical guidance of the project. Additionally, membership in the technical governance process is for individuals, not companies. That is, there are no seats reserved for specific companies, and membership is associated with the person rather than the company employing that person.\n\nModule Maintainers\n\nModules are defined as GitHub repositories within the PyTorch org, or as directories within the core repository pytorch/pytorch. Each module will have its own maintainer group. Maintainer groups are responsible for reviewing and approving commits, improving design, and changing the scope of the module. Each maintainer group may adopt its own rules and procedures for making decisions (majority vote being default). Module maintainers have the right to dispute decisions made by other module maintainers – especially if it affects them. When disputes are made, the module maintainer group should provide a reasonable and public explanation of the dispute, the relevant arguments, and the resolution. In the exceptional cases where module maintainers cannot come to a conclusion themselves, they will escalate to core maintainers for review. The escalations are resolved by the core maintainers in accordance with their rules and procedures.\n\nEach maintainer group should publish publicly available communication for their module (a vision, rough roadmap, design docs, any disputes and dispute resolutions) so that contributors and other interested parties understand the future direction of the project and can participate in discussion.\n\nResponsibilities of the maintainer includes:\n\nTriaging high priority issues of the module\n\nTriaging and reviewing and landing high priority pull requests of the module\n\nSupporting public documentation related to the module\n\nRunning public developer meetings\n\nCore Maintainers\n\nThe core maintainers are expected to have a deep understanding of the PyTorch code base and design philosophies. Their responsibilities include:\n\nArticulating a cohesive long-term vision for the project\n\nNegotiating and resolving contentious issues in ways acceptable to all parties involved\n\nReceiving broad requests for changes from stakeholders of PyTorch and evaluating / accepting them (small module-level requests are handled by module maintainers)\n\nThe core maintainers as a group have the power to veto any decision made at a Module maintainer level. The core maintainers have power to resolve disputes as they see fit. The core maintainers should publicly articulate their decision-making, and give a clear reasoning for their decisions, vetoes and dispute resolution.\n\nThe core maintainers are admins of the PyTorch GitHub Org and are listed in Maintainers.\n\nLead Core Maintainer (BDFL)\n\nThere may be decisions in which the core maintainers cannot come to a consensus. To make such difficult decisions, the core maintainers have an assigned and publicly declared Lead Core Maintainer amongst them, also commonly known in open-source governance models as a BDFL.\n\nThe Lead Core Maintainer should publicly articulate their decision-making, and give a clear reasoning for their decisions. The Lead Core Maintainer is also responsible for confirming or removing core maintainers.\n\nNominating, Confirming and Removing Maintainers\nThe Principles\n\nMembership in module maintainer groups is given to individuals on merit basis after they demonstrated strong expertise of the component through contributions, reviews and discussions and are aligned with how the component fits in overall PyTorch direction.\n\nFor membership in the maintainer group the individual has to demonstrate strong and continued alignment with the overall PyTorch principles.\n\nNo term limits for module maintainers or core maintainers\n\nLight criteria of moving module maintenance to ‘emeritus’ status if they don’t actively participate over long periods of time. Each module maintainer group may define the inactive period that’s appropriate for that module.\n\nThe membership is for an individual, not a company.\n\nThe Process for Nomination\n\nEach module has its own process. Please contact module maintainers for more information. However, if there is no process identified, you can file a request to the core maintainers by submitting this form. Core maintainers are meeting every three months.\n\nIf you are submitting a request to the core maintainers, the information in your request must include the following items:\n\nThe nominees depth and breadth of code, review and design contributions on the module\n\nTestimonials (positive and negative) of the nominee’s interactions with the maintainers, users, and the community\n\nGeneral testimonials of support from the maintainers\n\nThe core maintainers then evaluate all information and make a final decision to Confirm or Decline the nomination. The decision of the core maintainers has to be articulated well and would be public.\n\nThe Process for Removal\n\nSimilar to the process for nomination, anyone in the community can nominate a person to be removed from a Module maintainer position or a Core maintainer position.\n\nA person can also self-nominate to be removed\n\nThe core maintainers (excluding persons with conflict of interest) will request or put together more information around the following:\n\nTheir activity (or lack of) on the project\n\nTheir changing thinking of the space, which results in conflict with the overall direction of the project\n\nOther information that makes them unfit to be a maintainer, such as Code of Conduct issues, their activity outside the scope of the project that conflicts with the project’s values\n\nConflicts of interest: filial or romantic relationships\n\nThe core maintainers then evaluate all information and make a final decision to Confirm or Decline the removal. The decision of the core maintainers has to be articulated well and would be public.\n\nNominating Core Maintainers\n\nAny core or module maintainer can nominate someone to become a core maintainer\n\nThe lead maintainer (BDFL) is responsible for evaluating the nomination.\n\nThe lead maintainer requests or puts together more information around the strength of the candidate to be a core maintainer:\n\nLetters of support from other core and module maintainers\n\nGeneral letters of support from stakeholders within the PyTorch community\n\nAny new relevant information that is befitting for the candidacy\n\nThe lead maintainer evaluates all information and makes a final decision to Confirm or Decline the nomination, with a clear public articulation of their reasoning behind the decision.\n\nRemoving the Lead Core Maintainer and Nominating a New Lead Core Maintainer\n\nA super-majority of core maintainers (75%) can choose to remove the Lead Core Maintainer\n\nAfter a removal of the Lead Core Maintainer or in unforeseen circumstances (such as permanent unavailability of the Lead Core Maintainer), the core maintainers follow a Ranked-Choice voting method to elect a new Lead Core Maintainer.\n\nAdd, Remove, and Re-Scope Modules and Projects\n\nThe core maintainers together are responsible for taking decisions on adding, removing and re-scoping new modules in the PyTorch org, either as new repositories in the PyTorch GitHub org, or as folders in the pytorch/pytorch repository.\n\nThey invite proposals from members in the community (including themselves) for such changes. The proposals are open-ended, but should have some basic ground-work to make a convincing case to make change. The following is an example approach to this process:\n\nInterview researchers / stakeholders, talk to community, gather issues;\n\nRead papers, attend conferences, build example pipelines based on experience;\n\nCreate a state of the world - make sure this change is necessary, for example adding a new project or module is worth the maintenance cost; or removing a project or module will not remove too much value from PyTorch;\n\nCreate a proposal; the proposal covers the maintainership, development and community plan once the proposal is approved.\n\nThe core maintainers take final decisions on the proposal, articulating the reasoning behind the decision publicly.\n\nDecision Making\nUncontroversial Changes\n\nPrimary work happens through issues and pull requests on GitHub. Maintainers should avoid pushing their changes directly to the PyTorch repository, instead relying on pull requests. Approving a pull request by a core or module maintainer allows it to be merged without further process. Core and module maintainers, as listed on the Maintainers page and within CODEOWNERS ultimately approve these changes.\n\nNotifying relevant experts about an issue or a pull request is important. Reviews from experts in the given interest area are strongly preferred, especially on pull request approvals. Failure to do so might end up with the change being reverted by the relevant expert.\n\nControversial Decision Process\n\nSubstantial changes in a given interest area require a GitHub issue to be opened for discussion. This includes:\n\nAny semantic or syntactic change to the PyTorch framework or library.\n\nBackwards-incompatible changes to the Python or C++ API.\n\nAdditions to the core framework or library, including substantial new functionality within an existing library.\n\nRemoval of core features or platform support\n\nCore and module maintainers ultimately approve these changes.\n\nGeneral Project Policies\n\nPyTorch has been established as PyTorch a Series of LF Projects, LLC. Policies applicable to PyTorch and participants in PyTorch, including guidelines on the usage of trademarks, are located at https://www.lfprojects.org/policies/.\n\nPyTorch participants acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the project. Except as described below, all code contributions to the project must be made using the 3-Clause-BSD License available here: https://opensource.org/licenses/BSD-3-Clause (the “Project License”). All outbound code will be made available under the Project License. The Maintainers may approve the use of an alternative open license or licenses for inbound or outbound contributions on an exception basis.\n\nFAQ\n\nQ: What if I would like to own (or partly own) a part of the project such as a feature area or domain library, for example Linear Algebra or Torch Vision ? This is absolutely possible. The first step is to start contributing to the existing project area and supporting its health and success. In addition to this, you can make a proposal through a GitHub issue for new functionality or changes to improve the project area.\n\nQ: What if I am a company looking to use PyTorch internally for development, can I be granted or purchase a board seat to drive the project direction? No, the PyTorch project is strictly driven by the a maintainer project philosophy and clearly separates technical governance from business governance. However, if you want to be involved in sponsorship and support, you can become involved in the PyTorch Foundation (PTF) and sponsorship through this. You can also have individual engineers look to become maintainers, but this is not guaranteed and is merit-based.\n\nQ: Does the PyTorch project support grants or ways to support independent developers using or contributing to the project? No, not at this point. We are however looking at ways to better support the community of independent developers around PyTorch. If you have suggestions or inputs, please reach out on the PyTorch forums to discuss.\n\nQ: How do I contribute code to the project? If the change is relatively minor, a pull request on GitHub can be opened up immediately for review and merge by the project committers. For larger changes, please open an issue to make a proposal to discuss prior. Please also see the PyTorch Contributor Guide for contribution guidelines.\n\nQ: Can I become a committer on the project? Unfortunately, the current commit process to PyTorch involves an interaction with Facebook infrastructure that can only be triggered by Facebook employees. We are however looking at ways to expand the committer base to individuals outside of Facebook and will provide an update when the tooling exists to allow this.\n\nQ: What if I would like to deliver a PyTorch tutorial at a conference or otherwise? Do I need to be ‘officially’ a committer to do this? No, we encourage community members to showcase their work wherever and whenever they can. Please reach out to marketing@pytorch.org for marketing support.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch Governance | Mechanics\nSummary\nModule Maintainers\nCore Maintainers\nLead Core Maintainer (BDFL)\nNominating, Confirming and Removing Maintainers\nAdd, Remove, and Re-Scope Modules and Projects\nDecision Making\nFAQ\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch Design Philosophy — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/community/design.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch Design Philosophy\nShortcuts\nPYTORCH DESIGN PHILOSOPHY\n\nThis document is designed to help contributors and module maintainers understand the high-level design principles that have developed over time in PyTorch. These are not meant to be hard-and-fast rules, but to serve as a guide to help trade off different concerns and to resolve disagreements that may come up while developing PyTorch. For more information on contributing, module maintainership, and how to escalate a disagreement to the Core Maintainers, please see PyTorch Governance.\n\nDesign Principles\nPrinciple 1: Usability over Performance\n\nThis principle may be surprising! As one Hacker News poster wrote: PyTorch is amazing! […] Although I’m confused. How can a ML framework be not obsessed with speed/performance? See Hacker News discussion on PyTorch.\n\nSoumith’s blog post on Growing the PyTorch Community goes into this in some depth, but at a high-level:\n\nPyTorch’s primary goal is usability\n\nA secondary goal is to have reasonable performance\n\nWe believe the ability to maintain our flexibility to support researchers who are building on top of our abstractions remains critical. We can’t see what the future of what workloads will be, but we know we want them to be built first on PyTorch and that requires flexibility.\n\nIn more concrete terms, we operate in a usability-first manner and try to avoid jumping to restriction-first regimes (for example, static shapes, graph-mode only) without a clear-eyed view of the tradeoffs. Often there is a temptation to impose strict user restrictions upfront because it can simplify implementation, but this comes with risks:\n\nThe performance may not be worth the user friction, either because the performance benefit is not compelling enough or it only applies to a relatively narrow set of subproblems.\n\nEven if the performance benefit is compelling, the restrictions can fragment the ecosystem into different sets of limitations that can quickly become incomprehensible to users.\n\nWe want users to be able to seamlessly move their PyTorch code to different hardware and software platforms, to interoperate with different libraries and frameworks, and to experience the full richness of the PyTorch user experience, not a least common denominator subset.\n\nPrinciple 2: Simple Over Easy\n\nHere, we borrow from The Zen of Python:\n\nExplicit is better than implicit\n\nSimple is better than complex\n\nA more concise way of describing these two goals is Simple Over Easy. Let’s start with an example because simple and easy are often used interchangeably in everyday English. Consider how one may model devices in PyTorch:\n\nSimple / Explicit (to understand, debug): every tensor is associated with a device. The user explicitly specifies tensor device movement. Operations that require cross-device movement result in an error.\n\nEasy / Implicit (to use): the user does not have to worry about devices; the system figures out the globally optimal device placement.\n\nIn this specific case, and as a general design philosophy, PyTorch favors exposing simple and explicit building blocks rather than APIs that are easy-to-use by practitioners. The simple version is immediately understandable and debuggable by a new PyTorch user: you get a clear error if you call an operator requiring cross-device movement at the point in the program where the operator is actually invoked. The easy solution may let a new user move faster initially, but debugging such a system can be complex: How did the system make its determination? What is the API for plugging into such a system and how are objects represented in its IR?\n\nSome classic arguments in favor of this sort of design come from A Note on Distributed Computation (TLDR: Do not model resources with very different performance characteristics uniformly, the details will leak) and the End-to-End Principle (TLDR: building smarts into the lower-layers of the stack can prevent building performant features at higher layers in the stack, and often doesn’t work anyway). For example, we could build operator-level or global device movement rules, but the precise choices aren’t obvious and building an extensible mechanism has unavoidable complexity and latency costs.\n\nA caveat here is that this does not mean that higher-level “easy” APIs are not valuable; certainly there is a value in, for example, higher-levels in the stack to support efficient tensor computations across heterogeneous compute in a large cluster. Instead, what we mean is that focusing on simple lower-level building blocks helps inform the easy API while still maintaining a good experience when users need to leave the beaten path. It also allows space for innovation and the growth of more opinionated tools at a rate we cannot support in the PyTorch core library, but ultimately benefit from, as evidenced by our rich ecosystem. In other words, not automating at the start allows us to potentially reach levels of good automation faster.\n\nPrinciple 3: Python First with Best In Class Language Interoperability\n\nThis principle began as Python First:\n\nPyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy, SciPy, scikit-learn, or other Python libraries. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.\n\nOne thing PyTorch has needed to deal with over the years is Python overhead: we first rewrote the autograd engine in C++, then the majority of operator definitions, then developed TorchScript and the C++ frontend.\n\nStill, working in Python provides easily the best experience for our users: it is flexible, familiar, and perhaps most importantly, has a huge ecosystem of scientific computing libraries and extensions available for use. This fact motivates a few of our most recent contributions, which attempt to hit a Pareto optimal point close to the Python usability end of the curve:\n\nTorchDynamo, a Python frame evaluation tool capable of speeding up existing eager-mode PyTorch programs with minimal user intervention.\n\ntorch_function and torch_dispatch extension points, which have enabled Python-first functionality to be built on-top of C++ internals, such as the torch.fx tracer and functorch respectively.\n\nThese design principles are not hard-and-fast rules, but hard won choices and anchor how we built PyTorch to be the debuggable, hackable and flexible framework it is today. As we have more contributors and maintainers, we look forward to applying these core principles with you across our libraries and ecosystem. We are also open to evolving them as we learn new things and the AI space evolves, as we know it will.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch Design Philosophy\nDesign Principles\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch documentation — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/index.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch documentation\n Edit on GitHub\nShortcuts\nPYTORCH DOCUMENTATION\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n\nFeatures described in this documentation are classified by release status:\n\nStable: These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\n\nBeta: These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n\nPrototype: These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.\n\nCommunity\n\nPyTorch Governance | Build + CI\nPyTorch Contribution Guide\nPyTorch Design Philosophy\nPyTorch Governance | Mechanics\nPyTorch Governance | Maintainers\n\nDeveloper Notes\n\nCUDA Automatic Mixed Precision examples\nAutograd mechanics\nBroadcasting semantics\nCPU threading and TorchScript inference\nCUDA semantics\nDistributed Data Parallel\nExtending PyTorch\nExtending torch.func with autograd.Function\nFrequently Asked Questions\nGradcheck mechanics\nHIP (ROCm) semantics\nFeatures for large-scale deployments\nModules\nMPS backend\nMultiprocessing best practices\nNumerical accuracy\nReproducibility\nSerialization semantics\nWindows FAQ\n\nLanguage Bindings\n\nC++\nJavadoc\ntorch::deploy\n\nPython API\n\ntorch\nTensors\nGenerators\nRandom sampling\nSerialization\nParallelism\nLocally disabling gradient computation\nMath operations\nUtilities\nSymbolic Numbers\nExport Path\nOptimizations\nOperator Tags\ntorch.nn\nParameter\nUninitializedParameter\nUninitializedBuffer\nContainers\nConvolution Layers\nPooling layers\nPadding Layers\nNon-linear Activations (weighted sum, nonlinearity)\nNon-linear Activations (other)\nNormalization Layers\nRecurrent Layers\nTransformer Layers\nLinear Layers\nDropout Layers\nSparse Layers\nDistance Functions\nLoss Functions\nVision Layers\nShuffle Layers\nDataParallel Layers (multi-GPU, distributed)\nUtilities\nQuantized Functions\nLazy Modules Initialization\ntorch.nn.functional\nConvolution functions\nPooling functions\nAttention Mechanisms\nNon-linear activation functions\nLinear functions\nDropout functions\nSparse functions\nDistance functions\nLoss functions\nVision functions\nDataParallel functions (multi-GPU, distributed)\ntorch.Tensor\nData types\nInitializing and basic operations\nTensor class reference\nTensor Attributes\ntorch.dtype\ntorch.device\ntorch.layout\ntorch.memory_format\nTensor Views\ntorch.amp\nAutocasting\nGradient Scaling\nAutocast Op Reference\ntorch.autograd\ntorch.autograd.backward\ntorch.autograd.grad\nForward-mode Automatic Differentiation\nFunctional higher level API\nLocally disabling gradient computation\nDefault gradient layouts\nIn-place operations on Tensors\nVariable (deprecated)\nTensor autograd functions\nFunction\nContext method mixins\nNumerical gradient checking\nProfiler\nAnomaly detection\nAutograd graph\ntorch.library\nLibrary\nfallthrough_kernel()\ntorch.cpu\ntorch.cpu.current_stream\ntorch.cpu.is_available\ntorch.cpu.synchronize\ntorch.cpu.stream\ntorch.cpu.device_count\nStreamContext\nStreams and events\ntorch.cuda\nStreamContext\ntorch.cuda.can_device_access_peer\ntorch.cuda.current_blas_handle\ntorch.cuda.current_device\ntorch.cuda.current_stream\ntorch.cuda.default_stream\ndevice\ntorch.cuda.device_count\ndevice_of\ntorch.cuda.get_arch_list\ntorch.cuda.get_device_capability\ntorch.cuda.get_device_name\ntorch.cuda.get_device_properties\ntorch.cuda.get_gencode_flags\ntorch.cuda.get_sync_debug_mode\ntorch.cuda.init\ntorch.cuda.ipc_collect\ntorch.cuda.is_available\ntorch.cuda.is_initialized\ntorch.cuda.memory_usage\ntorch.cuda.set_device\ntorch.cuda.set_stream\ntorch.cuda.set_sync_debug_mode\ntorch.cuda.stream\ntorch.cuda.synchronize\ntorch.cuda.utilization\ntorch.cuda.temperature\ntorch.cuda.power_draw\ntorch.cuda.clock_rate\ntorch.cuda.OutOfMemoryError\nRandom Number Generator\nCommunication collectives\nStreams and events\nGraphs (beta)\nMemory management\nNVIDIA Tools Extension (NVTX)\nJiterator (beta)\nStream Sanitizer (prototype)\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nActive Memory Timeline\nAllocator State History\nSnapshot API Reference\n_record_memory_history()\n_snapshot()\n_dump_snapshot()\ntorch.mps\ntorch.mps.synchronize\ntorch.mps.get_rng_state\ntorch.mps.set_rng_state\ntorch.mps.manual_seed\ntorch.mps.seed\ntorch.mps.empty_cache\ntorch.mps.set_per_process_memory_fraction\ntorch.mps.current_allocated_memory\ntorch.mps.driver_allocated_memory\nMPS Profiler\nMPS Event\ntorch.backends\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.mps\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.xeon\ntorch.export\nOverview\nExporting a PyTorch Model\nLimitations of torch.export\nRead More\nAPI Reference\ntorch.distributed\nBackends\nBasics\nInitialization\nPost-Initialization\nDistributed Key-Value Store\nGroups\nPoint-to-point communication\nSynchronous and asynchronous collective operations\nCollective functions\nProfiling Collective Communication\nMulti-GPU collective functions\nThird-party backends\nLaunch utility\nSpawn utility\nDebugging torch.distributed applications\nLogging\ntorch.distributed.algorithms.join\nJoin\nJoinable\nJoinHook\ntorch.distributed.elastic\nGet Started\nDocumentation\ntorch.distributed.fsdp\nFullyShardedDataParallel\nBackwardPrefetch\nShardingStrategy\nMixedPrecision\nCPUOffload\nStateDictConfig\nFullStateDictConfig\nShardedStateDictConfig\nLocalStateDictConfig\nOptimStateDictConfig\nFullOptimStateDictConfig\nShardedOptimStateDictConfig\nLocalOptimStateDictConfig\nStateDictSettings\ntorch.distributed.optim\nDistributedOptimizer\nPostLocalSGDOptimizer\nZeroRedundancyOptimizer\ntorch.distributed.tensor.parallel\nparallelize_module()\nRowwiseParallel\nColwiseParallel\nPairwiseParallel\nSequenceParallel\nmake_input_replicate_1d()\nmake_input_reshard_replicate()\nmake_input_shard_1d()\nmake_input_shard_1d_last_dim()\nmake_output_replicate_1d()\nmake_output_reshard_tensor()\nmake_output_shard_1d()\nmake_output_tensor()\nenable_2d_with_fsdp()\npre_dp_module_transform()\ntorch.distributed.checkpoint\nload_state_dict()\nsave_state_dict()\nStorageReader\nStorageWriter\nLoadPlanner\nLoadPlan\nReadItem\nSavePlanner\nSavePlan\nWriteItem\nFileSystemReader\nFileSystemWriter\nDefaultSavePlanner\nDefaultLoadPlanner\ntorch.distributions\nScore function\nPathwise derivative\nDistribution\nExponentialFamily\nBernoulli\nBeta\nBinomial\nCategorical\nCauchy\nChi2\nContinuousBernoulli\nDirichlet\nExponential\nFisherSnedecor\nGamma\nGeometric\nGumbel\nHalfCauchy\nHalfNormal\nIndependent\nKumaraswamy\nLKJCholesky\nLaplace\nLogNormal\nLowRankMultivariateNormal\nMixtureSameFamily\nMultinomial\nMultivariateNormal\nNegativeBinomial\nNormal\nOneHotCategorical\nPareto\nPoisson\nRelaxedBernoulli\nLogitRelaxedBernoulli\nRelaxedOneHotCategorical\nStudentT\nTransformedDistribution\nUniform\nVonMises\nWeibull\nWishart\nKL Divergence\nTransforms\nConstraints\nConstraint Registry\ntorch.compiler\nRead More\ntorch.fft\nFast Fourier Transforms\nHelper Functions\ntorch.func\nWhat are composable function transforms?\nWhy composable function transforms?\nRead More\ntorch.futures\nFuture\ncollect_all()\nwait_all()\ntorch.fx\nOverview\nWriting Transformations\nDebugging\nLimitations of Symbolic Tracing\nAPI Reference\ntorch.hub\nPublishing models\nLoading models from Hub\ntorch.jit\nTorchScript Language Reference\nCreating TorchScript Code\nMixing Tracing and Scripting\nTorchScript Language\nBuilt-in Functions and Modules\nDebugging\nFrequently Asked Questions\nKnown Issues\nAppendix\ntorch.linalg\nMatrix Properties\nDecompositions\nSolvers\nInverses\nMatrix Functions\nMatrix Products\nTensor Operations\nMisc\nExperimental Functions\ntorch.monitor\nAPI Reference\ntorch.signal\ntorch.signal.windows\ntorch.special\nFunctions\ntorch.overrides\nFunctions\ntorch.package\nTutorials\nHow do I…\nExplanation\nAPI Reference\ntorch.profiler\nOverview\nAPI Reference\nIntel Instrumentation and Tracing Technology APIs\ntorch.nn.init\ncalculate_gain()\nuniform_()\nnormal_()\nconstant_()\nones_()\nzeros_()\neye_()\ndirac_()\nxavier_uniform_()\nxavier_normal_()\nkaiming_uniform_()\nkaiming_normal_()\ntrunc_normal_()\northogonal_()\nsparse_()\ntorch.onnx\nOverview\nTorchDynamo-based ONNX Exporter\nTorchScript-based ONNX Exporter\nContributing / Developing\ntorch.optim\nHow to use an optimizer\nBase class\nAlgorithms\nHow to adjust learning rate\nWeight Averaging (SWA and EMA)\nComplex Numbers\nCreating Complex Tensors\nTransition from the old representation\nAccessing real and imag\nAngle and abs\nLinear Algebra\nSerialization\nAutograd\nDDP Communication Hooks\nHow to Use a Communication Hook?\nWhat Does a Communication Hook Operate On?\nDefault Communication Hooks\nPowerSGD Communication Hook\nDebugging Communication Hooks\nCheckpointing of Communication Hooks\nAcknowledgements\nPipeline Parallelism\nModel Parallelism using multiple GPUs\nPipelined Execution\nPipe APIs in PyTorch\nTutorials\nAcknowledgements\nQuantization\nIntroduction to Quantization\nQuantization API Summary\nQuantization Stack\nQuantization Support Matrix\nQuantization API Reference\nQuantization Backend Configuration\nQuantization Accuracy Debugging\nQuantization Customizations\nBest Practices\nFrequently Asked Questions\nCommon Errors\nDistributed RPC Framework\nBasics\nRPC\nRRef\nRemoteModule\nDistributed Autograd Framework\nDistributed Optimizer\nDesign Notes\nTutorials\ntorch.random\nfork_rng()\nget_rng_state()\ninitial_seed()\nmanual_seed()\nseed()\nset_rng_state()\ntorch.masked\nIntroduction\nSupported Operators\ntorch.nested\nIntroduction\nConstruction\nsize\nunbind\nNested tensor constructor and conversion functions\nSupported operations\ntorch.sparse\nWhy and when to use sparsity\nFunctionality overview\nOperator overview\nSparse Semi-Structured Tensors\nSparse COO tensors\nSparse Compressed Tensors\nSupported operations\ntorch.Storage\nTypedStorage\nUntypedStorage\nDoubleStorage\nFloatStorage\nHalfStorage\nLongStorage\nIntStorage\nShortStorage\nCharStorage\nByteStorage\nBoolStorage\nBFloat16Storage\nComplexDoubleStorage\nComplexFloatStorage\nQUInt8Storage\nQInt8Storage\nQInt32Storage\nQUInt4x2Storage\nQUInt2x4Storage\ntorch.testing\nassert_close()\nmake_tensor()\nassert_allclose()\ntorch.utils\ntorch.utils.rename_privateuse1_backend\ntorch.utils.generate_methods_for_privateuse1_backend\ntorch.utils.get_cpp_backtrace\ntorch.utils.set_module\ntorch.utils.benchmark\nTimer\nMeasurement\nCallgrindStats\nFunctionCounts\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ncheckpoint()\ncheckpoint_sequential()\ntorch.utils.cpp_extension\nCppExtension()\nCUDAExtension()\nBuildExtension()\nload()\nload_inline()\ninclude_paths()\nget_compiler_abi_compatibility_and_version()\nverify_ninja_availability()\nis_ninja_available()\ntorch.utils.data\nDataset Types\nData Loading Order and Sampler\nLoading Batched and Non-Batched Data\nSingle- and Multi-process Data Loading\nMemory Pinning\ntorch.utils.jit\ntorch.utils.dlpack\nfrom_dlpack()\nto_dlpack()\ntorch.utils.mobile_optimizer\noptimize_for_mobile()\ntorch.utils.model_zoo\nload_url()\ntorch.utils.tensorboard\nSummaryWriter\nType Info\ntorch.finfo\ntorch.iinfo\nNamed Tensors\nCreating named tensors\nNamed dimensions\nName propagation semantics\nExplicit alignment by names\nManipulating dimensions\nAutograd support\nCurrently supported operations and subsystems\nNamed tensor API reference\nNamed Tensors operator coverage\nKeeps input names\nRemoves dimensions\nUnifies names from inputs\nPermutes dimensions\nContracts away dims\nFactory functions\nout function and in-place variants\ntorch.__config__\nshow()\nparallel_info()\ntorch._logging\ntorch._logging.set_logs\n\nLibraries\n\ntorchaudio\nTorchData\nTorchRec\nTorchServe\ntorchtext\ntorchvision\nPyTorch on XLA Devices\nINDICES AND TABLES\n\nIndex\n\nModule Index\n\nNext \n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch documentation\nIndices and tables\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch Contribution Guide — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/community/contribution_guide.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch Contribution Guide\nShortcuts\nPYTORCH CONTRIBUTION GUIDE\n\nPyTorch is a GPU-accelerated Python tensor computation package for building deep neural networks using a tape-based autograd systems.\n\nContribution Process\n\nThe PyTorch organization is governed by PyTorch Governance and the technical guide to contributing can be found in CONTRIBUTING.md.\n\nThe PyTorch development process involves a healthy amount of open discussions between the core development team and the community.\n\nPyTorch operates similarly to most open source projects on GitHub. However, if you’ve never contributed to an open source project before, here is the basic process.\n\nFigure out what you’re going to work on. The majority of open source contributions come from people scratching their own itches. However, if you don’t know what you want to work on, or are just looking to get more acquainted with the project, here are some tips for how to find appropriate tasks:\n\nLook through the issue tracker and see if there are any issues you know how to fix. Issues that are confirmed by other contributors tend to be better to investigate. We also maintain some labels for issues that are likely to be good for new people, e.g., bootcamp and 1hr, although these labels are less well maintained.\n\nJoin us on dev discuss and let us know you’re interested in getting to know PyTorch. We’re very happy to help out researchers and partners get up to speed with the codebase.\n\nFigure out the scope of your change and reach out for design comments on a GitHub issue if it’s large. The majority of pull requests are small; in that case, no need to let us know about what you want to do, just get cracking. But if the change is going to be large, it’s usually a good idea to get some design comments about it first by submitting an RFC.\n\nIf you don’t know how big a change is going to be, we can help you figure it out! Just post about it on issues or dev discuss.\n\nSome feature additions are very standardized; for example, lots of people add new operators or optimizers to PyTorch. Design discussion in these cases boils down mostly to, “Do we want this operator/optimizer?” Giving evidence for its utility, e.g., usage in peer reviewed papers, or existence in other frameworks, helps a bit when making this case.\n\nAdding operators / algorithms from recently-released research is generally not accepted unless there is overwhelming evidence that this newly published work has ground-breaking results and will eventually become a standard in the field. If you are not sure where your method falls, open an issue first before implementing a PR.\n\nCore changes and refactors can be quite difficult to coordinate since the pace of development on the PyTorch main branch is quite fast. Definitely reach out about fundamental or cross-cutting changes; we can often give guidance about how to stage such changes into more easily reviewable pieces.\n\nCode it out!\n\nSee the CONTRIBUTING.md file for advice for working with PyTorch in a technical form.\n\nOpen a pull request.\n\nIf you are not ready for the pull request to be reviewed, create a draft pull request first - you can later convert it to a full PR by pressing “Ready for review” button. You can also prepend the title of the PR with “[WIP]” (“work in progress”) while it’s still in draft. We will ignore draft PRs when doing review passes. If you are working on a complex change, it’s good to start things off as a draft, because you will need to spend time looking at CI results to see if things worked out or not.\n\nFind an appropriate reviewer for your change. We have some folks who regularly go through the PR queue and try to review everything, but if you happen to know who the maintainer for a given subsystem affected by your patch is, feel free to include them directly on the pull request. You can learn more about Persons of Interest that could review your code.\n\nIterate on the pull request until it’s accepted!\n\nWe’ll try our best to minimize the number of review round trips and block PRs only when there are major issues. For the most common issues in pull requests, take a look at Common Mistakes.\n\nOnce a pull request is accepted and CI is passing, there is nothing else you need to do; we will merge the PR for you.\n\nGetting Started\nProposing New Features\n\nNew feature ideas are best discussed on a specific issue. Please include as much information as you can, any accompanying data, and your proposed solution. The PyTorch team and community frequently review new issues and comments where they think they can help. If you feel confident in your solution, go ahead and implement it.\n\nReporting Issues\n\nIf you’ve identified an issue, first search through the list of existing issues on the repo. If you are unable to find a similar issue, then create a new one. Supply as much information you can to reproduce the problematic behavior. Also, include any additional insights like the behavior you expect.\n\nImplementing Features or Fixing Bugs\n\nIf you want to fix a specific issue, it’s best to comment on the individual issue with your intent. However, we do not lock or assign issues except in cases where we have worked with the developer before. It’s best to strike up a conversation on the issue and discuss your proposed solution. The PyTorch team can provide guidance that saves you time.\n\nIssues that are labeled first-new-issue, low, or medium priority provide the best entrance points and are great places to start.\n\nAdding Tutorials\n\nA great deal of the tutorials on pytorch.org come from the community itself and we welcome additional contributions. To learn more about how to contribute a new tutorial you can learn more here: PyTorch.org Tutorial Contribution Guide on GitHub\n\nImproving Documentation & Tutorials\n\nWe aim to produce high quality documentation and tutorials. On rare occasions that content includes typos or bugs. If you find something you can fix, send us a pull request for consideration.\n\nTake a look at the Documentation section to learn how our system works.\n\nParticipating in Online Discussions\n\nYou can find active discussions happening on the PyTorch Discussion Forums for users as well as the PyTorch Dev Discussion Forums for developers and maintainers.\n\nSubmitting Pull Requests to Fix Open Issues\n\nYou can view a list of all open issues here. Commenting on an issue is a great way to get the attention of the team. From here you can share your ideas and how you plan to resolve the issue.\n\nFor more challenging issues, the team will provide feedback and direction for how to best solve the issue.\n\nIf you’re not able to fix the issue yourself, commenting and sharing whether you can reproduce the issue can help the team identify problem areas.\n\nReviewing Open Pull Requests\n\nWe appreciate your help reviewing and commenting on pull requests. Our team strives to keep the number of open pull requests at a manageable size, we respond quickly for more information if we need it, and we merge PRs that we think are useful. However, due to the high level of interest, additional eyes on the pull requests are always appreciated.\n\nImproving Code Readability\n\nImproving code readability helps everyone. It is often better to submit a small number of pull requests that touch a few files versus a large pull request that touches many files. Starting a discussion in the PyTorch forum here or on an issue related to your improvement is the best way to get started.\n\nAdding Test Cases to Make the Codebase More Robust\n\nAdditional test coverage is appreciated.\n\nPromoting PyTorch\n\nYour use of PyTorch in your projects, research papers, write ups, blogs, or general discussions around the internet helps to raise awareness for PyTorch and our growing community. Please reach out to marketing@pytorch.org for marketing support.\n\nTriaging Issues\n\nIf you feel that an issue could benefit from a particular tag or level of complexity, comment on the issue and share your opinion. If you feel an issue isn’t categorized properly, comment and let the team know.\n\nAbout Open Source Development\n\nIf this is your first time contributing to an open source project, some aspects of the development process may seem unusual to you.\n\nThere is no way to “claim” issues. People often want to “claim” an issue when they decide to work on it, to ensure that there isn’t wasted work when someone else ends up working on it. This doesn’t really work too well in open source, since someone may decide to work on something, and end up not having time to do it. Feel free to give information in an advisory fashion, but at the end of the day, we will take running code and rough consensus to move forward quickly.\n\nThere is a high bar for new functionality. Unlike in a corporate environment, where the person who wrote code implicitly “owns” it and can be expected to take care of it for the code’s lifetime, once a pull request is merged into an open source project, it immediately becomes the collective responsibility of all maintainers on the project. When we merge code, we are saying that we, the maintainers, can review subsequent changes and make a bugfix to the code. This naturally leads to a higher standard of contribution.\n\nCommon Mistakes To Avoid\n\nDid you add tests? (Or if the change is hard to test, did you describe how you tested your change?)\n\nWe have a few motivations for why we ask for tests:\n\nto help us tell if we break it later\n\nto help us tell if the patch is correct in the first place (yes, we did review it, but as Knuth says, “beware of the following code, for I have not run it, merely proven it correct”)\n\nWhen is it OK not to add a test? Sometimes a change can’t be conveniently tested, or the change is so obviously correct (and unlikely to be broken) that it’s OK not to test it. On the contrary, if a change seems likely (or is known to be likely) to be accidentally broken, it’s important to put in the time to work out a testing strategy.\n\nIs your PR too long?\n\nIt’s easier for us to review and merge small PRs. The difficulty of reviewing a PR scales nonlinearly with its size.\n\nWhen is it OK to submit a large PR? It helps a lot if there was a corresponding design discussion in an issue, with sign off from the people who are going to review your diff. We can also help give advice about how to split up a large change into individually shippable parts. Similarly, it helps if there is a complete description of the contents of the PR: it’s easier to review code if we know what’s inside!\n\nComments for subtle things? In cases where the behavior of your code is nuanced, please include extra comments and documentation to allow us to better understand the intention of your code.\n\nDid you add a hack? Sometimes, the right answer is a hack. But usually, we will have to discuss it.\n\nDo you want to touch a very core component? To prevent major regressions, pull requests that touch core components receive extra scrutiny. Make sure you’ve discussed your changes with the team before undertaking major changes.\n\nWant to add a new feature? If you want to add new features, comment your intention on the related issue. Our team tries to comment on and provide feedback to the community. It’s better to have an open discussion with the team and the rest of the community before building new features. This helps us stay aware of what you’re working on and increases the chance that it’ll be merged.\n\nDid you touch code unrelated to the PR? To aid in code review, please only include files in your pull request that are directly related to your changes.\n\nFrequently Asked Questions\n\nHow can I contribute as a reviewer? There is lots of value if community developers reproduce issues, try out new functionality, or otherwise help us identify or troubleshoot issues. Commenting on tasks or pull requests with your environment details is helpful and appreciated.\n\nCI tests failed, what does it mean? Maybe your PR is based off a broken main bracnh? You can try to rebase your change on top of the latest main branch. You can also see the current status of main branch’s CI at https://hud.pytorch.org/.\n\nWhat are the most high risk changes? Anything that touches build configuration is a risky area. Please avoid changing these unless you’ve had a discussion with the team beforehand.\n\nHey, a commit showed up on my branch, what’s up with that? Sometimes another community member will provide a patch or fix to your pull request or branch. This is often needed for getting CI tests to pass.\n\nOn Documentation\nPython Docs\n\nPyTorch documentation is generated from python source using Sphinx. Generated HTML is copied to the docs folder in the main branch of pytorch.github.io, and is served via GitHub pages.\n\nSite: https://pytorch.org/docs\n\nGitHub: https://github.com/pytorch/pytorch/tree/main/docs\n\nServed from: https://github.com/pytorch/pytorch.github.io/tree/master/docs\n\nC++ Docs\n\nFor C++ code we use Doxygen to generate the content files. The C++ docs are built on a special server and the resulting files are copied to the https://github.com/pytorch/cppdocs repo, and are served from GitHub pages.\n\nSite: https://pytorch.org/cppdocs\n\nGitHub: https://github.com/pytorch/pytorch/tree/main/docs/cpp\n\nServed from: https://github.com/pytorch/cppdocs\n\nTutorials\n\nPyTorch tutorials are documents used to help understand using PyTorch to accomplish specific tasks or to understand more holistic concepts. Tutorials are built using Sphinx-Gallery from executable python source files, or from restructured-text (rst) files.\n\nSite: https://pytorch.org/tutorials\n\nGitHub: https://github.com/pytorch/tutorials\n\nTutorials Build Overview\n\nFor tutorials, pull requests trigger a rebuild of the entire site using CircleCI to test the effects of the change. This build is sharded into 9 worker builds and takes around 40 minutes total. At the same time, we do a Netlify build using make html-noplot, which builds the site without rendering the notebook output into pages for quick review.\n\nAfter a PR is accepted, the site is rebuilt and deployed using GitHub Actions.\n\nContributing a New Tutorial\n\nSee PyTorch.org Tutorial Contribution Guide.\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch Contribution Guide\nContribution Process\nGetting Started\nAbout Open Source Development\nCommon Mistakes To Avoid\nFrequently Asked Questions\nOn Documentation\nTutorials\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch Governance | Build + CI — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/community/build_ci_governance.html",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch Governance | Build + CI\nShortcuts\nPYTORCH GOVERNANCE | BUILD + CI\nHow to Add a New Maintainer\n\nFor the person to be a maintainer, a person needs to:\n\nLand at least six commits to the related part of the PyTorch repository\n\nAt least one of these commits must be submitted in the last six months\n\nTo add a qualified person to the maintainers’ list, please create a PR that adds a person to the persons of interests page and merge_rules files. Current maintainers will cast their votes of support. Decision criteria for approving the PR:\n\nNot earlier than two business days passed before merging (ensure the majority of the contributors have seen it)\n\nPR has the correct label (module: ci)\n\nThere are no objections from the current maintainers\n\nThere are at least three net thumbs up from current maintainers (or all maintainers vote thumbs up when the module has less than 3 maintainers).\n\nNext \n Previous\n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch Governance | Build + CI\nHow to Add a New Maintainer\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/."
  },
  {
    "title": "PyTorch documentation — PyTorch 2.1 documentation",
    "url": "https://pytorch.org/docs/stable/",
    "html": "Get Started\nEcosystem\nMobile\nBlog\nTutorials\nDocs\nResources\nGitHub\n2.1 ▼\n\nCommunity[ + ]\n\nDeveloper Notes[ + ]\n\nLanguage Bindings[ + ]\n\nPython API[ - ]\n\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.Tensor\nTensor Attributes\nTensor Views\ntorch.amp\ntorch.autograd\ntorch.library\ntorch.cpu\ntorch.cuda\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nSnapshot API Reference\ntorch.mps\ntorch.backends\ntorch.export\ntorch.distributed\ntorch.distributed.algorithms.join\ntorch.distributed.elastic\ntorch.distributed.fsdp\ntorch.distributed.optim\ntorch.distributed.tensor.parallel\ntorch.distributed.checkpoint\ntorch.distributions\ntorch.compiler\ntorch.fft\ntorch.func\ntorch.futures\ntorch.fx\ntorch.hub\ntorch.jit\ntorch.linalg\ntorch.monitor\ntorch.signal\ntorch.special\ntorch.overrides\ntorch.package\ntorch.profiler\ntorch.nn.init\ntorch.onnx\ntorch.optim\nComplex Numbers\nDDP Communication Hooks\nPipeline Parallelism\nQuantization\nDistributed RPC Framework\ntorch.random\ntorch.masked\ntorch.nested\ntorch.sparse\ntorch.Storage\ntorch.testing\ntorch.utils\ntorch.utils.benchmark\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.jit\ntorch.utils.dlpack\ntorch.utils.mobile_optimizer\ntorch.utils.model_zoo\ntorch.utils.tensorboard\nType Info\nNamed Tensors\nNamed Tensors operator coverage\ntorch.__config__\ntorch._logging\n\nLibraries[ + ]\n\nDocs > PyTorch documentation\n Edit on GitHub\nShortcuts\nPYTORCH DOCUMENTATION\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n\nFeatures described in this documentation are classified by release status:\n\nStable: These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\n\nBeta: These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n\nPrototype: These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.\n\nCommunity\n\nPyTorch Governance | Build + CI\nPyTorch Contribution Guide\nPyTorch Design Philosophy\nPyTorch Governance | Mechanics\nPyTorch Governance | Maintainers\n\nDeveloper Notes\n\nCUDA Automatic Mixed Precision examples\nAutograd mechanics\nBroadcasting semantics\nCPU threading and TorchScript inference\nCUDA semantics\nDistributed Data Parallel\nExtending PyTorch\nExtending torch.func with autograd.Function\nFrequently Asked Questions\nGradcheck mechanics\nHIP (ROCm) semantics\nFeatures for large-scale deployments\nModules\nMPS backend\nMultiprocessing best practices\nNumerical accuracy\nReproducibility\nSerialization semantics\nWindows FAQ\n\nLanguage Bindings\n\nC++\nJavadoc\ntorch::deploy\n\nPython API\n\ntorch\nTensors\nGenerators\nRandom sampling\nSerialization\nParallelism\nLocally disabling gradient computation\nMath operations\nUtilities\nSymbolic Numbers\nExport Path\nOptimizations\nOperator Tags\ntorch.nn\nParameter\nUninitializedParameter\nUninitializedBuffer\nContainers\nConvolution Layers\nPooling layers\nPadding Layers\nNon-linear Activations (weighted sum, nonlinearity)\nNon-linear Activations (other)\nNormalization Layers\nRecurrent Layers\nTransformer Layers\nLinear Layers\nDropout Layers\nSparse Layers\nDistance Functions\nLoss Functions\nVision Layers\nShuffle Layers\nDataParallel Layers (multi-GPU, distributed)\nUtilities\nQuantized Functions\nLazy Modules Initialization\ntorch.nn.functional\nConvolution functions\nPooling functions\nAttention Mechanisms\nNon-linear activation functions\nLinear functions\nDropout functions\nSparse functions\nDistance functions\nLoss functions\nVision functions\nDataParallel functions (multi-GPU, distributed)\ntorch.Tensor\nData types\nInitializing and basic operations\nTensor class reference\nTensor Attributes\ntorch.dtype\ntorch.device\ntorch.layout\ntorch.memory_format\nTensor Views\ntorch.amp\nAutocasting\nGradient Scaling\nAutocast Op Reference\ntorch.autograd\ntorch.autograd.backward\ntorch.autograd.grad\nForward-mode Automatic Differentiation\nFunctional higher level API\nLocally disabling gradient computation\nDefault gradient layouts\nIn-place operations on Tensors\nVariable (deprecated)\nTensor autograd functions\nFunction\nContext method mixins\nNumerical gradient checking\nProfiler\nAnomaly detection\nAutograd graph\ntorch.library\nLibrary\nfallthrough_kernel()\ntorch.cpu\ntorch.cpu.current_stream\ntorch.cpu.is_available\ntorch.cpu.synchronize\ntorch.cpu.stream\ntorch.cpu.device_count\nStreamContext\nStreams and events\ntorch.cuda\nStreamContext\ntorch.cuda.can_device_access_peer\ntorch.cuda.current_blas_handle\ntorch.cuda.current_device\ntorch.cuda.current_stream\ntorch.cuda.default_stream\ndevice\ntorch.cuda.device_count\ndevice_of\ntorch.cuda.get_arch_list\ntorch.cuda.get_device_capability\ntorch.cuda.get_device_name\ntorch.cuda.get_device_properties\ntorch.cuda.get_gencode_flags\ntorch.cuda.get_sync_debug_mode\ntorch.cuda.init\ntorch.cuda.ipc_collect\ntorch.cuda.is_available\ntorch.cuda.is_initialized\ntorch.cuda.memory_usage\ntorch.cuda.set_device\ntorch.cuda.set_stream\ntorch.cuda.set_sync_debug_mode\ntorch.cuda.stream\ntorch.cuda.synchronize\ntorch.cuda.utilization\ntorch.cuda.temperature\ntorch.cuda.power_draw\ntorch.cuda.clock_rate\ntorch.cuda.OutOfMemoryError\nRandom Number Generator\nCommunication collectives\nStreams and events\nGraphs (beta)\nMemory management\nNVIDIA Tools Extension (NVTX)\nJiterator (beta)\nStream Sanitizer (prototype)\nUnderstanding CUDA Memory Usage\nGenerating a Snapshot\nUsing the visualizer\nActive Memory Timeline\nAllocator State History\nSnapshot API Reference\n_record_memory_history()\n_snapshot()\n_dump_snapshot()\ntorch.mps\ntorch.mps.synchronize\ntorch.mps.get_rng_state\ntorch.mps.set_rng_state\ntorch.mps.manual_seed\ntorch.mps.seed\ntorch.mps.empty_cache\ntorch.mps.set_per_process_memory_fraction\ntorch.mps.current_allocated_memory\ntorch.mps.driver_allocated_memory\nMPS Profiler\nMPS Event\ntorch.backends\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.mps\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.xeon\ntorch.export\nOverview\nExporting a PyTorch Model\nLimitations of torch.export\nRead More\nAPI Reference\ntorch.distributed\nBackends\nBasics\nInitialization\nPost-Initialization\nDistributed Key-Value Store\nGroups\nPoint-to-point communication\nSynchronous and asynchronous collective operations\nCollective functions\nProfiling Collective Communication\nMulti-GPU collective functions\nThird-party backends\nLaunch utility\nSpawn utility\nDebugging torch.distributed applications\nLogging\ntorch.distributed.algorithms.join\nJoin\nJoinable\nJoinHook\ntorch.distributed.elastic\nGet Started\nDocumentation\ntorch.distributed.fsdp\nFullyShardedDataParallel\nBackwardPrefetch\nShardingStrategy\nMixedPrecision\nCPUOffload\nStateDictConfig\nFullStateDictConfig\nShardedStateDictConfig\nLocalStateDictConfig\nOptimStateDictConfig\nFullOptimStateDictConfig\nShardedOptimStateDictConfig\nLocalOptimStateDictConfig\nStateDictSettings\ntorch.distributed.optim\nDistributedOptimizer\nPostLocalSGDOptimizer\nZeroRedundancyOptimizer\ntorch.distributed.tensor.parallel\nparallelize_module()\nRowwiseParallel\nColwiseParallel\nPairwiseParallel\nSequenceParallel\nmake_input_replicate_1d()\nmake_input_reshard_replicate()\nmake_input_shard_1d()\nmake_input_shard_1d_last_dim()\nmake_output_replicate_1d()\nmake_output_reshard_tensor()\nmake_output_shard_1d()\nmake_output_tensor()\nenable_2d_with_fsdp()\npre_dp_module_transform()\ntorch.distributed.checkpoint\nload_state_dict()\nsave_state_dict()\nStorageReader\nStorageWriter\nLoadPlanner\nLoadPlan\nReadItem\nSavePlanner\nSavePlan\nWriteItem\nFileSystemReader\nFileSystemWriter\nDefaultSavePlanner\nDefaultLoadPlanner\ntorch.distributions\nScore function\nPathwise derivative\nDistribution\nExponentialFamily\nBernoulli\nBeta\nBinomial\nCategorical\nCauchy\nChi2\nContinuousBernoulli\nDirichlet\nExponential\nFisherSnedecor\nGamma\nGeometric\nGumbel\nHalfCauchy\nHalfNormal\nIndependent\nKumaraswamy\nLKJCholesky\nLaplace\nLogNormal\nLowRankMultivariateNormal\nMixtureSameFamily\nMultinomial\nMultivariateNormal\nNegativeBinomial\nNormal\nOneHotCategorical\nPareto\nPoisson\nRelaxedBernoulli\nLogitRelaxedBernoulli\nRelaxedOneHotCategorical\nStudentT\nTransformedDistribution\nUniform\nVonMises\nWeibull\nWishart\nKL Divergence\nTransforms\nConstraints\nConstraint Registry\ntorch.compiler\nRead More\ntorch.fft\nFast Fourier Transforms\nHelper Functions\ntorch.func\nWhat are composable function transforms?\nWhy composable function transforms?\nRead More\ntorch.futures\nFuture\ncollect_all()\nwait_all()\ntorch.fx\nOverview\nWriting Transformations\nDebugging\nLimitations of Symbolic Tracing\nAPI Reference\ntorch.hub\nPublishing models\nLoading models from Hub\ntorch.jit\nTorchScript Language Reference\nCreating TorchScript Code\nMixing Tracing and Scripting\nTorchScript Language\nBuilt-in Functions and Modules\nDebugging\nFrequently Asked Questions\nKnown Issues\nAppendix\ntorch.linalg\nMatrix Properties\nDecompositions\nSolvers\nInverses\nMatrix Functions\nMatrix Products\nTensor Operations\nMisc\nExperimental Functions\ntorch.monitor\nAPI Reference\ntorch.signal\ntorch.signal.windows\ntorch.special\nFunctions\ntorch.overrides\nFunctions\ntorch.package\nTutorials\nHow do I…\nExplanation\nAPI Reference\ntorch.profiler\nOverview\nAPI Reference\nIntel Instrumentation and Tracing Technology APIs\ntorch.nn.init\ncalculate_gain()\nuniform_()\nnormal_()\nconstant_()\nones_()\nzeros_()\neye_()\ndirac_()\nxavier_uniform_()\nxavier_normal_()\nkaiming_uniform_()\nkaiming_normal_()\ntrunc_normal_()\northogonal_()\nsparse_()\ntorch.onnx\nOverview\nTorchDynamo-based ONNX Exporter\nTorchScript-based ONNX Exporter\nContributing / Developing\ntorch.optim\nHow to use an optimizer\nBase class\nAlgorithms\nHow to adjust learning rate\nWeight Averaging (SWA and EMA)\nComplex Numbers\nCreating Complex Tensors\nTransition from the old representation\nAccessing real and imag\nAngle and abs\nLinear Algebra\nSerialization\nAutograd\nDDP Communication Hooks\nHow to Use a Communication Hook?\nWhat Does a Communication Hook Operate On?\nDefault Communication Hooks\nPowerSGD Communication Hook\nDebugging Communication Hooks\nCheckpointing of Communication Hooks\nAcknowledgements\nPipeline Parallelism\nModel Parallelism using multiple GPUs\nPipelined Execution\nPipe APIs in PyTorch\nTutorials\nAcknowledgements\nQuantization\nIntroduction to Quantization\nQuantization API Summary\nQuantization Stack\nQuantization Support Matrix\nQuantization API Reference\nQuantization Backend Configuration\nQuantization Accuracy Debugging\nQuantization Customizations\nBest Practices\nFrequently Asked Questions\nCommon Errors\nDistributed RPC Framework\nBasics\nRPC\nRRef\nRemoteModule\nDistributed Autograd Framework\nDistributed Optimizer\nDesign Notes\nTutorials\ntorch.random\nfork_rng()\nget_rng_state()\ninitial_seed()\nmanual_seed()\nseed()\nset_rng_state()\ntorch.masked\nIntroduction\nSupported Operators\ntorch.nested\nIntroduction\nConstruction\nsize\nunbind\nNested tensor constructor and conversion functions\nSupported operations\ntorch.sparse\nWhy and when to use sparsity\nFunctionality overview\nOperator overview\nSparse Semi-Structured Tensors\nSparse COO tensors\nSparse Compressed Tensors\nSupported operations\ntorch.Storage\nTypedStorage\nUntypedStorage\nDoubleStorage\nFloatStorage\nHalfStorage\nLongStorage\nIntStorage\nShortStorage\nCharStorage\nByteStorage\nBoolStorage\nBFloat16Storage\nComplexDoubleStorage\nComplexFloatStorage\nQUInt8Storage\nQInt8Storage\nQInt32Storage\nQUInt4x2Storage\nQUInt2x4Storage\ntorch.testing\nassert_close()\nmake_tensor()\nassert_allclose()\ntorch.utils\ntorch.utils.rename_privateuse1_backend\ntorch.utils.generate_methods_for_privateuse1_backend\ntorch.utils.get_cpp_backtrace\ntorch.utils.set_module\ntorch.utils.benchmark\nTimer\nMeasurement\nCallgrindStats\nFunctionCounts\ntorch.utils.bottleneck\ntorch.utils.checkpoint\ncheckpoint()\ncheckpoint_sequential()\ntorch.utils.cpp_extension\nCppExtension()\nCUDAExtension()\nBuildExtension()\nload()\nload_inline()\ninclude_paths()\nget_compiler_abi_compatibility_and_version()\nverify_ninja_availability()\nis_ninja_available()\ntorch.utils.data\nDataset Types\nData Loading Order and Sampler\nLoading Batched and Non-Batched Data\nSingle- and Multi-process Data Loading\nMemory Pinning\ntorch.utils.jit\ntorch.utils.dlpack\nfrom_dlpack()\nto_dlpack()\ntorch.utils.mobile_optimizer\noptimize_for_mobile()\ntorch.utils.model_zoo\nload_url()\ntorch.utils.tensorboard\nSummaryWriter\nType Info\ntorch.finfo\ntorch.iinfo\nNamed Tensors\nCreating named tensors\nNamed dimensions\nName propagation semantics\nExplicit alignment by names\nManipulating dimensions\nAutograd support\nCurrently supported operations and subsystems\nNamed tensor API reference\nNamed Tensors operator coverage\nKeeps input names\nRemoves dimensions\nUnifies names from inputs\nPermutes dimensions\nContracts away dims\nFactory functions\nout function and in-place variants\ntorch.__config__\nshow()\nparallel_info()\ntorch._logging\ntorch._logging.set_logs\n\nLibraries\n\ntorchaudio\nTorchData\nTorchRec\nTorchServe\ntorchtext\ntorchvision\nPyTorch on XLA Devices\nINDICES AND TABLES\n\nIndex\n\nModule Index\n\nNext \n\n© Copyright 2023, PyTorch Contributors.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\nPyTorch documentation\nIndices and tables\nDocs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\nTutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\nResources\n\nFind development resources and get your questions answered\n\nView Resources\nPyTorch\nGet Started\nFeatures\nEcosystem\nBlog\nContributing\nResources\nTutorials\nDocs\nDiscuss\nGithub Issues\nBrand Guidelines\nStay up to date\nFacebook\nTwitter\nYouTube\nLinkedIn\nPyTorch Podcasts\nSpotify\nApple\nGoogle\nAmazon\nTerms\n | \nPrivacy\n\n© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/.\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy."
  }
]